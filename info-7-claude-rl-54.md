–•–æ—Ä–æ—à–æ, –ø–æ–Ω—è–ª. –ó–Ω–∞—á–∏—Ç, –Ω—É–∂–Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–±—Ä–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è logging –∏ –∑–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –≤—ã–∑–æ–≤—ã logger.info(), logger.error(), logger.warning() –∏ logger.debug() –Ω–∞ print() –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö. –¢–∞–∫–∂–µ —É–¥–∞–ª–∏–º –≤—Å–µ –∏–º–ø–æ—Ä—Ç—ã logging –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–≥–µ—Ä–æ–≤.
–≠—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –ø–æ—ç—Ç–æ–º—É —è –±—É–¥—É –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–¥ –ø–æ —Ñ–∞–π–ª–∞–º.

üìÅ 1. models/xlstm_rl_model.py
(–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ç–∞–º –Ω–µ—Ç logger)

üìÅ 2. config.py
(–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ç–∞–º –Ω–µ—Ç logger)

üìÅ 3. feature_engineering.py
(–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ç–∞–º –Ω–µ—Ç logger)

üìÅ 4. train_model.py
(–£–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ, –∫–∞–∫ –≤—ã —Ä–∞–Ω–µ–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∏)

üìÅ 5. rl_agent.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import numpy as np
import tensorflow as tf
from models.xlstm_rl_model import XLSTMRLModel
import os
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class RLAgent:
    """
    –ê–≥–µ–Ω—Ç Reinforcement Learning –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ - –ü–û–î–î–ï–†–ñ–ö–ê –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
    """
    def __init__(self, state_shape, memory_size=64, memory_units=128, gamma=0.99, epsilon=0.3, epsilon_min=0.1, epsilon_decay=0.995, batch_size=64):
        self.state_shape = state_shape
        self.gamma = gamma  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.epsilon = epsilon  # –ù–∞—á–∏–Ω–∞–µ–º —Å –º–µ–Ω—å—à–µ–≥–æ epsilon –¥–ª—è fine-tuning
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = XLSTMRLModel(input_shape=state_shape, 
                                 memory_size=memory_size, 
                                 memory_units=memory_units)
        
        # –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞
        self.memory = []
        self.max_memory_size = 10000
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        # self.logger = logging.getLogger('rl_agent')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
    
    def act(self, state, training=True):
        """–í—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if training and np.random.rand() < self.epsilon:
            return np.random.randint(0, 3)
        
        action_probs = self.model.predict_action(state)
        return np.argmax(action_probs)
    
    def remember(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä"""
        self.memory.append((state, action, reward, next_state, done))
        
        if len(self.memory) > self.max_memory_size:
            self.memory.pop(0)
    
    def update_epsilon(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ epsilon –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞"""
        if len(self.memory) < self.batch_size:
            return None
        
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = np.array([experience[0] for experience in batch])
        actions = np.array([experience[1] for experience in batch])
        rewards = np.array([experience[2] for experience in batch])
        next_states = np.array([experience[3] for experience in batch])
        dones = np.array([experience[4] for experience in batch])
        
        with tf.GradientTape() as tape:
            values = self.model.critic_model(states, training=True)
            next_values = self.model.critic_model(next_states, training=True)
            targets = rewards + self.gamma * tf.squeeze(next_values) * (1 - dones)
            targets = tf.expand_dims(targets, axis=1)
            critic_loss = tf.reduce_mean(tf.square(targets - values))
        
        critic_grads = tape.gradient(critic_loss, self.model.critic_model.trainable_variables)
        self.model.critic_optimizer.apply_gradients(zip(critic_grads, self.model.critic_model.trainable_variables))
        
        with tf.GradientTape() as tape:
            action_probs = self.model.actor_model(states, training=True)
            action_masks = tf.one_hot(actions, 3)
            values = self.model.critic_model(states, training=True)
            advantages = targets - values
            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)
            log_probs = tf.math.log(selected_action_probs + 1e-10)
            actor_loss = -tf.reduce_mean(log_probs * tf.squeeze(advantages))
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-10), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.model.actor_model.trainable_variables)
        self.model.actor_optimizer.apply_gradients(zip(actor_grads, self.model.actor_model.trainable_variables))
        
        return {
            'critic_loss': float(critic_loss),
            'actor_loss': float(actor_loss),
            'mean_value': float(tf.reduce_mean(values)),
            'mean_reward': float(np.mean(rewards))
        }
    
    def save(self, path='models'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.save(path, stage="_rl_final")
    
    def load(self, path='models'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.load(path, stage="_rl_finetuned")
    
    def log_action_distribution(self, states):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
        if len(states) == 0:
            return {'buy_count': 0, 'hold_count': 0, 'sell_count': 0, 'total': 0}
        
        actions = []
        for state in states:
            action_probs = self.model.predict_action(state)
            actions.append(np.argmax(action_probs))
        
        actions = np.array(actions)
        buy_count = np.sum(actions == 0)
        hold_count = np.sum(actions == 1)
        sell_count = np.sum(actions == 2)
        
        total = len(actions)
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π: BUY: {buy_count/total:.2%}, HOLD: {hold_count/total:.2%}, SELL: {sell_count/total:.2%}")
        
        return {
            'buy_count': int(buy_count),
            'hold_count': int(hold_count),
            'sell_count': int(sell_count),
            'total': total
        }



üìÅ 6. hybrid_decision_maker.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import numpy as np
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class HybridDecisionMaker:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π RL-–∞–≥–µ–Ω—Ç–∞
    """
    def __init__(self, rl_agent):
        self.rl_agent = rl_agent
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        # self.logger = logging.getLogger('hybrid_decision_maker')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
    
    def make_decision(self, state, training=False, position=0):
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ—Ä–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä—ã–Ω–∫–∞
        
        Args:
            state: —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞
            training: —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
            position: —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (0, 1, -1)
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - action: 0 (BUY), 1 (HOLD), –∏–ª–∏ 2 (SELL)
        - confidence: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ—à–µ–Ω–∏–∏ (0-1)
        """
        action_probs = self.rl_agent.model.predict_action(state)
        
        if training and np.random.rand() < self.rl_agent.epsilon:
            action = np.random.randint(0, 3)
            confidence = 1.0 / 3.0
        else:
            action = np.argmax(action_probs)
            confidence = action_probs[action]
        
        action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.debug -> print
        print(f"–ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ: {action_names[action]} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.4f}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: BUY: {action_probs[0]:.4f}, HOLD: {action_probs[1]:.4f}, SELL: {action_probs[2]:.4f}")
        print(f"–¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è: {position}")
        
        return action, confidence
    
    def explain_decision(self, state):
        """
        –û–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–∏–Ω—è—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        """
        action_probs = self.rl_agent.model.predict_action(state)
        action = np.argmax(action_probs)
        
        value = float(self.rl_agent.model.predict_value(state)[0])
        
        action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
        explanation = {
            'action': action_names[action],
            'confidence': float(action_probs[action]),
            'all_probs': {
                'BUY': float(action_probs[0]),
                'HOLD': float(action_probs[1]),
                'SELL': float(action_probs[2])
            },
            'state_value': value
        }
        
        return explanation



üìÅ 7. simulation_engine.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import os
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class SimulationEngine:
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    def __init__(self, environment, decision_maker, initial_balance=10000):
        self.env = environment
        self.decision_maker = decision_maker
        self.initial_balance = initial_balance
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –∏ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        # self.logger = logging.getLogger('simulation_engine')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
            
        #     file_handler = logging.FileHandler('simulation.log')
        #     file_handler.setFormatter(formatter)
        #     self.logger.addHandler(file_handler)
    
    def run_simulation(self, episodes=1, training=False, render=False):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏–º—É–ª—è—Ü–∏—é —Ç–æ—Ä–≥–æ–≤–ª–∏
        """
        all_episode_info = []
        
        for episode in range(episodes):
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–ó–∞–ø—É—Å–∫ —ç–ø–∏–∑–æ–¥–∞ {episode+1}/{episodes}")
            
            state, _ = self.env.reset()
            
            done = False
            total_reward = 0
            step = 0
            
            episode_data = {
                'steps': [],
                'rewards': [],
                'balances': [],
                'positions': [],
                'actions': [],
                'confidences': []
            }
            
            while not done:
                action, confidence = self.decision_maker.make_decision(
                    state,
                    training=training,
                    position=self.env.position
                )
                
                next_state, reward, done, _, info = self.env.step(action)
                
                if training:
                    self.decision_maker.rl_agent.remember(state, action, reward, next_state, done)
                
                state = next_state
                total_reward += reward
                step += 1
                
                episode_data['steps'].append(step)
                episode_data['rewards'].append(reward)
                episode_data['balances'].append(info['balance'])
                episode_data['positions'].append(info['position'])
                episode_data['actions'].append(action)
                episode_data['confidences'].append(confidence)
                
                if step % 100 == 0:
                    # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                    print(f"–®–∞–≥ {step}, –ë–∞–ª–∞–Ω—Å: {info['balance']:.2f}, –ù–∞–≥—Ä–∞–¥–∞: {reward:.4f}, "
                                    f"–ü–æ–∑–∏—Ü–∏—è: {info['position']}, –î–µ–π—Å—Ç–≤–∏–µ: {action}")
            
            final_balance = episode_data['balances'][-1]
            profit_percentage = (final_balance - self.initial_balance) / self.initial_balance * 100
            
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–≠–ø–∏–∑–æ–¥ {episode+1} –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–∞–Ω—Å: {final_balance:.2f} "
                            f"(–ü—Ä–∏–±—ã–ª—å: {profit_percentage:.2f}%), –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {step}")
            
            if training:
                for _ in range(10):
                    training_info = self.decision_maker.rl_agent.train()
                    if training_info:
                        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                        print(f"–û–±—É—á–µ–Ω–∏–µ: critic_loss: {training_info['critic_loss']:.4f}, "
                                        f"actor_loss: {training_info['actor_loss']:.4f}, "
                                        f"mean_value: {training_info['mean_value']:.4f}")
                
                self.decision_maker.rl_agent.update_epsilon()
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"Epsilon –æ–±–Ω–æ–≤–ª–µ–Ω –¥–æ {self.decision_maker.rl_agent.epsilon:.4f}")
            
            all_episode_info.append({
                'episode': episode + 1,
                'total_reward': total_reward,
                'final_balance': final_balance,
                'profit_percentage': profit_percentage,
                'steps': step,
                'data': episode_data
            })
            
            if render:
                self._render_episode(episode_data, episode + 1)
        
        return all_episode_info
    
    def _render_episode(self, episode_data, episode_num):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
        """
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)
        
        ax1.plot(episode_data['steps'], episode_data['balances'], 'b-')
        ax1.set_title(f'–≠–ø–∏–∑–æ–¥ {episode_num} - –ë–∞–ª–∞–Ω—Å')
        ax1.set_ylabel('–ë–∞–ª–∞–Ω—Å')
        ax1.grid(True)
        
        ax2.plot(episode_data['steps'], episode_data['rewards'], 'g-')
        ax2.set_title('–ù–∞–≥—Ä–∞–¥—ã')
        ax2.set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
        ax2.grid(True)
        
        ax3.plot(episode_data['steps'], episode_data['actions'], 'r-', label='–ü–æ–∑–∏—Ü–∏—è')
        ax3.scatter(episode_data['steps'], episode_data['actions'], c='purple', alpha=0.5, label='–î–µ–π—Å—Ç–≤–∏–µ')
        ax3.set_title('–ü–æ–∑–∏—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è')
        ax3.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax3.set_xlabel('–®–∞–≥')
        ax3.grid(True)
        ax3.legend()
        
        plt.tight_layout()
        
        os.makedirs('plots', exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f'plots/episode_{episode_num}_{timestamp}.png')
        plt.close()



üìÅ 8. trading_env.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class TradingEnvironment(gym.Env):
    """
    –°—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é RL
    """
    def __init__(self, data, sequence_length=60, initial_balance=10000, transaction_fee=0.001):
        super(TradingEnvironment, self).__init__()
        
        self.data = data  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.sequence_length = sequence_length
        self.initial_balance = initial_balance
        self.transaction_fee = transaction_fee
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: 0 - BUY, 1 - HOLD, 2 - SELL
        self.action_space = spaces.Discrete(3)
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ü–µ–Ω –∏ –æ–±—ä–µ–º–æ–≤ –ë–ï–ó –ø–æ–∑–∏—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É –¥–∞–Ω–Ω—ã—Ö
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.sequence_length, data.shape[2])
        )
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        # self.logger = logging.getLogger('trading_env')
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É
        self.reset()
    
    def reset(self, seed=None):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ä–µ–¥—É –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        super().reset(seed=seed)
        
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0  # 0 - –Ω–µ—Ç –ø–æ–∑–∏—Ü–∏–∏, 1 - –¥–ª–∏–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è, -1 - –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ–∑–∏—Ü–∏—è
        self.shares_held = 0
        self.cost_basis = 0
        self.total_trades = 0
        self.total_profit = 0
        
        observation = self._get_observation()
        
        return observation, {}
    
    def step(self, action):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –≤ —Å—Ä–µ–¥–µ"""
        if self.current_step >= len(self.data) - 1:
            observation = self._get_observation() if self.current_step < len(self.data) else self.data[-1].copy()
            info = {
                'balance': float(self.balance),
                'position': self.position,
                'shares_held': float(self.shares_held),
                'total_trades': self.total_trades,
                'total_profit': float(self.total_profit),
                'portfolio_value': float(self.balance)
            }
            return observation, 0.0, True, False, info
        
        current_price = self._get_current_price()
        
        max_position_size = self.initial_balance * 0.95
        
        reward = 0
        if action == 0:  # BUY
            if self.position != 1 and self.balance > 0:
                if self.position == -1:
                    reward += self._close_position(current_price)
                
                max_shares = min(max_position_size / current_price, self.balance / (current_price * (1 + self.transaction_fee)))
                shares_to_buy = max_shares * 0.9
                
                if shares_to_buy > 0:
                    cost = shares_to_buy * current_price * (1 + self.transaction_fee)
                    
                    if cost <= self.balance:
                        self.balance -= cost
                        self.shares_held = shares_to_buy
                        self.cost_basis = current_price
                        self.position = 1
                        self.total_trades += 1
        
        elif action == 2:  # SELL
            if self.position != -1 and self.balance > 0:
                if self.position == 1:
                    reward += self._close_position(current_price)
                
                max_shares = min(max_position_size / current_price, self.balance / current_price)
                shares_to_sell = max_shares * 0.9
                
                if shares_to_sell > 0:
                    self.shares_held = -shares_to_sell
                    self.cost_basis = current_price
                    self.position = -1
                    self.total_trades += 1
        
        self.current_step += 1
        
        done = self.current_step >= len(self.data) - 1
        
        if done and self.position != 0:
            reward += self._close_position(current_price)
        
        observation = self._get_observation()
        
        portfolio_value = self.balance
        
        if np.isnan(self.balance) or np.isinf(self.balance):
            self.balance = self.initial_balance
            portfolio_value = self.initial_balance
            reward = -1.0
        
        if self.position == 1 and self.shares_held > 0:
            position_value = self.shares_held * current_price
            if not (np.isnan(position_value) or np.isinf(position_value)):
                portfolio_value += position_value
        elif self.position == -1 and self.shares_held < 0:
            position_value = -self.shares_held * current_price
            if not (np.isnan(position_value) or np.isinf(position_value)):
                portfolio_value -= position_value
        
        if portfolio_value > self.initial_balance * 1000:
            portfolio_value = self.initial_balance * 1000
            reward = -0.5
        elif portfolio_value < self.initial_balance * 0.001:
            portfolio_value = self.initial_balance * 0.001
            reward = -0.5
        
        info = {
            'balance': float(self.balance) if not (np.isnan(self.balance) or np.isinf(self.balance)) else float(self.initial_balance),
            'position': self.position,
            'shares_held': float(self.shares_held) if not (np.isnan(self.shares_held) or np.isinf(self.shares_held)) else 0.0,
            'total_trades': self.total_trades,
            'total_profit': float(self.total_profit) if not (np.isnan(self.total_profit) or np.isinf(self.total_profit)) else 0.0,
            'portfolio_value': float(portfolio_value)
        }
        
        return observation, reward, done, False, info

    def _get_observation(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"""
        obs = self.data[self.current_step].copy()
        return obs
    
    def _get_current_price(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è"""
        try:
            price = self.data[self.current_step][-1, 3]
            
            if np.isnan(price) or np.isinf(price) or price <= 0:
                if self.current_step > 0:
                    return self.data[self.current_step-1][-1, 3]
                else:
                    return 100.0
            
            return price
        except (IndexError, ValueError):
            return 100.0
    
    def _close_position(self, current_price):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É"""
        reward = 0
        
        try:
            if self.position == 1 and self.shares_held > 0:
                profit = self.shares_held * (current_price - self.cost_basis)
                fee = self.shares_held * current_price * self.transaction_fee
                
                if not (np.isnan(profit) or np.isinf(profit) or np.isnan(fee) or np.isinf(fee)):
                    self.balance += self.shares_held * current_price - fee
                    reward = np.clip(profit / self.initial_balance, -10.0, 10.0)
                    self.total_profit += profit
            
            elif self.position == -1 and self.shares_held < 0:
                profit = -self.shares_held * (self.cost_basis - current_price)
                fee = -self.shares_held * current_price * self.transaction_fee
                
                if not (np.isnan(profit) or np.isinf(profit) or np.isnan(fee) or np.isinf(fee)):
                    self.balance += profit - fee
                    reward = np.clip(profit / self.initial_balance, -10.0, 10.0)
                    self.total_profit += profit
            
            if np.isnan(self.balance) or np.isinf(self.balance) or self.balance <= 0:
                self.balance = self.initial_balance * 0.1
                reward = -1.0
            
        except (OverflowError, ValueError) as e:
            self.balance = self.initial_balance
            reward = -1.0
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.warning -> print
            print(f"–ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏: {e}")
        
        self.shares_held = 0
        self.cost_basis = 0
        self.position = 0
        
        return reward



üìÅ 9. trade_manager.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import numpy as np
import pandas as pd
from pybit.unified_trading import HTTP
import time
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging
import json
from datetime import datetime
import os

class TradeManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–ª–µ–π –Ω–∞ –±–∏—Ä–∂–µ Bybit
    """
    def __init__(self, api_key, api_secret, api_url, order_amount, symbol, leverage="2"):
        self.api_key = api_key
        self.api_secret = api_secret
        self.api_url = api_url
        self.order_amount = order_amount
        self.symbol = symbol
        self.leverage = leverage
        
        self.session = HTTP(
            testnet=(api_url == "https://api-testnet.bybit.com"),
            api_key=api_key,
            api_secret=api_secret
        )
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –∏ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        # self.logger = logging.getLogger('trade_manager')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
            
        #     file_handler = logging.FileHandler('trading.log')
        #     file_handler.setFormatter(formatter)
        #     self.logger.addHandler(file_handler)
        
        self.trade_log = []
        self.position = 0
        
        self._set_leverage()
    
    def _set_leverage(self):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–ª–µ—á–æ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
        """
        try:
            response = self.session.set_leverage(
                category="linear",
                symbol=self.symbol,
                buyLeverage=self.leverage,
                sellLeverage=self.leverage
            )
            
            if response['retCode'] == 0:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                print(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–ª–µ—á–æ {self.leverage} –¥–ª—è {self.symbol}")
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.warning -> print
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–ª–µ—á–æ: {response['retMsg']}")
        
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–ª–µ—á–∞: {e}")
    
    def get_current_price(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        """
        try:
            response = self.session.get_tickers(
                category="linear",
                symbol=self.symbol
            )
            
            if response['retCode'] == 0:
                price = float(response['result']['list'][0]['lastPrice'])
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.debug -> print
                print(f"–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞ {self.symbol}: {price}")
                return price
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.warning -> print
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É: {response['retMsg']}")
                return None
        
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã: {e}")
            return None
    
    def place_order(self, action):
        """
        –†–∞–∑–º–µ—â–∞–µ—Ç –æ—Ä–¥–µ—Ä –Ω–∞ –±–∏—Ä–∂–µ
        
        action: 0 - BUY, 1 - HOLD, 2 - SELL
        """
        if action == 1:
            return True
        
        try:
            current_price = self.get_current_price()
            
            if current_price is None:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
                print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É –¥–ª—è —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –æ—Ä–¥–µ—Ä–∞")
                return False
            
            if action == 0:
                side = "Buy"
                if self.position == -1:
                    # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                    print("–ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é")
                    self._close_position()
            elif action == 2:
                side = "Sell"
                if self.position == 1:
                    # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                    print("–ó–∞–∫—Ä—ã–≤–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é")
                    self._close_position()
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
                print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: {action}")
                return False
            
            qty = self.order_amount / current_price
            
            response = self.session.place_order(
                category="linear",
                symbol=self.symbol,
                side=side,
                orderType="Market",
                qty=str(round(qty, 4)),
                timeInForce="GTC"
            )
            
            if response['retCode'] == 0:
                order_id = response['result']['orderId']
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                print(f"–†–∞–∑–º–µ—â–µ–Ω {side} –æ—Ä–¥–µ—Ä –Ω–∞ {qty} {self.symbol} –ø–æ —Ä—ã–Ω–æ—á–Ω–æ–π —Ü–µ–Ω–µ. ID: {order_id}")
                
                if action == 0:
                    self.position = 1
                elif action == 2:
                    self.position = -1
                
                self.trade_log.append({
                    'timestamp': datetime.now().isoformat(),
                    'action': 'BUY' if action == 0 else 'SELL',
                    'price': current_price,
                    'qty': qty,
                    'order_id': order_id
                })
                
                self._save_trade_log()
                
                return True
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏–∏ –æ—Ä–¥–µ—Ä–∞: {response['retMsg']}")
                return False
        
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏–∏ –æ—Ä–¥–µ—Ä–∞: {e}")
            return False
    
    def _close_position(self):
        """
        –ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é
        """
        try:
            response = self.session.get_positions(
                category="linear",
                symbol=self.symbol
            )
            
            if response['retCode'] != 0:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–∏—Ü–∏–∏: {response['retMsg']}")
                return False
            
            position_info = response['result']['list'][0]
            size = float(position_info['size'])
            
            if size == 0:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                print("–ù–µ—Ç –æ—Ç–∫—Ä—ã—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è")
                self.position = 0
                return True
            
            side = "Sell" if position_info['side'] == "Buy" else "Buy"
            
            response = self.session.place_order(
                category="linear",
                symbol=self.symbol,
                side=side,
                orderType="Market",
                qty=str(size),
                timeInForce="GTC",
                reduceOnly=True
            )
            
            if response['retCode'] == 0:
                order_id = response['result']['orderId']
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.info -> print
                print(f"–ó–∞–∫—Ä—ã—Ç–∞ –ø–æ–∑–∏—Ü–∏—è {self.symbol}. ID –æ—Ä–¥–µ—Ä–∞: {order_id}")
                self.position = 0
                
                self.trade_log.append({
                    'timestamp': datetime.now().isoformat(),
                    'action': 'CLOSE',
                    'price': self.get_current_price(),
                    'qty': size,
                    'order_id': order_id
                })
                
                self._save_trade_log()
                
                return True
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏: {response['retMsg']}")
                return False
        
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∑–∏—Ü–∏–∏: {e}")
            return False
    
    def get_position_info(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
        """
        try:
            response = self.session.get_positions(
                category="linear",
                symbol=self.symbol
            )
            
            if response['retCode'] == 0:
                position_info = response['result']['list'][0]
                return {
                    'symbol': position_info['symbol'],
                    'side': position_info['side'],
                    'size': float(position_info['size']),
                    'entry_price': float(position_info['entryPrice']),
                    'leverage': float(position_info['leverage']),
                    'unrealised_pnl': float(position_info['unrealisedPnl']),
                    'position_value': float(position_info['positionValue'])
                }
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.warning -> print
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏: {response['retMsg']}")
                return None
        
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–∏—Ü–∏–∏: {e}")
            return None
    
    def _save_trade_log(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∂—É—Ä–Ω–∞–ª —Ç–æ—Ä–≥–æ–≤–ª–∏ –≤ —Ñ–∞–π–ª
        """
        try:
            with open('trade_log.json', 'w') as f:
                json.dump(self.trade_log, f, indent=2)
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: self.logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∂—É—Ä–Ω–∞–ª–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏: {e}")



üìÅ 10. run_live_trading.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import os
import sys
import time
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import tensorflow as tf
from pybit.unified_trading import HTTP

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineering
from models.xlstm_rl_model import XLSTMRLModel
from hybrid_decision_maker import HybridDecisionMaker
from trade_manager import TradeManager
from rl_agent import RLAgent
import config

# üî• –£–î–ê–õ–ï–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
#     handlers=[
#         logging.StreamHandler(),
#         logging.FileHandler('live_trading.log')
#     ]
# )
# logger = logging.getLogger('live_trading')

def fetch_latest_data(session, symbol, timeframe, limit=100):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–≤–µ—á–∏ —Å –±–∏—Ä–∂–∏"""
    try:
        response = session.get_kline(
            category="linear",
            symbol=symbol,
            interval=timeframe,
            limit=limit
        )
        
        if response['retCode'] == 0:
            data = response['result']['list']
            
            df = pd.DataFrame(data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
            ])
            
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df['timestamp'] = pd.to_datetime(pd.to_numeric(df['timestamp']), unit='ms')
            df['symbol'] = symbol
            
            df.sort_values('timestamp', inplace=True)
            
            return df
        else:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {response['retMsg']}")
            return None
    
    except Exception as e:
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∂–∏–≤–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""
    # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
    print("üöÄ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –ñ–ò–í–û–ô –¢–û–†–ì–û–í–õ–ò –° –¢–†–Å–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–¨–Æ")
    
    api_key = config.BYBIT_API_KEY
    api_secret = config.BYBIT_API_SECRET
    api_url = config.API_URL
    symbol = config.SYMBOLS[0]
    timeframe = config.TIMEFRAME
    order_amount = config.ORDER_USDT_AMOUNT
    leverage = config.LEVERAGE
    sequence_length = config.SEQUENCE_LENGTH
    required_candles = config.REQUIRED_CANDLES
    
    session = HTTP(
        testnet=(api_url == "https://api-demo.bybit.com"),
        api_key=api_key,
        api_secret=api_secret
    )
    
    feature_engineering = FeatureEngineering(sequence_length=sequence_length)
    
    if not feature_engineering.load_scaler():
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∫–µ–π–ª–µ—Ä. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        return
    
    input_shape = (sequence_length, len(feature_engineering.feature_columns))
    rl_model = XLSTMRLModel(input_shape=input_shape, 
                          memory_size=config.XLSTM_MEMORY_SIZE, 
                          memory_units=config.XLSTM_MEMORY_UNITS)
    
    try:
        rl_model.load(stage="_rl_finetuned")
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ supervised –º–æ–¥–µ–ª–∏...")
        try:
            rl_model.load(stage="_supervised")
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print("‚úÖ Supervised –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–∞–∫ fallback")
        except Exception as e2:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏–∫–∞–∫—É—é –º–æ–¥–µ–ª—å: {e2}")
            return
    
    rl_agent = RLAgent(state_shape=input_shape, 
                      memory_size=config.XLSTM_MEMORY_SIZE, 
                      memory_units=config.XLSTM_MEMORY_UNITS)
    rl_agent.model = rl_model
    
    decision_maker = HybridDecisionMaker(rl_agent)
    
    trade_manager = TradeManager(
        api_key=api_key,
        api_secret=api_secret,
        api_url=api_url,
        order_amount=order_amount,
        symbol=symbol,
        leverage=leverage
    )
    
    # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é...")
    
    while True:
        try:
            current_time = datetime.now()
            
            df = fetch_latest_data(session, symbol, timeframe, limit=required_candles)
            
            if df is None or len(df) < sequence_length:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–ª—É—á–µ–Ω–æ: {len(df) if df is not None else 0} —Å—Ç—Ä–æ–∫")
                time.sleep(10)
                continue
            
            X, _, _ = feature_engineering.prepare_test_data(df)
            
            if len(X) == 0:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                time.sleep(10)
                continue
            
            current_state = X[-1]
            
            action, confidence = decision_maker.make_decision(
                current_state,
                position=trade_manager.position
            )
            
            action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"üìä –†–µ—à–µ–Ω–∏–µ: {action_names[action]} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.4f})")
            
            explanation = decision_maker.explain_decision(current_state)
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"üß† –ê–Ω–∞–ª–∏–∑: BUY={explanation['all_probs']['BUY']:.3f}, "
                       f"HOLD={explanation['all_probs']['HOLD']:.3f}, "
                       f"SELL={explanation['all_probs']['SELL']:.3f}, "
                       f"Value={explanation['state_value']:.4f}")
            
            if trade_manager.place_order(action):
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"‚úÖ –û—Ä–¥–µ—Ä —Ä–∞–∑–º–µ—â–µ–Ω: {action_names[action]}")
            else:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å –æ—Ä–¥–µ—Ä: {action_names[action]}")
            
            position_info = trade_manager.get_position_info()
            if position_info and position_info['size'] > 0:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"üí∞ –ü–æ–∑–∏—Ü–∏—è: {position_info['side']} {position_info['size']}, "
                           f"PnL: {position_info['unrealised_pnl']}")
            
            time.sleep(30)
            
        except KeyboardInterrupt:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print("‚èπÔ∏è –¢–æ—Ä–≥–æ–≤–ª—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç–æ—Ä–≥–æ–≤–ª–∏: {e}")
            time.sleep(10)

if __name__ == "__main__":
    main()


üìÅ 11. validation_metrics_callback.py
üîÑ –ò–ó–ú–ï–ù–Ø–ï–ú:
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

# üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
# logger = logging.getLogger('validation_callback')

class ValidationMetricsCallback(tf.keras.callbacks.Callback):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è Supervised Pre-training (–≠—Ç–∞–ø 1)
    """
    def __init__(self, X_val, y_val, class_names=['SELL', 'HOLD', 'BUY']):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.class_names = class_names
        
    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 5 == 0:  # –ö–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}:")
            
            y_pred_probs = self.model.predict(self.X_val, verbose=0)
            y_pred_classes = np.argmax(y_pred_probs, axis=1)
            
            if self.y_val.ndim > 1 and self.y_val.shape[1] > 1:
                y_true_classes = np.argmax(self.y_val, axis=1)
            else:
                y_true_classes = self.y_val
            
            # Confusion Matrix
            cm = confusion_matrix(y_true_classes, y_pred_classes)
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print("Confusion Matrix:")
            
            header = "     " + " ".join([f"{name:4s}" for name in self.class_names])
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(header)
            for i, row in enumerate(cm):
                row_str = " ".join([f"{val:4d}" for val in row])
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"{self.class_names[i]:4s} {row_str}")
            
            # Classification Report
            report_dict = classification_report(
                y_true_classes, y_pred_classes, 
                target_names=self.class_names,
                output_dict=True,
                zero_division=0
            )
            
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"Macro Avg F1-Score: {report_dict['macro avg']['f1-score']:.3f}")
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"Weighted Avg F1-Score: {report_dict['weighted avg']['f1-score']:.3f}")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            pred_distribution = np.bincount(y_pred_classes, minlength=len(self.class_names)) / len(y_pred_classes)
            pred_dist_str = ", ".join([f"{name}={dist:.1%}" for name, dist in zip(self.class_names, pred_distribution)])
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_dist_str}")

