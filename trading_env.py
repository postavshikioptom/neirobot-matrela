import gymnasium as gym
import numpy as np
import pandas as pd
from typing import Dict, Any

class TradingEnvRL(gym.Env):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è RL –∞–≥–µ–Ω—Ç–∞ —Å VSA –∏ xLSTM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
    """
    
    def __init__(self, df: pd.DataFrame, xlstm_model, initial_balance=10000, commission=0.0008):
        super(TradingEnvRL, self).__init__()
        
        self.df = df.copy()
        self.xlstm_model = xlstm_model
        self.initial_balance = initial_balance
        self.commission = commission
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: 0=SELL, 1=BUY, 2=HOLD
        self.action_space = gym.spaces.Discrete(3)
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: xLSTM –≤—ã—Ö–æ–¥ + –ø–æ—Ä—Ç—Ñ–µ–ª—å
        # xLSTM –≤—ã—Ö–æ–¥ (3) + –ø–æ—Ä—Ç—Ñ–µ–ª—å (4) = 7 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32
        )
        
        self.reset()
    
    def _get_xlstm_prediction(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç xLSTM –º–æ–¥–µ–ª–∏"""
        if self.current_step < 10:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 10 —Å–≤–µ—á–µ–π –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            return np.array([0.33, 0.33, 0.34])  # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–≤–µ—á–µ–π –¥–ª—è xLSTM
        sequence_data = self.df.iloc[self.current_step-10:self.current_step]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏)
        features = sequence_data[self.feature_columns].values
        features_reshaped = features.reshape(1, 10, len(self.feature_columns))
        
        return self.xlstm_model.predict(features_reshaped)[0]
    
    
    def _get_portfolio_state(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è"""
        return np.array([
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            self.position,  # -1, 0, 1
            self.unrealized_pnl / self.initial_balance if self.position != 0 else 0,
            self.steps_in_position / 100.0  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –≤ –ø–æ–∑–∏—Ü–∏–∏
        ])
    
    def _get_observation(self):
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –¥–ª—è RL –∞–≥–µ–Ω—Ç–∞"""
        xlstm_pred = self._get_xlstm_prediction()  # 3 —ç–ª–µ–º–µ–Ω—Ç–∞
        portfolio_state = self._get_portfolio_state()  # 4 —ç–ª–µ–º–µ–Ω—Ç–∞
        
        return np.concatenate([xlstm_pred, portfolio_state]).astype(np.float32)
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 10  # –ù–∞—á–∏–Ω–∞–µ–º —Å 10-–π —Å–≤–µ—á–∏ –¥–ª—è xLSTM
        self.balance = self.initial_balance
        self.position = 0  # -1=short, 0=flat, 1=long
        self.entry_price = 0
        self.unrealized_pnl = 0
        self.steps_in_position = 0
        
        # üî• –ù–û–í–´–ï FEATURE_COLUMNS - –¢–û–õ–¨–ö–û –ò–ù–î–ò–ö–ê–¢–û–†–´ (–¥–ª—è RL —Å—Ä–µ–¥—ã)
        self.feature_columns = [
            # ‚úÖ –í–°–ï –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ (–ë–ï–ó –ë–û–õ–õ–ò–ù–î–ñ–ï–†–ê –ò ATR_14)
            'RSI_14', 'MACD_12_26_9', 'MACD_signal', 'MACD_hist',
            'ADX_14', 'STOCHk_14_3_3', 'STOCHd_14_3_3',
            'WILLR_14', # üî• –ù–û–í–´–ô –ò–ù–î–ò–ö–ê–¢–û–†
            'AO_5_34',  # üî• –ù–û–í–´–ô –ò–ù–î–ò–ö–ê–¢–û–†
            
            # ‚ùå –í–°–ï –ü–ê–¢–¢–ï–†–ù–´ –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–´
            # 'CDLHAMMER', 'CDLENGULFING', 'CDLDOJI', 'CDLSHOOTINGSTAR',
            # 'CDLHANGINGMAN', 'CDLMARUBOZU',
            # 'CDLINVERTEDHAMMER', 'CDLDRAGONFLYDOJI', 'CDLBELTHOLD',
            # 'hammer_f', 'hangingman_f', 'engulfing_f', 'doji_f',
            # 'shootingstar_f', 'bullish_marubozu_f',
            # 'inverted_hammer_f', 'dragonfly_doji_f', 'bullish_pin_bar_f', 'bullish_belt_hold_f',
            
            # ‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú EVENT SAMPLING
            'is_event'
        ]
        
        return self._get_observation(), {}
    
    def step(self, action):
        if self.current_step >= len(self.df) - 1:
            return self._get_observation(), 0, True, False, {}
            
        current_price = self.df['close'].iloc[self.current_step]
        reward = 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π PnL
        if self.position != 0:
            if self.position == 1:  # Long
                self.unrealized_pnl = (current_price - self.entry_price) / self.entry_price
            else:  # Short
                self.unrealized_pnl = (self.entry_price - current_price) / self.entry_price
            self.steps_in_position += 1
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        if action == 0:  # SELL
            if self.position == 1:  # –ó–∞–∫—Ä—ã–≤–∞–µ–º long
                pnl = self.unrealized_pnl - (self.commission * 2)
                xlstm_pred_for_reward = self._get_xlstm_prediction()
                reward = self._calculate_advanced_reward(action, pnl * 100, xlstm_pred_for_reward)
                self.balance *= (1 + pnl)
                self.position = 0
                self.steps_in_position = 0
                    
            elif self.position == 0:  # –û—Ç–∫—Ä—ã–≤–∞–µ–º short
                self.position = -1
                self.entry_price = current_price
                self.steps_in_position = 0
                
        elif action == 1:  # BUY
            if self.position == -1:  # –ó–∞–∫—Ä—ã–≤–∞–µ–º short
                pnl = self.unrealized_pnl - (self.commission * 2)
                xlstm_pred_for_reward = self._get_xlstm_prediction()
                reward = self._calculate_advanced_reward(action, pnl * 100, xlstm_pred_for_reward)
                self.balance *= (1 + pnl)
                self.position = 0
                self.steps_in_position = 0
                    
            elif self.position == 0:  # –û—Ç–∫—Ä—ã–≤–∞–µ–º long
                self.position = 1
                self.entry_price = current_price
                self.steps_in_position = 0
                
        else:  # HOLD
            if self.position == 0:
                reward = -0.1  # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ
            else:
                # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                if self.steps_in_position > 50:
                    reward = -2
                else:
                    reward = self.unrealized_pnl * 10  # –ü–æ–æ—â—Ä—è–µ–º –ø—Ä–∏–±—ã–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
        
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1
        
        return self._get_observation(), reward, done, False, {}
    def _calculate_advanced_reward(self, action, pnl_pct, xlstm_prediction):
        """
        Advanced reward system for RL agent with VSA and xLSTM integration.
        """
        base_reward = pnl_pct if pnl_pct != 0 else 0
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        speed_bonus = 0
        if pnl_pct > 0 and self.steps_in_position < 20:
            speed_bonus = 2

        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–æ–ª–≥–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ —É–±—ã—Ç–æ—á–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        hold_penalty = 0
        if pnl_pct < 0 and self.steps_in_position > 30:
            hold_penalty = -3

        # –ë–æ–Ω—É—Å –∑–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å xLSTM
        xlstm_conf = np.max(xlstm_prediction)
        if xlstm_conf > 0.7:
            base_reward += xlstm_conf * 2

        # –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ xLSTM
        predicted_action_idx = np.argmax(xlstm_prediction)
        xlstm_to_rl_map = {0: 1, 1: 0, 2: 2}  # xLSTM_BUY->RL_BUY, xLSTM_SELL->RL_SELL
        
        if action != 2 and action != xlstm_to_rl_map.get(predicted_action_idx):
            base_reward -= 1

        # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –±–∞–ª–∞–Ω—Å–∞ (—Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç)
        if self.balance < self.initial_balance * 0.9:
            base_reward -= 5

        # –°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–ù–´–ô –ë–û–ù–£–° –ó–ê –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ò –≠–ù–¢–†–û–ü–ò–Æ
        exploration_bonus = 0
        if action in [0, 1]:
            exploration_bonus = 0.2
        
        entropy_bonus = 0
        entropy = -np.sum(xlstm_prediction * np.log(xlstm_prediction + 1e-10))
        normalized_entropy = entropy / np.log(len(xlstm_prediction))
        entropy_bonus = normalized_entropy * 0.2

        # –ù–û–í–´–ô –ö–û–î - –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–≥—Ä–∞–¥ –¥–ª—è RL (–±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ HOLD)
        hold_reward = 0
        overtrading_penalty = 0

        current_row = self.df.iloc[self.current_step]
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è "—è–≤–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"
        buy_signal_strength = (
            (current_row.get('RSI_14', 50) < 30) +
            (current_row.get('ADX_14', 0) > 25) +
            (current_row.get('MACD_hist', 0) > 0.001) +
            (current_row.get('WILLR_14', -50) < -80) + # üî• –ù–û–í–û–ï: WILLR_14 –¥–ª—è BUY (—Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–æ)
            (current_row.get('AO_5_34', 0) > 0) # üî• –ù–û–í–û–ï: AO –≤—ã—à–µ –Ω—É–ª—è
        )
        sell_signal_strength = (
            (current_row.get('RSI_14', 50) > 70) +
            (current_row.get('ADX_14', 0) > 25) +
            (current_row.get('MACD_hist', 0) < -0.001) +
            (current_row.get('WILLR_14', -50) > -20) + # üî• –ù–û–í–û–ï: WILLR_14 –¥–ª—è SELL (—Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–æ)
            (current_row.get('AO_5_34', 0) < 0) # üî• –ù–û–í–û–ï: AO –Ω–∏–∂–µ –Ω—É–ª—è
        )

        if action == 2: # HOLD
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ AO_5_34 –∏ ADX_14 –¥–ª—è HOLD reward
            ao_value = current_row.get('AO_5_34', 0)
            adx = current_row.get('ADX_14', 0)

            # –ï—Å–ª–∏ –º–æ–º–µ–Ω—Ç—É–º –Ω–∏–∑–∫–∏–π (AO –±–ª–∏–∑–∫–æ –∫ 0) –∏ ADX –Ω–∏–∑–∫–∏–π (—Ñ–ª—ç—Ç)
            if abs(ao_value) < 0.001 and adx < 20: # –ü–æ—Ä–æ–≥–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ–¥–æ–±—Ä–∞—Ç—å
                hold_reward = 0.5
            # –ï—Å–ª–∏ —Å–∏–ª—å–Ω—ã–π –º–æ–º–µ–Ω—Ç—É–º (–±–æ–ª—å—à–æ–π AO) –∏–ª–∏ —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥ (–±–æ–ª—å—à–æ–π ADX)
            elif abs(ao_value) > 0.005 or adx > 30:
                hold_reward = -0.5
            else:
                hold_reward = 0.1
            
            if pnl_pct < 0 and self.steps_in_position > 30:
                hold_penalty = -3
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å –∑–∞ HOLD, –µ—Å–ª–∏ –Ω–µ—Ç —Å–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
            if buy_signal_strength < 1 and sell_signal_strength < 1:
                hold_reward += 1.0
            else:
                hold_reward -= 1.0

        else: # –ï—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ BUY –∏–ª–∏ SELL (–Ω–µ HOLD)
            # –®—Ç—Ä–∞—Ñ –∑–∞ overtrading (—Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–¥–µ–ª–∫–∏, –∫–æ–≥–¥–∞ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞)
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∞–±—ã–µ BUY-—Å–∏–≥–Ω–∞–ª—ã, –µ—Å–ª–∏ RL –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç BUY
            if action == 1 and buy_signal_strength < 2:
                overtrading_penalty = -1.0
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∞–±—ã–µ SELL-—Å–∏–≥–Ω–∞–ª—ã, –µ—Å–ª–∏ RL –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç SELL
            elif action == 0 and sell_signal_strength < 2:
                overtrading_penalty = -1.0

        total_reward = base_reward + speed_bonus + hold_penalty + exploration_bonus + entropy_bonus + hold_reward + overtrading_penalty
        
        return total_reward