import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import os
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
try:
    import numpy as np
except Exception:
    np = None

try:
    import matplotlib.pyplot as plt
except Exception:
    plt = None

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("‚ö†Ô∏è SHAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

class SimulationEngine:
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    def __init__(self, environment, decision_maker, initial_balance=10000):
        self.env = environment
        self.decision_maker = decision_maker
        self.initial_balance = initial_balance
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –∏ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        # self.logger = logging.getLogger('simulation_engine')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
            
        #     file_handler = logging.FileHandler('simulation.log')
        #     file_handler.setFormatter(formatter)
        #     self.logger.addHandler(file_handler)
    
    def run_simulation(self, episodes=1, training=False, render=False):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏–º—É–ª—è—Ü–∏—é —Ç–æ—Ä–≥–æ–≤–ª–∏
        """
        all_episode_info = []
        
        for episode in range(episodes):
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–ó–∞–ø—É—Å–∫ —ç–ø–∏–∑–æ–¥–∞ {episode+1}/{episodes}")
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É
            state, _ = self.env.reset()
            
            done = False
            total_reward = 0
            step = 0
            
            episode_data = {
                'steps': [],
                'rewards': [],
                'balances': [],
                'positions': [],
                'actions': [],
                'confidences': []
            }
            
            while not done:
                # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
                # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ (–ø–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é)
                action, confidence = self.decision_maker.make_decision(
                    state,
                    training=training,
                    position=self.env.position
                )
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                next_state, reward, done, _, info = self.env.step(action)
                
                # –ï—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
                if training:
                    self.decision_maker.rl_agent.remember(state, action, reward, next_state, done)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                state = next_state
                total_reward += reward
                step += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —à–∞–≥–∞
                episode_data['steps'].append(step)
                episode_data['rewards'].append(reward)
                episode_data['balances'].append(info['balance'])
                episode_data['positions'].append(info['position'])
                episode_data['actions'].append(action)
                episode_data['confidences'].append(confidence)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
                if step % 100 == 0:
                    # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                    print(f"–®–∞–≥ {step}, –ë–∞–ª–∞–Ω—Å: {info['balance']:.2f}, –ù–∞–≥—Ä–∞–¥–∞: {reward:.4f}, "
                                    f"–ü–æ–∑–∏—Ü–∏—è: {info['position']}, –î–µ–π—Å—Ç–≤–∏–µ: {action}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
            final_balance = episode_data['balances'][-1]
            profit_percentage = (final_balance - self.initial_balance) / self.initial_balance * 100
            
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–≠–ø–∏–∑–æ–¥ {episode+1} –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–∞–Ω—Å: {final_balance:.2f} "
                            f"(–ü—Ä–∏–±—ã–ª—å: {profit_percentage:.2f}%), –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {step}")
            
            # –ï—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è, –æ–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –∏ epsilon
            if training:
                for _ in range(10):  # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥
                    training_info = self.decision_maker.rl_agent.train()
                    if training_info:
                        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                        print(f"–û–±—É—á–µ–Ω–∏–µ: critic_loss: {training_info['critic_loss']:.4f}, "
                                        f"actor_loss: {training_info['actor_loss']:.4f}, "
                                        f"mean_value: {training_info['mean_value']:.4f}")
                
                self.decision_maker.rl_agent.update_epsilon()
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"Epsilon –æ–±–Ω–æ–≤–ª–µ–Ω –¥–æ {self.decision_maker.rl_agent.epsilon:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —ç–ø–∏–∑–æ–¥–∞
            all_episode_info.append({
                'episode': episode + 1,
                'total_reward': total_reward,
                'final_balance': final_balance,
                'profit_percentage': profit_percentage,
                'steps': step,
                'data': episode_data
            })
            
            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
            if render:
                self._render_episode(episode_data, episode + 1)
        
        return all_episode_info

    def analyze_feature_importance_shap(self, sample_data, feature_names=None, max_samples=100):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é SHAP

        Args:
            sample_data: –û–±—Ä–∞–∑—Ü—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (X_test –∏–ª–∏ X_val)
            feature_names: –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        if not SHAP_AVAILABLE:
            print("‚ùå SHAP –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install shap")
            return None

        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ decision_maker
            if hasattr(self.decision_maker, 'model') and hasattr(self.decision_maker.model, 'actor_model'):
                model = self.decision_maker.model.actor_model
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è SHAP –∞–Ω–∞–ª–∏–∑–∞")
                return None

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            if len(sample_data) > max_samples:
                indices = np.random.choice(len(sample_data), max_samples, replace=False)
                sample_data = sample_data[indices]

            print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º SHAP –∞–Ω–∞–ª–∏–∑ –Ω–∞ {len(sample_data)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")

            # –°–æ–∑–¥–∞–µ–º background dataset (—Å–ª—É—á–∞–π–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã)
            background_size = min(50, len(sample_data))
            background_indices = np.random.choice(len(sample_data), background_size, replace=False)
            background = sample_data[background_indices]

            # –°–æ–∑–¥–∞–µ–º explainer
            explainer = shap.DeepExplainer(model, background)

            # –í—ã—á–∏—Å–ª—è–µ–º SHAP –∑–Ω–∞—á–µ–Ω–∏—è
            shap_values = explainer.shap_values(sample_data)

            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥–æ–≤, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            if isinstance(shap_values, list):
                shap_values = shap_values[0]

            # –°–æ–∑–¥–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
            if feature_names is None:
                feature_names = [f"Feature_{i}" for i in range(sample_data.shape[-1])]

            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º summary plot
            plt.figure(figsize=(12, 8))
            shap.summary_plot(shap_values, sample_data.reshape(sample_data.shape[0], -1),
                            feature_names=feature_names, show=False)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            os.makedirs('plots', exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plt.savefig(f'plots/shap_summary_{timestamp}.png', bbox_inches='tight')
            plt.close()

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            mean_shap = np.mean(np.abs(shap_values), axis=0)

            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ 3D (sequence_length, features), —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–∞–º
            if len(mean_shap.shape) > 1:
                mean_shap = np.mean(mean_shap, axis=0)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            feature_importance = sorted(zip(feature_names, mean_shap),
                                      key=lambda x: x[1], reverse=True)

            print("üìä –¢–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ SHAP:")
            for i, (feature, importance) in enumerate(feature_importance[:10]):
                print("2d")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            shap_results = {
                'feature_importance': feature_importance,
                'shap_values': shap_values,
                'sample_data': sample_data,
                'feature_names': feature_names,
                'timestamp': timestamp
            }

            return shap_results

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ SHAP –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return None

    def _render_episode(self, episode_data, episode_num):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
        """
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –±–∞–ª–∞–Ω—Å–∞
        ax1.plot(episode_data['steps'], episode_data['balances'], 'b-')
        ax1.set_title(f'–≠–ø–∏–∑–æ–¥ {episode_num} - –ë–∞–ª–∞–Ω—Å')
        ax1.set_ylabel('–ë–∞–ª–∞–Ω—Å')
        ax1.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥
        ax2.plot(episode_data['steps'], episode_data['rewards'], 'g-')
        ax2.set_title('–ù–∞–≥—Ä–∞–¥—ã')
        ax2.set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
        ax2.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ–∑–∏—Ü–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π
        ax3.plot(episode_data['steps'], episode_data['positions'], 'r-', label='–ü–æ–∑–∏—Ü–∏—è')
        ax3.scatter(episode_data['steps'], episode_data['actions'], c='purple', alpha=0.5, label='–î–µ–π—Å—Ç–≤–∏–µ')
        ax3.set_title('–ü–æ–∑–∏—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è')
        ax3.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax3.set_xlabel('–®–∞–≥')
        ax3.grid(True)
        ax3.legend()
        
        plt.tight_layout()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs('plots', exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f'plots/episode_{episode_num}_{timestamp}.png')
        plt.close()