import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import os
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class SimulationEngine:
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    def __init__(self, environment, decision_maker, initial_balance=10000):
        self.env = environment
        self.decision_maker = decision_maker
        self.initial_balance = initial_balance
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –∏ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        # self.logger = logging.getLogger('simulation_engine')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
            
        #     file_handler = logging.FileHandler('simulation.log')
        #     file_handler.setFormatter(formatter)
        #     self.logger.addHandler(file_handler)
    
    def run_simulation(self, episodes=1, training=False, render=False):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏–º—É–ª—è—Ü–∏—é —Ç–æ—Ä–≥–æ–≤–ª–∏
        """
        all_episode_info = []
        
        for episode in range(episodes):
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–ó–∞–ø—É—Å–∫ —ç–ø–∏–∑–æ–¥–∞ {episode+1}/{episodes}")
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É
            state, _ = self.env.reset()
            
            done = False
            total_reward = 0
            step = 0
            
            episode_data = {
                'steps': [],
                'rewards': [],
                'balances': [],
                'positions': [],
                'actions': [],
                'confidences': []
            }
            
            while not done:
                # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
                # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ (–ø–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é)
                action, confidence = self.decision_maker.make_decision(
                    state,
                    training=training,
                    position=self.env.position
                )
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                next_state, reward, done, _, info = self.env.step(action)
                
                # –ï—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
                if training:
                    self.decision_maker.rl_agent.remember(state, action, reward, next_state, done)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                state = next_state
                total_reward += reward
                step += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —à–∞–≥–∞
                episode_data['steps'].append(step)
                episode_data['rewards'].append(reward)
                episode_data['balances'].append(info['balance'])
                episode_data['positions'].append(info['position'])
                episode_data['actions'].append(action)
                episode_data['confidences'].append(confidence)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
                if step % 100 == 0:
                    # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                    print(f"–®–∞–≥ {step}, –ë–∞–ª–∞–Ω—Å: {info['balance']:.2f}, –ù–∞–≥—Ä–∞–¥–∞: {reward:.4f}, "
                                    f"–ü–æ–∑–∏—Ü–∏—è: {info['position']}, –î–µ–π—Å—Ç–≤–∏–µ: {action}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
            final_balance = episode_data['balances'][-1]
            profit_percentage = (final_balance - self.initial_balance) / self.initial_balance * 100
            
            # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            print(f"–≠–ø–∏–∑–æ–¥ {episode+1} –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–∞–Ω—Å: {final_balance:.2f} "
                            f"(–ü—Ä–∏–±—ã–ª—å: {profit_percentage:.2f}%), –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {step}")
            
            # –ï—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è, –æ–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –∏ epsilon
            if training:
                for _ in range(10):  # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥
                    training_info = self.decision_maker.rl_agent.train()
                    if training_info:
                        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                        print(f"–û–±—É—á–µ–Ω–∏–µ: critic_loss: {training_info['critic_loss']:.4f}, "
                                        f"actor_loss: {training_info['actor_loss']:.4f}, "
                                        f"mean_value: {training_info['mean_value']:.4f}")
                
                self.decision_maker.rl_agent.update_epsilon()
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
                print(f"Epsilon –æ–±–Ω–æ–≤–ª–µ–Ω –¥–æ {self.decision_maker.rl_agent.epsilon:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —ç–ø–∏–∑–æ–¥–∞
            all_episode_info.append({
                'episode': episode + 1,
                'total_reward': total_reward,
                'final_balance': final_balance,
                'profit_percentage': profit_percentage,
                'steps': step,
                'data': episode_data
            })
            
            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
            if render:
                self._render_episode(episode_data, episode + 1)
        
        return all_episode_info
    
    def _render_episode(self, episode_data, episode_num):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
        """
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –±–∞–ª–∞–Ω—Å–∞
        ax1.plot(episode_data['steps'], episode_data['balances'], 'b-')
        ax1.set_title(f'–≠–ø–∏–∑–æ–¥ {episode_num} - –ë–∞–ª–∞–Ω—Å')
        ax1.set_ylabel('–ë–∞–ª–∞–Ω—Å')
        ax1.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥
        ax2.plot(episode_data['steps'], episode_data['rewards'], 'g-')
        ax2.set_title('–ù–∞–≥—Ä–∞–¥—ã')
        ax2.set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
        ax2.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ–∑–∏—Ü–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π
        ax3.plot(episode_data['steps'], episode_data['positions'], 'r-', label='–ü–æ–∑–∏—Ü–∏—è')
        ax3.scatter(episode_data['steps'], episode_data['actions'], c='purple', alpha=0.5, label='–î–µ–π—Å—Ç–≤–∏–µ')
        ax3.set_title('–ü–æ–∑–∏—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è')
        ax3.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax3.set_xlabel('–®–∞–≥')
        ax3.grid(True)
        ax3.legend()
        
        plt.tight_layout()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs('plots', exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f'plots/episode_{episode_num}_{timestamp}.png')
        plt.close()