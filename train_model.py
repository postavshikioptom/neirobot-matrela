import pandas as pd
import numpy as np
import argparse
import os
import gc
import pickle  # –î–û–ë–ê–í–¨–¢–ï —ç—Ç—É —Å—Ç—Ä–æ–∫—É
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

import tensorflow as tf

def configure_gpu_memory():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã"""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –í–∫–ª—é—á–∞–µ–º —Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏ + –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –¥–æ 80% –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π
                tf.config.experimental.set_virtual_device_configuration(
                    gpu,
                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*10)]  # 10GB max
                )
            print(f"‚úÖ GPU –ø–∞–º—è—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è {len(gpus)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
    else:
        print("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")

if __name__ == "__main__":
    configure_gpu_memory()

# ‚úÖ –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –° –†–ê–ó–ù–´–ú–ò –°–†–ï–î–ê–ú–ò
# –û—Ç–∫–ª—é—á–∞–µ–º XLA –µ—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã (–º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ)
# os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'

# –ù–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã
from feature_engineering import calculate_features, detect_candlestick_patterns, calculate_vsa_features
from models.xlstm_rl_model import XLSTMRLModel
from rl_agent import IntelligentRLAgent
from trading_env import TradingEnvRL
from hybrid_decision_maker import HybridDecisionMaker
from regularization_callback import AntiOverfittingCallback
from validation_metrics import ValidationMetricsCallback

import tensorflow.keras.backend as K

@tf.keras.utils.register_keras_serializable()
class CustomFocalLoss(tf.keras.losses.Loss):
    def __init__(self, gamma=1.0, alpha=0.3, class_weights=None, name='CustomFocalLoss', **kwargs): # –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω **kwargs
        super().__init__(name=name, **kwargs) # –ò–ó–ú–ï–ù–ï–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º **kwargs –≤ super()
        self.gamma = gamma
        self.alpha = alpha
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ class_weights - —ç—Ç–æ tf.constant
        if class_weights is None:
            self.class_weights = tf.constant([1.2, 1.2, 0.8], dtype=tf.float32) # Default weights
        else:
            self.class_weights = tf.constant(class_weights, dtype=tf.float32)

    def call(self, y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)
        y_true = tf.cast(y_true, tf.float32)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Å-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤–µ—Å–∞
        weights = tf.reduce_sum(self.class_weights * y_true, axis=-1, keepdims=True)
        
        cross_entropy = -y_true * K.log(y_pred)
        loss = self.alpha * K.pow(1 - y_pred, self.gamma) * cross_entropy * weights
        
        return K.sum(loss, axis=-1)

    def get_config(self):
        config = super().get_config()
        config.update({
            'gamma': self.gamma,
            'alpha': self.alpha,
            'class_weights': self.class_weights.numpy().tolist(), # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫
        })
        return config

def prepare_xlstm_rl_data(data_path, sequence_length=10):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –µ–¥–∏–Ω–æ–π xLSTM+RL —Å–∏—Å—Ç–µ–º—ã
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...")
    full_df = pd.read_csv(data_path)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    feature_cols = [
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        'RSI_14', 'MACD_12_26_9', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',
        'ADX_14', 'STOCHk_14_3_3', 'STOCHd_14_3_3',
        'ATR_14', # <--- –î–û–ë–ê–í–õ–ï–ù–û
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã
        'CDLHAMMER', 'CDLENGULFING', 'CDLDOJI', 'CDLSHOOTINGSTAR',
        'CDLHANGINGMAN', 'CDLMARUBOZU',
        # VSA –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–æ–≤—ã–µ!)
        'vsa_no_demand', 'vsa_no_supply', 'vsa_stopping_volume',
        'vsa_climactic_volume', 'vsa_test', 'vsa_effort_vs_result', 'vsa_strength',
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        'volume_ratio', 'spread_ratio', 'close_position',
        'is_event' # <--- –î–û–ë–ê–í–õ–ï–ù–û: –ù–æ–≤—ã–π –ø—Ä–∏–∑–Ω–∞–∫
    ]
    
    all_X = []
    all_y = []
    processed_dfs = {}  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è RL
    
    symbols = full_df['symbol'].unique()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π...")
    
    for symbol in symbols:
        df = full_df[full_df['symbol'] == symbol].copy()
        print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–∞: {symbol}, —Å—Ç—Ä–æ–∫: {len(df)}")
        
        if len(df) < sequence_length + 50:  # –ù—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            continue
            
        # === –ù–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –° VSA ===
        df = calculate_features(df)
        df = detect_candlestick_patterns(df)
        df = calculate_vsa_features(df)  # –î–æ–±–∞–≤–ª—è–µ–º VSA!
        
        # =====================================================================
        # –ù–û–í–´–ô –ë–õ–û–ö: –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û 'is_event' (Event-Based Sampling)
        # =====================================================================
        initial_rows = len(df)
        df_event_filtered = df[df['is_event'] == 1].copy()
        
        if len(df_event_filtered) < sequence_length + 50:
            print(f"‚ö†Ô∏è –î–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±—ã—Ç–∏–π ({len(df_event_filtered)}), –∏—Å–ø–æ–ª—å–∑—É—é –≤—Å–µ –¥–∞–Ω–Ω—ã–µ.")
            df_processed = df.copy() # –ï—Å–ª–∏ —Å–æ–±—ã—Ç–∏–π –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        else:
            print(f"‚úÖ –î–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df_event_filtered)} —Å–æ–±—ã—Ç–∏–π –∏–∑ {initial_rows} –±–∞—Ä–æ–≤.")
            df_processed = df_event_filtered.copy()
        
        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º
        df_processed.reset_index(drop=True, inplace=True)

        if len(df_processed) < sequence_length + 50: # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        
        df = df_processed # –¢–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ–º —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º DataFrame
        # =====================================================================
        # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
        # =====================================================================
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω + VSA –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        df['future_return'] = (df['close'].shift(-5) - df['close']) / df['close']
        
        # =====================================================================
        # –ù–û–í–´–ô –ö–û–î - –ë–û–õ–ï–ï –°–¢–†–û–ì–ò–ï –£–°–õ–û–í–ò–Ø –î–õ–Ø BUY/SELL
        # =====================================================================
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        df['base_threshold'] = 0.008  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 0.0005 –¥–æ 0.008 (0.8%)
        df['dynamic_threshold'] = np.maximum(
            df['base_threshold'],
            (df['ATR_14'] / df['close'] * 1.2).fillna(0.008)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
        )

        # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ VSA —É—Å–ª–æ–≤–∏—è
        df['vsa_buy_strength'] = (
            0.5 * (df['vsa_no_supply'] == 1) +
            0.5 * (df['vsa_stopping_volume'] == 1) +
            0.3 * np.clip(df['vsa_strength'] / 2.0, 0, 1)  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        )

        df['vsa_sell_strength'] = (
            0.5 * (df['vsa_no_demand'] == 1) +
            0.5 * (df['vsa_climactic_volume'] == 1) +
            0.3 * np.clip(-df['vsa_strength'] / 2.0, 0, 1)
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
        strong_trend = df['ADX_14'] > 25
        high_volume = df['volume_ratio'] > 1.5
        rsi_extreme_buy = df['RSI_14'] < 30
        rsi_extreme_sell = df['RSI_14'] > 70

        # –ë–û–õ–ï–ï –°–¢–†–û–ì–ò–ï —É—Å–ª–æ–≤–∏—è –¥–ª—è BUY/SELL
        buy_condition = (
            (df['future_return'] > df['dynamic_threshold']) &
            (df['vsa_buy_strength'] > 0.6) &  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —Å 0.2 –¥–æ 0.6
            (strong_trend | high_volume | rsi_extreme_buy)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        )

        sell_condition = (
            (df['future_return'] < -df['dynamic_threshold']) &
            (df['vsa_sell_strength'] > 0.6) &  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —Å 0.2 –¥–æ 0.6
            (strong_trend | high_volume | rsi_extreme_sell)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        )
        # =====================================================================
        # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
        # =====================================================================
        
        # –°–Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –≤ HOLD, –∑–∞—Ç–µ–º –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º
        df['target'] = 2  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é HOLD
        df.loc[buy_condition, 'target'] = 0 # BUY
        df.loc[sell_condition, 'target'] = 1 # SELL

        # –î–û–ë–ê–í–¨–¢–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)
        # –≠—Ç–æ—Ç –±–ª–æ–∫ –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å, –µ—Å–ª–∏ –ø–æ—Å–ª–µ –æ—Å–ª–∞–±–ª–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤ –±–∞–ª–∞–Ω—Å –≤—Å–µ –µ—â–µ –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ–π.
        # –û–Ω –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç—å "HOLD" –≤ BUY/SELL –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥—Ä—É–≥–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤.
        # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å "–≥—Ä—è–∑–Ω—ã–º" —Ä–µ—à–µ–Ω–∏–µ–º, –Ω–æ –∏–Ω–æ–≥–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        current_buy_count = (df['target'] == 0).sum()
        current_sell_count = (df['target'] == 1).sum()
        current_hold_count = (df['target'] == 2).sum()

        # =====================================================================
        # –ù–û–í–´–ô –ö–û–î - –£–ú–ï–ù–¨–®–ê–ï–ú –ü–ï–†–ï–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Æ
        # =====================================================================
        # –¢–µ–ø–µ—Ä—å –ù–ï –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º, –µ—Å–ª–∏ HOLD —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω—å—à–µ 70%
        if current_hold_count < (current_buy_count + current_sell_count) * 2.0:  # –ï—Å–ª–∏ HOLD < 66%
            print(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ HOLD —Å–∏–≥–Ω–∞–ª–æ–≤. –î–û–ë–ê–í–õ–Ø–ï–ú HOLD –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")
            
            # –í–º–µ—Å—Ç–æ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ HOLD –≤ BUY/SELL, –¥–µ–ª–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ
            # –ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —á–∞—Å—Ç—å —Å–ª–∞–±—ã—Ö BUY/SELL –≤ HOLD
            
            weak_buy_indices = df[
                (df['target'] == 0) &
                (df['vsa_buy_strength'] < 0.4) &  # –°–ª–∞–±—ã–µ VSA —Å–∏–≥–Ω–∞–ª—ã
                (df['RSI_14'] > 35) & (df['RSI_14'] < 65)  # RSI –≤ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∑–æ–Ω–µ
            ].index
            
            weak_sell_indices = df[
                (df['target'] == 1) &
                (df['vsa_sell_strength'] < 0.4) &  # –°–ª–∞–±—ã–µ VSA —Å–∏–≥–Ω–∞–ª—ã
                (df['RSI_14'] > 35) & (df['RSI_14'] < 65)  # RSI –≤ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∑–æ–Ω–µ
            ].index
            
            # –ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º 30% —Å–ª–∞–±—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ HOLD
            import random
            random.seed(42)
            
            if len(weak_buy_indices) > 0:
                reclassify_buy = random.sample(
                    list(weak_buy_indices),
                    min(int(len(weak_buy_indices) * 0.3), len(weak_buy_indices))
                )
                df.loc[reclassify_buy, 'target'] = 2  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ HOLD
            
            if len(weak_sell_indices) > 0:
                reclassify_sell = random.sample(
                    list(weak_sell_indices),
                    min(int(len(weak_sell_indices) * 0.3), len(weak_sell_indices))
                )
                df.loc[reclassify_sell, 'target'] = 2  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ HOLD

        else:
            print(f"‚úÖ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø—Ä–∏–µ–º–ª–µ–º—ã–π, –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞.")
        # =====================================================================
        # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
        # =====================================================================
        
        # –£–±–∏—Ä–∞–µ–º NaN –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df.dropna(subset=['future_return'], inplace=True)
        for col in feature_cols:
            if col not in df.columns:
                df[col] = 0
        
        df = df[feature_cols + ['target', 'close', 'volume']].copy()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è RL
        processed_dfs[symbol] = df
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è xLSTM
        if len(df) > sequence_length:
            for i in range(len(df) - sequence_length):
                all_X.append(df.iloc[i:i + sequence_length][feature_cols].values)
                all_y.append(df.iloc[i + sequence_length]['target'])
    
    if not all_X:
        raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤")
        
    print(f"–°–æ–∑–¥–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(all_X)}")
    
    X = np.array(all_X, dtype=np.float32)
    y = to_categorical(np.array(all_y), num_classes=3)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)
    
    # =====================================================================
    # –ù–û–í–´–ô –ë–õ–û–ö: –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï IMBLEARN –î–õ–Ø –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò –ö–õ–ê–°–°–û–í
    # =====================================================================
    try:
        from imblearn.over_sampling import SMOTE
        from imblearn.under_sampling import RandomUnderSampler
        from imblearn.pipeline import Pipeline
        from collections import Counter

        print("\nüîÑ –ü—Ä–∏–º–µ–Ω—è—é Oversampling/Undersampling –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º y –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è imblearn
        y_labels = np.argmax(y, axis=1)
        
        print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –î–û imblearn: {Counter(y_labels)}")

        # –¶–µ–ª–µ–≤–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ:
        # –ù–∞—á–Ω–µ–º —Å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ: 20% BUY, 20% SELL, 60% HOLD
        # –í—ã –º–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —ç—Ç–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã.
        # –í–∞–∂–Ω–æ: SMOTE —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤ (0, 1, 2)
        
        # –°–Ω–∞—á–∞–ª–∞ oversampling –º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞ (BUY, SELL)
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º BUY –∏ SELL –¥–æ 20% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
        # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –±—É–¥–µ—Ç –æ–∫–æ–ª–æ len(X) * (1 + oversampling_ratio))
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
        # –¶–µ–ª–µ–≤–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 20% BUY, 20% SELL, 60% HOLD (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π oversampling)
        total_samples = len(X)
        target_buy_count = int(total_samples * 0.20)  # –ò–ó–ú–ï–ù–ï–ù–û: —Å 0.10 –¥–æ 0.20
        target_sell_count = int(total_samples * 0.20) # –ò–ó–ú–ï–ù–ï–ù–û: —Å 0.10 –¥–æ 0.20
        
        current_buy_count = Counter(y_labels)[0]
        current_sell_count = Counter(y_labels)[1]

        sampling_strategy_smote = {
            0: max(current_buy_count, target_buy_count),
            1: max(current_sell_count, target_sell_count)
        }
        
        if current_buy_count > 0 or current_sell_count > 0:
            k_neighbors = min(5,
                              (current_buy_count - 1 if current_buy_count > 1 else 1),
                              (current_sell_count - 1 if current_sell_count > 1 else 1))
            k_neighbors = max(1, k_neighbors)

            if any(count <= k_neighbors for count in [current_buy_count, current_sell_count] if count > 0):
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è SMOTE —Å k_neighbors, –∏—Å–ø–æ–ª—å–∑—É—é RandomOverSampler.")
                from imblearn.over_sampling import RandomOverSampler
                oversampler = RandomOverSampler(sampling_strategy=sampling_strategy_smote, random_state=42)
            else:
                oversampler = SMOTE(sampling_strategy=sampling_strategy_smote, random_state=42, k_neighbors=k_neighbors)

            X_temp, y_temp_labels = oversampler.fit_resample(X.reshape(len(X), -1), y_labels)
            print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ Oversampling: {Counter(y_temp_labels)} (BUY/SELL —É–≤–µ–ª–∏—á–µ–Ω—ã)")
        else:
            X_temp, y_temp_labels = X.reshape(len(X), -1), y_labels
            print("–ü—Ä–æ–ø—É—Å—Ç–∏–ª Oversampling, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç BUY/SELL —Å–∏–≥–Ω–∞–ª–æ–≤.")

        # Undersampling HOLD: –¶–µ–ª—å - —á—Ç–æ–±—ã HOLD –±—ã–ª –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 1.5 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ, —á–µ–º —Å—É–º–º–∞ BUY + SELL
        current_hold_count_after_oversample = Counter(y_temp_labels)[2]
        target_hold_count = min(current_hold_count_after_oversample, int((Counter(y_temp_labels)[0] + Counter(y_temp_labels)[1]) * 1.5)) # –ò–ó–ú–ï–ù–ï–ù–û: —Å 3.0 –¥–æ 1.5
        
        undersampler = RandomUnderSampler(sampling_strategy={2: target_hold_count}, random_state=42)
        X_resampled, y_resampled_labels = undersampler.fit_resample(X_temp, y_temp_labels)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º X –æ–±—Ä–∞—Ç–Ω–æ –≤ 3D —Ñ–æ—Ä–º—É
        X = X_resampled.reshape(len(X_resampled), sequence_length, X.shape[-1])
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ one-hot
        y = to_categorical(y_resampled_labels, num_classes=3)

        print(f"‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(X)}")
        print(f"–ù–æ–≤—ã–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ü–û–°–õ–ï imblearn:")
        unique, counts = np.unique(np.argmax(y, axis=1), return_counts=True)
        class_names = ['BUY', 'SELL', 'HOLD']
        for class_idx, count in zip(unique, counts):
            print(f"  {class_names[class_idx]}: {count} ({count/len(y)*100:.1f}%)")

    except ImportError:
        print("‚ö†Ô∏è imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–ø—É—Å—Ç–∏–ª oversampling/undersampling. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install imbalanced-learn")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ oversampling/undersampling: {e}")
    # =====================================================================
    # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê IMBLEARN
    # =====================================================================
    
    # –î–û–ë–ê–í–¨–¢–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:")
    unique, counts = np.unique(np.argmax(y, axis=1), return_counts=True)
    class_names = ['BUY', 'SELL', 'HOLD']
    for class_idx, count in zip(unique, counts):
        print(f"  {class_names[class_idx]}: {count} ({count/len(y)*100:.1f}%)")
    
    return X, y, processed_dfs, feature_cols

def train_xlstm_rl_system(X, y, processed_dfs, feature_cols):
    """
    –û–±—É—á–∞–µ—Ç –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É xLSTM + RL
    """
    print("\n=== –≠–¢–ê–ü 1: –û–ë–£–ß–ï–ù–ò–ï xLSTM –ú–û–î–ï–õ–ò ===")
    
    # –î–û–ë–ê–í–¨–¢–ï: –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    os.makedirs('models', exist_ok=True)
    
    # =====================================================================
    # =====================================================================
    # –ù–û–í–´–ô –ë–õ–û–ö: –£–õ–£–ß–®–ï–ù–ù–´–ô TIME SERIES SPLIT
    # =====================================================================
    from sklearn.model_selection import StratifiedKFold # –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º StratifiedKFold
    from collections import Counter

    print("\nüîÑ –ü—Ä–∏–º–µ–Ω—è—é –°–¢–†–ê–¢–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô TimeSeriesSplit –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")

    # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª–∏–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% –¥–∞–Ω–Ω—ã—Ö)
    test_size = int(len(X) * 0.2)
    X_temp, X_test = X[:-test_size], X[-test_size:]
    y_temp, y_test = y[:-test_size], y[-test_size:]

    # –î–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º StratifiedKFold,
    # —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–º —Å–ø–ª–∏—Ç–µ.
    # –ú—ã –Ω–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TimeSeriesSplit —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –Ω–∞–ø—Ä—è–º—É—é,
    # –ø–æ—ç—Ç–æ–º—É –∏–º–∏—Ç–∏—Ä—É–µ–º –µ–≥–æ, –±–µ—Ä—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
    n_splits_stratified = 5 # –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º 5 —Å–ø–ª–∏—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    skf = StratifiedKFold(n_splits=n_splits_stratified, shuffle=False) # shuffle=False –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞

    train_indices_list = []
    val_indices_list = []

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    y_temp_labels = np.argmax(y_temp, axis=1)

    for train_idx, val_idx in skf.split(X_temp, y_temp_labels): # –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º y_temp_labels –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        train_indices_list.append(train_idx)
        val_indices_list.append(val_idx)

    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–ª–∏—Ç –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å "–≤—Ä–µ–º–µ–Ω–Ω–æ–π" –∞—Å–ø–µ–∫—Ç
    X_train, y_train = X_temp[train_indices_list[-1]], y_temp[train_indices_list[-1]]
    X_val, y_val = X_temp[val_indices_list[-1]], y_temp[val_indices_list[-1]]

    print(f"‚úÖ –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π TimeSeriesSplit –∑–∞–≤–µ—Ä—à–µ–Ω.")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ X_train: {Counter(np.argmax(y_train, axis=1))}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ X_val: {Counter(np.argmax(y_val, axis=1))}")
    # =====================================================================
    # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
    # =====================================================================
    
    # =====================================================================
    # –ù–û–í–´–ô –ë–õ–û–ö: –í–´–ß–ò–°–õ–ï–ù–ò–ï –ò –ü–ï–†–ï–î–ê–ß–ê –í–ï–°–û–í –ö–õ–ê–°–°–û–í
    # =====================================================================
    from sklearn.utils.class_weight import compute_class_weight
    y_integers = np.argmax(y_train, axis=1)
    class_weights_array = compute_class_weight(
        'balanced',
        classes=np.unique(y_integers),
        y=y_integers
    )
    class_weight_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}

    # –ù–û–í–´–ô –ö–û–î - –ë–ê–õ–ê–ù–°–ò–†–£–ï–ú –í–ï–°–ê –ü–†–ê–í–ò–õ–¨–ù–û
    # –ü—Ä–æ–±–ª–µ–º–∞: HOLD –∏–º–µ–µ—Ç —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π recall, –Ω—É–∂–Ω–æ –£–í–ï–õ–ò–ß–ò–¢–¨ –µ–≥–æ –≤–µ—Å
    
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å–∞ BUY/SELL –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å —É–¥–µ–ª—è–ª–∞ –∏–º –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è,
    # –Ω–æ –Ω–µ –Ω–∞—Å—Ç–æ–ª—å–∫–æ, —á—Ç–æ–±—ã –æ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∞ HOLD.
    # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å HOLD, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ.
    if 0 in class_weight_dict:
        class_weight_dict[0] *= 1.5  # –ò–ó–ú–ï–ù–ï–ù–û: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å BUY
    if 1 in class_weight_dict:
        class_weight_dict[1] *= 1.5  # –ò–ó–ú–ï–ù–ï–ù–û: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å SELL
    
    if 2 in class_weight_dict:
        class_weight_dict[2] *= 0.7  # –ò–ó–ú–ï–ù–ï–ù–û: –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å HOLD
    
    print(f"üìä –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weight_dict}")
    # =====================================================================
    # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
    # =====================================================================
    
    # –î–û–ë–ê–í–¨–¢–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    gc.collect()
    tf.keras.backend.clear_session()
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # –í —Ñ—É–Ω–∫—Ü–∏–∏ train_xlstm_rl_system(), –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è xlstm_model –¥–æ–±–∞–≤—å—Ç–µ:
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: X_train={X_train.shape}, y_train={y_train.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ NaN/Inf –≤ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):
        print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
        X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)
        X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)
        print("‚úÖ NaN/Inf –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã")
        
    # =====================================================================
    # –ù–û–í–´–ô –ë–õ–û–ö: –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø/–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
    # =====================================================================
    xlstm_model = XLSTMRLModel(
        input_shape=(X_train.shape[1], X_train.shape[2]),
        memory_units=128,
        attention_units=64
    )
    xlstm_model.build_model() # <--- –°–ù–ê–ß–ê–õ–ê –°–¢–†–û–ò–ú –ú–û–î–ï–õ–¨!

    checkpoint_path = 'models/xlstm_checkpoint_latest.keras'
    scaler_path = 'models/xlstm_rl_scaler.pkl'

    if os.path.exists(checkpoint_path):
        print("–ù–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞...")
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞, —Ç–∞–∫ –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —É–∂–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞
            xlstm_model.model.load_weights(checkpoint_path) # <--- –ò–ó–ú–ï–ù–ï–ù–û: load_weights –≤–º–µ—Å—Ç–æ load_model
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    xlstm_model.scaler = pickle.load(f)
                xlstm_model.is_trained = True
                print("‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏ scaler –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
            else:
                print("‚ö†Ô∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –Ω–æ–≤—ã–π")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏: {e}, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
            # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏, —á—Ç–æ –∏ –Ω—É–∂–Ω–æ.
    else:
        print("–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
    # =====================================================================
    # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
    # =====================================================================

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
    xlstm_model.scaler.fit(X_train_reshaped)
    X_train_scaled = xlstm_model.scaler.transform(X_train_reshaped).reshape(X_train.shape)
    X_val_scaled = xlstm_model.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)

    # =====================================================================
    # –ù–û–í–´–ô –ë–õ–û–ö: –ò–ù–™–ï–ö–¶–ò–Ø –®–£–ú–ê –í–û –í–•–û–î–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (–¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏)
    # –≠—Ç–æ—Ç –±–ª–æ–∫ —Ç–µ–ø–µ—Ä—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –°–†–ê–ó–£ –ü–û–°–õ–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è X_train_scaled –∏ X_val_scaled
    # =====================================================================
    print("\n —à—É–º–æ–≤—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º —Ç–æ–ª—å–∫–æ –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
    noise_std_multiplier = 0.005 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è —à—É–º–∞ (0.5%)

    # –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: —à—É–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∏–ª–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
    noise_level = np.std(X_train_scaled) * noise_std_multiplier # –®—É–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω std –¥–∞–Ω–Ω—ã—Ö

    X_train_noisy = X_train_scaled + np.random.normal(0, noise_level, X_train_scaled.shape)
    # –û—Å—Ç–∞–≤–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –±–µ–∑ —à—É–º–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –æ—Ü–µ–Ω–∫–∏
    X_val_noisy = X_val_scaled

    # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å
    X_train_to_model = X_train_noisy
    X_val_to_model = X_val_noisy
    print(f"‚úÖ –®—É–º –¥–æ–±–∞–≤–ª–µ–Ω –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º (—É—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {noise_level:.4f})")
    # =====================================================================
    # –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê
    # =====================================================================

    # –¢–µ–ø–µ—Ä—å xlstm_model.model –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–µ None, –º–æ–∂–Ω–æ –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å
    # –ù–û–í–´–ô –ö–û–î - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Learning Rate –∫–∞–∫ float
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=0.0005,  # –ò–ó–ú–ï–ù–ï–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º float literal
        clipnorm=0.5,
        weight_decay=0.0001
    )
    xlstm_model.model.compile(
        optimizer=optimizer,
        loss=CustomFocalLoss(gamma=1.0, alpha=0.3, class_weights=[1.2, 1.2, 0.8]), # –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å
        metrics=[
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.Precision(name='precision_0', class_id=0),
            tf.keras.metrics.Precision(name='precision_1', class_id=1),
            tf.keras.metrics.Precision(name='precision_2', class_id=2),
            tf.keras.metrics.Recall(name='recall_0', class_id=0),
            tf.keras.metrics.Recall(name='recall_1', class_id=1),
            tf.keras.metrics.Recall(name='recall_2', class_id=2),
        ]
    )

    # –î–û–ë–ê–í–¨–¢–ï: –ö–∞—Å—Ç–æ–º–Ω—ã–µ –∫–æ–ª–±—ç–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
    class MemoryCleanupCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            if epoch % 10 == 0:  # –ö–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
                gc.collect()
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–ï –æ—á–∏—â–∞–µ–º —Å–µ—Å—Å–∏—é –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
                # tf.keras.backend.clear_session()  # –≠—Ç–æ –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ!
                print(f"–≠–ø–æ—Ö–∞ {epoch}: –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
    
    class DetailedProgressCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            logs = logs or {} # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ logs –Ω–µ None
            try:
                lr = self.model.optimizer.learning_rate.numpy()
                # –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ accuracy, precision, recall
                # –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ accuracy, precision, recall
                print(f"–≠–ø–æ—Ö–∞ {epoch+1}/100 - loss: {logs.get('loss', 0):.4f} - val_loss: {logs.get('val_loss', 0):.4f} - "
                      f"accuracy: {logs.get('accuracy', 0):.2f} - val_accuracy: {logs.get('val_accuracy', 0):.2f} - "
                      f"precision: {logs.get('precision', 0):.2f} - val_precision: {logs.get('val_precision', 0):.2f} - "
                      f"recall: {logs.get('recall', 0):.2f} - val_recall: {logs.get('val_recall', 0):.2f} - lr: {lr:.2e}")
            
                # –î–û–ë–ê–í–õ–ï–ù–û: –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
                # –≠—Ç–æ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if 'precision_0' in logs:
                    print(f"  Class 0 (BUY): Prec={logs.get('precision_0', 0):.2f}, Rec={logs.get('recall_0', 0):.2f}")
                if 'precision_1' in logs:
                    print(f"  Class 1 (SELL): Prec={logs.get('precision_1', 0):.2f}, Rec={logs.get('recall_1', 0):.2f}")
                if 'precision_2' in logs:
                    print(f"  Class 2 (HOLD): Prec={logs.get('precision_2', 0):.2f}, Rec={logs.get('recall_2', 0):.2f}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
                if logs.get('val_loss', 0) > logs.get('loss', 0) * 2:
                    print("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!")
            except Exception as e:
                print(f"–≠–ø–æ—Ö–∞ {epoch+1}/100 - loss: {logs.get('loss', 0):.4f} - val_loss: {logs.get('val_loss', 0):.4f} (–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e})")
            
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            mode='min',
            patience=15,
            restore_best_weights=True,
            verbose=1,
            min_delta=0.001
        ),
        AntiOverfittingCallback(patience=8, min_improvement=0.005),  # –ù–û–í–´–ô –ö–û–õ–ë–≠–ö
        MemoryCleanupCallback(),
        DetailedProgressCallback(),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.7,
            patience=8,
            min_lr=1e-6,
            verbose=1
        ),
        ValidationMetricsCallback(X_val_to_model, y_val),  # –ù–û–í–´–ô –ö–û–õ–ë–≠–ö
    ]

    # –û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–±—ç–∫–∞–º–∏
    history = xlstm_model.train(
        X_train_to_model, y_train, # <--- –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º X_train_to_model
        X_val_to_model, y_val,     # <--- –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º X_val_to_model
        epochs=80,      # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö —Å 100 –¥–æ 80
        batch_size=32,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch_size —Å 16 –¥–æ 32
        class_weight=class_weight_dict,
        custom_callbacks=callbacks
    )

    # –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è xLSTM, –¥–æ–±–∞–≤—å—Ç–µ:
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è xLSTM:")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è loss: {history.history['loss'][-1]:.4f}")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è val_loss: {history.history['val_loss'][-1]:.4f}")
    print(f"–õ—É—á—à–∞—è val_loss: {min(history.history['val_loss']):.4f}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {len(history.history['loss'])}")
    
    # –û—Ü–µ–Ω–∫–∞ xLSTM
    try:
        X_test_scaled = xlstm_model.scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
        # –ù–û–í–´–ô –ö–û–î - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ model.evaluate —Å return_dict=True
        evaluation_results_dict = xlstm_model.model.evaluate(X_test_scaled, y_test, verbose=0, return_dict=True) # –ò–ó–ú–ï–ù–ï–ù–û: return_dict=True

        loss = evaluation_results_dict.get('loss', 0.0)
        accuracy = evaluation_results_dict.get('accuracy', 0.0)
        precision = evaluation_results_dict.get('precision', 0.0)
        recall = evaluation_results_dict.get('recall', 0.0)

        print(f"xLSTM Loss: {loss:.4f}")
        print(f"xLSTM –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100:.2f}%")
        print(f"xLSTM Precision: {precision * 100:.2f}%")
        print(f"xLSTM Recall: {recall * 100:.2f}%")

        # –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        for i, class_name in enumerate(['BUY', 'SELL', 'HOLD']):
            prec_i = evaluation_results_dict.get(f'precision_{i}', 0.0)
            rec_i = evaluation_results_dict.get(f'recall_{i}', 0.0)
            print(f"  Class {i} ({class_name}): Prec={prec_i:.2f}, Rec={rec_i:.2f}")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º xLSTM –º–æ–¥–µ–ª—å
    xlstm_model.save_model()

    # –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è xlstm_model, –æ–±—É—á–∏—Ç–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–æ–≤
    # –í–æ–∑—å–º–∏—Ç–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π DataFrame –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    # –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ –≤–æ–∑—å–º–∏—Ç–µ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π
    regime_training_df = pd.concat(list(processed_dfs.values())).reset_index(drop=True)
    decision_maker_temp = HybridDecisionMaker(
        xlstm_model_path='models/xlstm_rl_model.keras',
        rl_agent_path=None,  # <--- –ò–ó–ú–ï–ù–ï–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º None, —Ç–∞–∫ –∫–∞–∫ RL –∞–≥–µ–Ω—Ç –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω
        feature_columns=feature_cols,
        sequence_length=X.shape[1]
    )
    decision_maker_temp.fit_regime_detector(regime_training_df, xlstm_model, feature_cols)
    decision_maker_temp.regime_detector.save_detector('models/market_regime_detector.pkl')
    print("‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    print("\n=== –≠–¢–ê–ü 2: –û–ë–£–ß–ï–ù–ò–ï RL –ê–ì–ï–ù–¢–ê ===")
    
    # –î–û–ë–ê–í–¨–¢–ï: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    rl_symbols = list(processed_dfs.keys())[:2]  # –¢–æ–ª—å–∫–æ 2 —Å–∏–º–≤–æ–ª–∞ –≤–º–µ—Å—Ç–æ 3
    
    rl_agent = None
    for i, symbol in enumerate(rl_symbols):
        df = processed_dfs[symbol]
        print(f"\n–û–±—É—á–µ–Ω–∏–µ RL –Ω–∞ —Å–∏–º–≤–æ–ª–µ {symbol} ({i+1}/{len(rl_symbols)})")
        
        # –î–û–ë–ê–í–¨–¢–ï: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º RL –∞–≥–µ–Ω—Ç–æ–º
        gc.collect()
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        split_idx = int(len(df) * 0.8)
        train_df = df.iloc[:split_idx].copy()
        eval_df = df.iloc[split_idx:].copy()
        
        if len(train_df) < 100 or len(eval_df) < 50:
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
            
        # –°–æ–∑–¥–∞–µ–º RL –∞–≥–µ–Ω—Ç–∞
        rl_agent = IntelligentRLAgent(algorithm='PPO')
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—ã
            vec_env = rl_agent.create_training_environment(train_df, xlstm_model)
            rl_agent.create_evaluation_environment(eval_df, xlstm_model)
            
            # –°—Ç—Ä–æ–∏–º –∏ –æ–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞
            rl_agent.build_agent(vec_env)
            
            # –î–û–ë–ê–í–¨–¢–ï: –û–±—É—á–µ–Ω–∏–µ –º–µ–Ω—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è–º–∏
            for step in range(0, 50000, 10000):  # –ü–æ 10k —à–∞–≥–æ–≤
                print(f"RL –æ–±—É—á–µ–Ω–∏–µ: —à–∞–≥–∏ {step}-{min(step+10000, 50000)}")
                rl_agent.train_with_callbacks(
                    total_timesteps=10000,
                    eval_freq=2000
                )
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                rl_agent.save_agent(f'models/rl_agent_{symbol}_step_{step}')
                gc.collect()  # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            rl_agent.save_agent(f'models/rl_agent_{symbol}')
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ RL –¥–ª—è {symbol}: {e}")
            continue
    
    print("\n=== –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û ===")
    
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
    saved_files = [
        'models/xlstm_rl_model.keras',
        'models/xlstm_rl_scaler.pkl',
        'models/market_regime_detector.pkl'
    ]

    for file_path in saved_files:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"‚úÖ {file_path} ({size:.1f} MB)")
        else:
            print(f"‚ùå {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º RL –∞–≥–µ–Ω—Ç–æ–≤
    for symbol in rl_symbols:
        rl_path = f'models/rl_agent_{symbol}'
        if os.path.exists(rl_path + '.zip'):
            size = os.path.getsize(rl_path + '.zip') / (1024*1024)
            print(f"‚úÖ {rl_path}.zip ({size:.1f} MB)")
            
    return xlstm_model, rl_agent

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã xLSTM + VSA + RL')
    parser.add_argument('--data', type=str, default='historical_data.csv', help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º')
    parser.add_argument('--sequence_length', type=int, default=10, help='–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
    args = parser.parse_args()
    
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X, y, processed_dfs, feature_cols = prepare_xlstm_rl_data(args.data, args.sequence_length)
        
        # –û–±—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º—É
        xlstm_model, rl_agent = train_xlstm_rl_system(X, y, processed_dfs, feature_cols)
        
        print("‚úÖ –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ xLSTM + VSA + RL —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
