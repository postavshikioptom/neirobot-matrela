import os
import sys
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from datetime import datetime
# from sklearn.utils import class_weight # üî• –£–î–ê–õ–ï–ù–û: –ë–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º sample_weights
import math
import psutil  # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç psutil –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏
import gc
from collections import deque
import itertools

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
from device_config import DeviceConfig

# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
has_gpu, num_gpus = DeviceConfig.setup()

# –£–¥–æ–±–Ω–æ –≤–∫–ª—é—á–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ
tf.config.run_functions_eagerly(False) # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ False –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineering, apply_smote_to_training_data
from trading_env import TradingEnvironment
from rl_agent import RLAgent
from hybrid_decision_maker import HybridDecisionMaker
from simulation_engine import SimulationEngine
from models.xlstm_rl_model import XLSTMRLModel
import config
from validation_metrics_callback import ValidationMetricsCallback

# üî• –£–î–ê–õ–ï–ù–û: logging.basicConfig –∏ logger

class CosineDecayCallback(tf.keras.callbacks.Callback):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π Cosine Decay callback –¥–ª—è TensorFlow 2.19.0"""
    def __init__(self, initial_learning_rate, decay_steps, alpha=0.0):
        super().__init__()
        self.initial_learning_rate = initial_learning_rate
        self.decay_steps = decay_steps
        self.alpha = alpha
        self.optimizer_ref = None 
    
    def on_train_begin(self, logs=None):
        if hasattr(self.model, 'optimizer'):
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∞—Ç—Ä–∏–±—É—Ç learning_rate
            if hasattr(self.model.optimizer, 'learning_rate'):
                self.optimizer_ref = self.model.optimizer.learning_rate
            else:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ 'learning_rate' –≤ on_train_begin. –¢–∏–ø: {type(self.model.optimizer)}")
        else:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ 'optimizer' –≤ on_train_begin.")

    def on_epoch_begin(self, epoch, logs=None):
        if self.optimizer_ref is None:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è CosineDecayCallback –Ω–∞ —ç–ø–æ—Ö–µ {epoch}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ LR.")
            return

        if epoch < self.decay_steps:
            cosine_decay = 0.5 * (1 + math.cos(math.pi * epoch / self.decay_steps))
            decayed_learning_rate = (self.initial_learning_rate - self.alpha) * cosine_decay + self.alpha
            
            try:
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é –≤ –∞—Ç—Ä–∏–±—É—Ç learning_rate
                self.optimizer_ref.assign(decayed_learning_rate)
                print(f"Epoch {epoch+1}: –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {self.optimizer_ref.numpy():.6f}")
            except AttributeError as e:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ CosineDecayCallback –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}: {e}")
                print(f"–¢–∏–ø self.optimizer_ref: {type(self.optimizer_ref)}")
                print(f"–ê—Ç—Ä–∏–±—É—Ç—ã self.optimizer_ref: {dir(self.optimizer_ref)}")
            except Exception as e:
                print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ CosineDecayCallback –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}: {e}")

class ThreeStageTrainer:
    """
    –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è xLSTM + RL –º–æ–¥–µ–ª–∏
    """
    def __init__(self, data_path, has_gpu=False, num_gpus=0):
        self.data_path = data_path
        self.has_gpu = has_gpu
        self.num_gpus = num_gpus
        self.feature_eng = FeatureEngineering(sequence_length=config.SEQUENCE_LENGTH)
        self.model = None
        self.X_train_supervised = None # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        self.X_val_supervised = None   # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        self.X_test_supervised = None  # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        self.y_train_supervised = None # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        self.y_val_supervised = None   # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        self.y_test_supervised = None  # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è supervised
        
        self.X_rl_train_by_symbol = {} # üî• –î–û–ë–ê–í–õ–ï–ù–û: –î–∞–Ω–Ω—ã–µ –¥–ª—è RL, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        self.X_rl_val_by_symbol = {}   # üî• –î–û–ë–ê–í–õ–ï–ù–û: –î–∞–Ω–Ω—ã–µ –¥–ª—è RL, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs('models', exist_ok=True)
        os.makedirs('results', exist_ok=True)
        os.makedirs('plots', exist_ok=True)
    
    def _get_optimal_batch_size(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        return DeviceConfig.get_optimal_batch_size(config.SUPERVISED_BATCH_SIZE)
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤"""
        print("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è timestamp
        df = pd.read_csv(self.data_path, dtype={
            'timestamp': np.int64,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –¥–ª—è timestamp
            'open': float, 
            'high': float, 
            'low': float, 
            'close': float, 
            'volume': float, 
            'turnover': float,
            'symbol': str
        })
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø timestamp
        print(f"–¢–∏–ø timestamp: {df['timestamp'].dtype}")
        
        symbol_counts = df['symbol'].value_counts()
        valid_symbols = symbol_counts[symbol_counts >= config.MIN_ROWS_PER_SYMBOL].index.tolist()
        
        if len(valid_symbols) == 0:
            valid_symbols = symbol_counts.head(20).index.tolist()
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(valid_symbols)} —Å–∏–º–≤–æ–ª–æ–≤: {valid_symbols[:5]}...")
        
        df_filtered = df[df['symbol'].isin(valid_symbols)].copy()
        
        all_X_supervised = [] # üî• –ò–ó–ú–ï–ù–ï–ù–û
        all_y_supervised = [] # üî• –ò–ó–ú–ï–ù–ï–ù–û
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –î–ª—è RL-—ç—Ç–∞–ø–∞ –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å X –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
        X_data_for_rl = {} 
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_to_id = {s: idx for idx, s in enumerate(valid_symbols)}
        all_symbol_ids = []  # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å all_X_supervised/all_y_supervised

        for i, symbol in enumerate(valid_symbols):
            symbol_data = df_filtered[df_filtered['symbol'] == symbol].copy()
            
            if len(symbol_data) < config.SEQUENCE_LENGTH + config.FUTURE_WINDOW + 10:
                print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª {symbol}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(symbol_data)} —Å—Ç—Ä–æ–∫)")
                continue
            
            try:
                if i == 0:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫
                    X_scaled_sequences, labels = self.feature_eng.prepare_supervised_data(symbol_data)
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
                    # –∏ —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
                    
                    # üî• –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨: –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                    symbol_data_with_indicators = self.feature_eng._add_technical_indicators(symbol_data.copy())
                    
                    # –¢–µ–ø–µ—Ä—å –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤–∫–ª—é—á–∞—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã) –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
                    temp_df_for_scaling = symbol_data_with_indicators.copy()
                    for col in self.feature_eng.feature_columns:
                        temp_df_for_scaling[col] = pd.to_numeric(temp_df_for_scaling[col], errors='coerce')
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
                    scaled_data = self.feature_eng.scaler.transform(temp_df_for_scaling[self.feature_eng.feature_columns].values)
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    X_scaled_sequences, _ = self.feature_eng._create_sequences(scaled_data)
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥)
                    labels = self.feature_eng.create_trading_labels(symbol_data)
                    
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                    min_len = min(len(X_scaled_sequences), len(labels))
                    X_scaled_sequences = X_scaled_sequences[:min_len]
                    labels = labels[:min_len]
                
                if len(X_scaled_sequences) > 0:
                    all_X_supervised.append(X_scaled_sequences)
                    all_y_supervised.append(labels)
                    
                    # –°–∏–º–≤–æ–ª—å–Ω—ã–µ ID –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
                    all_symbol_ids.append(np.full(shape=(len(X_scaled_sequences),), fill_value=symbol_to_id[symbol], dtype=np.int32))
                    
                    X_data_for_rl[symbol] = X_scaled_sequences  # –¥–ª—è RL
                    
                    # –í—ã–≤–æ–¥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                    try:
                        if labels is not None and len(labels) > 0:
                            u, c = np.unique(labels, return_counts=True)
                            dist = {int(k): int(v) for k, v in zip(u, c)}
                        else:
                            pass
                    except Exception:
                        pass
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∏–º–≤–æ–ª–∞ {symbol}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º
        if all_X_supervised and all_y_supervised:
            X_supervised = np.vstack(all_X_supervised)
            y_supervised = np.concatenate(all_y_supervised)
            # –í—ã—Ä–æ–≤–Ω—è–µ–º –∏ —Å–∏–º–≤–æ–ª—ã
            symbol_ids = np.concatenate(all_symbol_ids) if all_symbol_ids else np.zeros((len(y_supervised),), dtype=np.int32)
            
            # –ê–Ω–∞–ª–∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫
            u, c = np.unique(y_supervised, return_counts=True)
            global_dist = {int(k): int(v) for k, v in zip(u, c)}
            total = y_supervised.shape[0]
            print(f"[GLOBAL LABELS] distribution: {global_dist}, total={total}")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ HOLD –≤—Å–µ –µ—â–µ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç
            hold_count = global_dist.get(1, 0)
            if hold_count / total > 0.8:
                print(f"‚ö†Ô∏è [GLOBAL WARNING] HOLD fraction is very high: {hold_count}/{total} = {hold_count/total:.2%}")
                print(f"‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å –±–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ –≤ config.py")
            
            print(f"–ò—Ç–æ–≥–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–ª—è Supervised: X={X_supervised.shape}, y={y_supervised.shape}")
            print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: SELL={np.sum(y_supervised==0)}, HOLD={np.sum(y_supervised==1)}, BUY={np.sum(y_supervised==2)}")
        else:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∏–º–≤–æ–ª–æ–≤ –≤—ã—à–µ.")
            return False
        
        # 3. –£–ª—É—á—à–µ–Ω–∏–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        # –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞:
        # –í –º–µ—Ç–æ–¥–µ load_and_prepare_data –≤ train_model.py
        # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        print(f"\n=== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–ò–ú–í–û–õ–ê–ú ===")
        print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(valid_symbols)}")
        print(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {sum(len(data) for data in all_X_supervised)}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –ø–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º:")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_labels = np.concatenate(all_y_supervised) if all_y_supervised else np.array([])
        if len(all_labels) > 0:
            unique, counts = np.unique(all_labels, return_counts=True)
            distribution = {int(u): int(c) for u, c in zip(unique, counts)}
            total = len(all_labels)
            print(f"SELL (0): {distribution.get(0, 0)} ({distribution.get(0, 0)/total:.2%})")
            print(f"HOLD (1): {distribution.get(1, 0)} ({distribution.get(1, 0)/total:.2%})")
            print(f"BUY (2): {distribution.get(2, 0)} ({distribution.get(2, 0)/total:.2%})")
        else:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        def augment_sequences_batched(X, y, factor=2, max_memory_gb=4.0):
            """–õ—ë–≥–∫–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏ –∏ –æ–ø—Ü–∏—è–º–∏ –∏–∑ config"""
            if not getattr(config, 'USE_AUGMENTATIONS', True):
                return X, y
            if len(X) == 0:
                return X, y
            
            noise_std = float(getattr(config, 'AUG_NOISE_STD', 0.01))
            max_shift = int(getattr(config, 'AUG_TIME_SHIFT', 1))
            mask_prob = float(getattr(config, 'AUG_MASK_PROB', 0.05))
            mask_max_t = int(getattr(config, 'AUG_MASK_MAX_T', 2))
            
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            if available_memory_gb < (max_memory_gb * 0.2):
                print(f"‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: available={available_memory_gb:.2f}GB, required buffer={max_memory_gb*0.2:.2f}GB")
                return X, y
            
            batch_size = min(500, max(64, len(X)//50))
            augmented_X = deque()
            augmented_y = deque()
            
            for start_idx in range(0, len(X), batch_size):
                end_idx = min(start_idx + batch_size, len(X))
                batch_X = X[start_idx:end_idx]
                batch_y = y[start_idx:end_idx]
                augmented_X.extend(batch_X)
                augmented_y.extend(batch_y)
                
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                if available_memory_gb < (max_memory_gb * 0.2):
                    print(f"‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: available={available_memory_gb:.2f}GB, required buffer={max_memory_gb*0.2:.2f}GB")
                    break
                
                for i in range(len(batch_X)):
                    x = batch_X[i].copy()
                    # 1) –ù–µ–±–æ–ª—å—à–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–¥–≤–∏–≥
                    if max_shift > 0 and x.shape[0] > 2:
                        shift = np.random.randint(-max_shift, max_shift+1)
                        if shift != 0:
                            x = np.roll(x, shift, axis=0)
                    # 2) –õ—ë–≥–∫–∏–π –≥–∞—É—Å—Å–æ–≤ —à—É–º
                    if noise_std > 0:
                        x = x + np.random.normal(0, noise_std, size=x.shape)
                    # 3) –ö—Ä–æ—à–µ—á–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–∞—Å–∫–∞
                    if np.random.rand() < mask_prob and x.shape[0] > 3:
                        t = np.random.randint(1, min(mask_max_t, x.shape[0]//4) + 1)
                        s = np.random.randint(0, x.shape[0]-t+1)
                        x[s:s+t, :] = 0.0
                    augmented_X.append(x)
                    augmented_y.append(batch_y[i])
                
                if start_idx % (batch_size * 5) == 0:
                    gc.collect()
            
            gc.collect()
            result_X = np.array(list(augmented_X))
            result_y = np.array(list(augmented_y))
            augmented_X.clear(); augmented_y.clear(); gc.collect()
            print(f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(X)} -> {len(result_X)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            return result_X, result_y
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –±–∞—Ç—á–µ–≤—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        X_supervised, y_supervised = augment_sequences_batched(X_supervised, y_supervised)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è supervised learning
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ stratify
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –¥–≤–∞ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if len(np.unique(y_supervised)) > 1:
            X_temp, self.X_test_supervised, y_temp, self.y_test_supervised = train_test_split(
                X_supervised, y_supervised, test_size=0.1, shuffle=True, random_state=42, stratify=y_supervised
            )
            if len(np.unique(y_temp)) > 1:
                self.X_train_supervised, self.X_val_supervised, self.y_train_supervised, self.y_val_supervised = train_test_split(
                    X_temp, y_temp, test_size=config.SUPERVISED_VALIDATION_SPLIT, 
                    shuffle=True, random_state=42, stratify=y_temp
                )
            else:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/val. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.")
                self.X_train_supervised, self.X_val_supervised, self.y_train_supervised, self.y_val_supervised = train_test_split(
                    X_temp, y_temp, test_size=config.SUPERVISED_VALIDATION_SPLIT, 
                    shuffle=True, random_state=42
                )
        else:
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ test. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.")
            X_temp, self.X_test_supervised, y_temp, self.y_test_supervised = train_test_split(
                X_supervised, y_supervised, test_size=0.1, shuffle=True, random_state=42
            )
            self.X_train_supervised, self.X_val_supervised, self.y_train_supervised, self.y_val_supervised = train_test_split(
                X_temp, y_temp, test_size=config.SUPERVISED_VALIDATION_SPLIT, 
                shuffle=True, random_state=42
            )
        
        print(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ (Supervised): Train={len(self.X_train_supervised)}, Val={len(self.X_val_supervised)}, Test={len(self.X_test_supervised)}") # üî• –ò–ó–ú–ï–ù–ï–ù–û

        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if config.USE_SMOTE:
            print("üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            try:
                if getattr(config, 'USE_CHUNKED_SMOTE', False):
                    from feature_engineering import apply_chunked_smote
                    self.X_train_supervised, self.y_train_supervised = apply_chunked_smote(
                        self.X_train_supervised,
                        self.y_train_supervised,
                        minority_classes=tuple(getattr(config, 'CHUNKED_SMOTE_MINORITY_CLASSES', [0,1])),
                        max_synth_per_class=getattr(config, 'CHUNKED_SMOTE_MAX_SYNTH_PER_CLASS', 15000),
                        memory_guard_gb=1.5,
                        chunk_size=2000,
                        verbose=True
                    )
                else:
                    # –¶–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: SELL=30%, HOLD=40%, BUY=30% (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—É—Å–∏–ª–∏—Ç—å minority –∫–ª–∞—Å—Å—ã)
                    target_distribution = {0: 30.0, 1: 40.0, 2: 30.0}
                    self.X_train_supervised, self.y_train_supervised = apply_smote_to_training_data(
                        self.X_train_supervised, self.y_train_supervised, target_distribution
                    )
                print(f"‚úÖ SMOTE –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                print(f"üìä –ü–æ—Å–ª–µ SMOTE: Train={len(self.X_train_supervised)}, Val={len(self.X_val_supervised)}, Test={len(self.X_test_supervised)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
                unique, counts = np.unique(self.y_train_supervised, return_counts=True)
                total = len(self.y_train_supervised)
                print("üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ SMOTE:")
                for cls, count in zip(unique, counts):
                    percentage = count / total * 100
                    print(f"   –ö–ª–∞—Å—Å {cls}: {percentage:.2f}% ({count} –æ–±—Ä–∞–∑—Ü–æ–≤)")
                    
            except Exception as e:
                print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ SMOTE: {e}")
                import traceback
                traceback.print_exc()
                print("üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ SMOTE...")
        else:
            print("‚ö†Ô∏è SMOTE –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

        print("üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ RL –¥–∞–Ω–Ω—ã—Ö...")
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è RL-–æ–±—É—á–µ–Ω–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol, data_sequences in X_data_for_rl.items():
            # –î–µ–ª–∏–º –∫–∞–∂–¥—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –¥–ª—è RL
            # (RL env –±—É–¥–µ—Ç –±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –æ—Ç—Ä–µ–∑–æ–∫ –∏–∑ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
            train_size = int(len(data_sequences) * (1 - 0.1 - config.SUPERVISED_VALIDATION_SPLIT)) # 10% test, SUPERVISED_VALIDATION_SPLIT% val
            val_size = int(len(data_sequences) * config.SUPERVISED_VALIDATION_SPLIT)
            
            self.X_rl_train_by_symbol[symbol] = data_sequences[:train_size]
            self.X_rl_val_by_symbol[symbol] = data_sequences[train_size:train_size + val_size]
            
            print(f"RL –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}: Train={len(self.X_rl_train_by_symbol[symbol])}, Val={len(self.X_rl_val_by_symbol[symbol])}")
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        print("üîÑ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä...")
        self.feature_eng.save_scaler()
        print("‚úÖ –°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: input_shape —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–ª–∏–Ω—É feature_columns –∏–∑ feature_eng
        input_shape = (config.SEQUENCE_LENGTH, len(self.feature_eng.feature_columns)) 
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏: {input_shape}")
        
        self.model = XLSTMRLModel(
            input_shape=input_shape,
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        print("üéâ === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û ===")
        return True
    
    def stage1_supervised_pretraining(self):
        """–≠–¢–ê–ü 1: Supervised Pre-training"""
        print("=== –≠–¢–ê–ü 1: SUPERVISED PRE-TRAINING ===")
        
        self.model.compile_for_supervised_learning()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ callbacks –∏–∑ –º–æ–¥–µ–ª–∏
        callbacks = self.model.get_training_callbacks(
            total_epochs=config.SUPERVISED_EPOCHS,
            patience=10
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        callbacks.append(ValidationMetricsCallback(self.X_val_supervised, self.y_val_supervised))
        
        print(f"–ù–∞—á–∏–Ω–∞–µ–º supervised –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {config.SUPERVISED_EPOCHS} —ç–ø–æ—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        batch_size = self._get_optimal_batch_size()
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        
        # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—Ç–∫–ª—é—á–∞–µ–º sample_weights, –ø–æ–ª–∞–≥–∞–µ–º—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ AFL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        # –ü—Ä–æ–±–ª–µ–º–∞: sample_weights –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞–ª–∏ —Å AFL_ALPHA, —Å–æ–∑–¥–∞–≤–∞—è –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
        # –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ AFL (Asymmetric Focal Loss) –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        print("üéØ Sample weights –æ—Ç–∫–ª—é—á–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ AFL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ config –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
        sample_weights_base = None  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞

        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –Ø–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y_train_processed = np.array(self.y_train_supervised, dtype=np.int32)
        y_val_processed = np.array(self.y_val_supervised, dtype=np.int32)
        

        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è class-balanced batching, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        train_X = self.X_train_supervised
        train_y = y_train_processed
        val_X = self.X_val_supervised
        val_y = y_val_processed

        if getattr(config, 'CLASS_BALANCED_BATCHING', False):
            print("üîÑ –í–∫–ª—é—á–µ–Ω class-balanced batching –ø–æ TARGET_CLASS_RATIOS")
            import math
            ratios = getattr(config, 'TARGET_CLASS_RATIOS', [0.3, 0.3, 0.4])

            # –†–∞–∑–±–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ –∫–ª–∞—Å—Å–∞–º
            idx_sell = np.where(train_y == 0)[0]
            idx_hold = np.where(train_y == 1)[0]
            idx_buy  = np.where(train_y == 2)[0]
            rng = np.random.default_rng(42)

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
            symbol_ids_train = symbol_ids[:len(train_y)] if 'symbol_ids' in locals() else np.zeros_like(train_y)
            symbol_to_indices = {}
            if getattr(config, 'SYMBOL_STRATIFIED_BATCHING', False):
                for sid in np.unique(symbol_ids_train):
                    symbol_to_indices[int(sid)] = np.where(symbol_ids_train == sid)[0]
                # –ñ–µ–ª–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–∏–º–≤–æ–ª—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ
                uniq_sids = list(symbol_to_indices.keys())
                sid_cycle = itertools.cycle(uniq_sids) if len(uniq_sids) > 0 else None

            # Hard Negative Mining (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç—Ä—É–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è SELL/HOLD)
            hard_sell = None
            hard_hold = None
            if getattr(config, 'USE_HARD_NEGATIVE_MINING', False):
                try:
                    print("üîé –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ hard negatives –ø–æ loss")
                    probs = self.model.actor_model.predict(train_X, verbose=0)
                    true_prob = np.clip(probs[np.arange(len(train_y)), train_y], 1e-9, 1.0)
                    losses = -np.log(true_prob)
                    frac = float(getattr(config, 'HNM_TOP_K_FRACTION', 0.05))
                    k_sell = max(1, int(len(idx_sell) * frac)) if len(idx_sell) > 0 else 0
                    k_hold = max(1, int(len(idx_hold) * frac)) if len(idx_hold) > 0 else 0
                    if k_sell > 0:
                        hard_sell = idx_sell[np.argsort(losses[idx_sell])[-k_sell:]]
                    if k_hold > 0:
                        hard_hold = idx_hold[np.argsort(losses[idx_hold])[-k_hold:]]
                    print(f"‚úÖ HNM: SELL hard={0 if hard_sell is None else len(hard_sell)}, HOLD hard={0 if hard_hold is None else len(hard_hold)}")
                except Exception as e:
                    print(f"‚ö†Ô∏è HNM –æ—Ç–∫–ª—é—á–µ–Ω: {e}")
                    hard_sell, hard_hold = None, None

            def balanced_batch_generator(X, y, batch_size):
                q = (np.array(ratios) / np.sum(ratios)).astype(float)
                per_class = np.maximum(1, (q * batch_size).astype(int))
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Å—É–º–º—É
                diff = batch_size - per_class.sum()
                if diff != 0:
                    per_class[np.argmax(q)] += diff
                pools = [idx_sell.copy(), idx_hold.copy(), idx_buy.copy()]

                # –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∏ hard-negative –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞
                hard_start = float(getattr(config, 'HNM_HARD_SAMPLE_START', 0.20))
                hard_end = float(getattr(config, 'HNM_HARD_SAMPLE_END', 0.50))
                warm_epochs = max(1, int(getattr(config, 'AFL_WARMUP_EPOCHS', 5)))
                update_period = max(1, int(getattr(config, 'HNM_UPDATE_PERIOD', 5)))

                current_epoch = 0
                last_update_epoch = -1

                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ–º hard-–ø—É–ª—ã —Ç–æ–ª—å–∫–æ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ "—ç–ø–æ—Ö–∏" –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
                steps_per_epoch_local = max(1, math.ceil(len(y) / max(1, batch_size)))
                batch_counter = 0

                while True:
                    # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è hnm_ratio –≤ –ø–µ—Ä–≤—ã–µ warm_epochs
                    t = min(1.0, current_epoch / float(warm_epochs))
                    hnm_ratio = hard_start * (1.0 - t) + hard_end * t

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ hard-–ø—É–ª–æ–≤ –Ω–µ —á–∞—â–µ, —á–µ–º —Ä–∞–∑ –≤ update_period –≠–ü–û–• (–∞ –Ω–µ –∫–∞–∂–¥—ã–µ N –±–∞—Ç—á–µ–π)
                    if (getattr(config, 'USE_HARD_NEGATIVE_MINING', False)
                        and (batch_counter % steps_per_epoch_local == 0)
                        and (current_epoch - last_update_epoch >= update_period)):
                        try:
                            probs = self.model.actor_model.predict(X, verbose=0)
                            true_prob = np.clip(probs[np.arange(len(y)), y], 1e-9, 1.0)
                            losses = -np.log(true_prob)
                            frac = float(getattr(config, 'HNM_TOP_K_FRACTION', 0.05))
                            if len(idx_sell) > 0:
                                k_sell = max(1, int(len(idx_sell) * frac))
                                nonlocal hard_sell
                                hard_sell = idx_sell[np.argsort(losses[idx_sell])[-k_sell:]]
                            if len(idx_hold) > 0:
                                k_hold = max(1, int(len(idx_hold) * frac))
                                nonlocal hard_hold
                                hard_hold = idx_hold[np.argsort(losses[idx_hold])[-k_hold:]]
                            last_update_epoch = current_epoch
                        except Exception as e:
                            print(f"‚ö†Ô∏è HNM update skipped: {e}")

                    batch_idx = []
                    if getattr(config, 'SYMBOL_STRATIFIED_BATCHING', False) and 'sid_cycle' in locals() and sid_cycle is not None:
                        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º: –Ω–∞–±–∏—Ä–∞–µ–º –º–∏–Ω–∏-–≥—Ä—É–ø–ø—ã –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Å—Ö–µ–º–∞: —Ä–∞–≤–Ω—ã–µ –¥–æ–ª–∏ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –±–∞—Ç—á
                        uniq_sids = list(symbol_to_indices.keys())
                        per_symbol = max(1, batch_size // max(1, len(uniq_sids)))
                        chosen_indices = []
                        for _ in range(max(1, len(uniq_sids))):
                            sid = next(sid_cycle)
                            sid_pool = symbol_to_indices.get(int(sid), np.arange(len(y)))
                            if len(sid_pool) == 0:
                                continue
                            # –≤–Ω—É—Ç—Ä–∏ —Å–∏–º–≤–æ–ª–∞ ‚Äî —Å–æ–±–ª—é–¥–∞–µ–º –∫–ª–∞—Å—Å–æ–≤—ã–µ –¥–æ–ª–∏
                            sid_sel = []
                            for cls, need in enumerate(per_class):
                                pool = np.intersect1d(pools[cls], sid_pool, assume_unique=False)
                                if len(pool) == 0:
                                    continue
                                take = max(1, int(round(need * (per_symbol / float(batch_size)))))
                                take = min(take, len(pool))
                                sid_sel.append(rng.choice(pool, size=take, replace=False))
                            if sid_sel:
                                chosen_indices.append(np.concatenate(sid_sel))
                        if chosen_indices:
                            batch_idx = np.concatenate(chosen_indices)
                        else:
                            # fallback: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–∑ —Å–∏–º–≤–æ–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            for cls, need in enumerate(per_class):
                                pool = pools[cls]
                                if len(pool) < need:
                                    chosen_pool = pool if len(pool) > 0 else np.arange(len(y))
                                    chosen = rng.choice(chosen_pool, size=need, replace=True)
                                else:
                                    hard_pool = hard_sell if cls == 0 else (hard_hold if cls == 1 else None)
                                    if hard_pool is not None and len(hard_pool) > 0:
                                        h = max(1, int(round(need * hnm_ratio)))
                                        h = min(h, len(hard_pool))
                                        h_idx = rng.choice(hard_pool, size=h, replace=False)
                                        rest = need - h
                                        rest_idx = rng.choice(pool, size=rest, replace=False)
                                        chosen = np.concatenate([h_idx, rest_idx])
                                    else:
                                        chosen = rng.choice(pool, size=need, replace=False)
                                batch_idx.append(chosen)
                            batch_idx = np.concatenate(batch_idx)
                    else:
                        # –û–±—ã—á–Ω—ã–π –∫–ª–∞—Å—Å-—Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–±–æ—Ä
                        for cls, need in enumerate(per_class):
                            pool = pools[cls]
                            if len(pool) < need:
                                # —Ä–µ—Å–µ–º–ø–ª —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º
                                chosen_pool = pool if len(pool) > 0 else np.arange(len(y))
                                chosen = rng.choice(chosen_pool, size=need, replace=True)
                            else:
                                # —á–∞—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤ –±–µ—Ä–µ–º –∏–∑ hard-–ø—É–ª–∞, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –∫–ª–∞—Å—Å SELL/HOLD
                                hard_pool = hard_sell if cls == 0 else (hard_hold if cls == 1 else None)
                                if hard_pool is not None and len(hard_pool) > 0:
                                    h = max(1, int(round(need * hnm_ratio)))
                                    h = min(h, len(hard_pool))
                                    h_idx = rng.choice(hard_pool, size=h, replace=False)
                                    rest = need - h
                                    rest_idx = rng.choice(pool, size=rest, replace=False)
                                    chosen = np.concatenate([h_idx, rest_idx])
                                else:
                                    chosen = rng.choice(pool, size=need, replace=False)
                            batch_idx.append(chosen)
                        batch_idx = np.concatenate(batch_idx)

                    rng.shuffle(batch_idx)

                    # –°—á–∏—Ç–∞–µ–º –±–∞—Ç—á–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —ç–ø–æ—Ö —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ steps_per_epoch_local –±–∞—Ç—á–µ–π
                    batch_counter += 1
                    if batch_counter % steps_per_epoch_local == 0:
                        current_epoch += 1

                    yield X[batch_idx], y[batch_idx]

            steps_per_epoch = math.ceil(len(train_X) / batch_size)
            train_data = balanced_batch_generator(train_X, train_y, batch_size)
            validation_data = (val_X, val_y)
            fit_kwargs = dict(x=train_data, steps_per_epoch=steps_per_epoch, validation_data=validation_data)
            sample_weight_arg = None
        else:
            fit_kwargs = dict(x=train_X, y=train_y, validation_data=(val_X, val_y), batch_size=batch_size)
            sample_weight_arg = sample_weights_base

        history = self.model.actor_model.fit(
            epochs=config.SUPERVISED_EPOCHS,
            callbacks=callbacks,
            verbose=1,
            sample_weight=sample_weight_arg,
            **fit_kwargs
        )
        
        print("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ SUPERVISED –û–ë–£–ß–ï–ù–ò–Ø ===")
        
        # === –í–∞–ª–∏–¥–∞—Ü–∏—è —Å TTA –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã ===
        def _moving_average_3(x):
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤–¥–æ–ª—å –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏
            # x: (n, T, F)
            if x.shape[1] < 3:
                return x
            x_pad = np.pad(x, ((0,0),(1,1),(0,0)), mode='edge')
            return (x_pad[:, :-2, :] + 2*x_pad[:, 1:-1, :] + x_pad[:, 2:, :]) / 4.0
        
        def _zscore_window(x):
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É –æ–±—Ä–∞–∑—Ü—É (–ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ñ–∏—á–∞–º), –±–µ–∑ —É—Ç–µ—á–∫–∏ –º–µ–∂–¥—É —Å—ç–º–ø–ª–∞–º–∏
            mean = x.mean(axis=(1,2), keepdims=True) if x.ndim==3 else x.mean(axis=0, keepdims=True)
            std = x.std(axis=(1,2), keepdims=True) + 1e-6 if x.ndim==3 else x.std(axis=0, keepdims=True) + 1e-6
            return (x - mean) / std
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã)
        val_probs = self.model.actor_model.predict(val_X, verbose=0)
        if getattr(config, 'USE_TTA_VALIDATION', False):
            print("üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º TTA –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            tta_list = getattr(config, 'TTA_TRANSFORMS', ['identity'])
            prob_stack = [val_probs]
            for t in tta_list:
                if t == 'identity':
                    continue
                elif t == 'zscore_window':
                    X_t = _zscore_window(val_X)
                elif t == 'gaussian_smooth':
                    X_t = _moving_average_3(val_X)
                else:
                    continue
                prob_stack.append(self.model.actor_model.predict(X_t, verbose=0))
            val_probs = np.mean(prob_stack, axis=0)
        
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–ø–æ–¥–±–æ—Ä T –ø–æ NLL –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
        best_T = 1.0
        if getattr(config, 'USE_TEMPERATURE_SCALING', False):
            print("üîß –ü–æ–¥–±–∏—Ä–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            def softmax_logits_scaled(probs, T):
                logits = np.log(np.clip(probs, 1e-7, 1-1e-7))
                z = logits / T
                z = z - z.max(axis=1, keepdims=True)
                ez = np.exp(z)
                return ez / ez.sum(axis=1, keepdims=True)
            def nll(probs, y):
                p = np.clip(probs[np.arange(len(y)), y], 1e-7, 1-1e-7)
                return -np.mean(np.log(p))
            grid = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
            best_val = float('inf')
            for T in grid:
                scaled = softmax_logits_scaled(val_probs, T)
                loss = nll(scaled, val_y)
                if loss < best_val:
                    best_val = loss
                    best_T = T
            print(f"‚úÖ –õ—É—á—à–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: T={best_T}")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ + TTA + –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        test_probs = self.model.actor_model.predict(self.X_test_supervised, verbose=0)
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ TTA –Ω–∞ —Ç–µ—Å—Ç–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Å–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π, —á—Ç–æ –∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
        if getattr(config, 'USE_TTA_VALIDATION', False):  # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–ª–∞–≥ USE_TTA_TEST
            prob_stack_test = [test_probs]
            tta_list = getattr(config, 'TTA_TRANSFORMS', ['identity'])
            for t in tta_list:
                if t == 'identity':
                    continue
                elif t == 'zscore_window':
                    X_t = _zscore_window(self.X_test_supervised)
                elif t == 'gaussian_smooth':
                    X_t = _moving_average_3(self.X_test_supervised)
                else:
                    continue
                prob_stack_test.append(self.model.actor_model.predict(X_t, verbose=0))
            test_probs = np.mean(prob_stack_test, axis=0)
        
        # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        if getattr(config, 'USE_TEMPERATURE_SCALING', False) and best_T != 1.0:
            logits = np.log(np.clip(test_probs, 1e-7, 1-1e-7))
            z = logits / best_T
            z = z - z.max(axis=1, keepdims=True)
            ez = np.exp(z)
            test_probs = ez / ez.sum(axis=1, keepdims=True)
        
        y_pred = np.argmax(test_probs, axis=1)
        
        accuracy = accuracy_score(self.y_test_supervised, y_pred) # üî• –ò–ó–ú–ï–ù–ï–ù–û
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {accuracy:.4f}")
        
        class_names = ['SELL', 'HOLD', 'BUY']
        report = classification_report(self.y_test_supervised, y_pred, target_names=class_names, labels=[0, 1, 2], zero_division=0) # üî• –ò–ó–ú–ï–ù–ï–ù–û
        print(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report}")
        
        cm = confusion_matrix(self.y_test_supervised, y_pred, labels=[0, 1, 2]) # üî• –ò–ó–ú–ï–ù–ï–ù–û
        print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã:\n{cm}")
        
        pred_dist = np.bincount(y_pred, minlength=3)
        total_pred = len(y_pred)
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        print(f"SELL: {pred_dist[0]} ({pred_dist[0]/total_pred:.2%})")
        print(f"HOLD: {pred_dist[1]} ({pred_dist[1]/total_pred:.2%})")
        print(f"BUY: {pred_dist[2]} ({pred_dist[2]/total_pred:.2%})")
        
        self.model.save(stage="_supervised")
        self.model.is_supervised_trained = True
        
        self._plot_training_history(history, "supervised")
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'history': history.history
        }
    
    def stage2_reward_model_training(self):
        """–≠–¢–ê–ü 2: Reward Model Training"""
        print("=== –≠–¢–ê–ü 2: REWARD MODEL TRAINING ===")
        
        if not self.model.is_supervised_trained:
            print("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å supervised pre-training!")
            return None
        
        self.model.compile_for_reward_modeling()
        
        print("–°–æ–∑–¥–∞—ë–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã...")
        
        rewards_train = self._generate_simulated_rewards(self.X_train_supervised, self.y_train_supervised) # üî• –ò–ó–ú–ï–ù–ï–ù–û
        rewards_val = self._generate_simulated_rewards(self.X_val_supervised, self.y_val_supervised)     # üî• –ò–ó–ú–ï–ù–ï–ù–û
        
        print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞–≥—Ä–∞–¥: Train={len(rewards_train)}, Val={len(rewards_val)}")
        print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥: Mean={np.mean(rewards_train):.4f}, Std={np.std(rewards_train):.4f}")
        
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=8, restore_best_weights=True, monitor='val_loss'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5, patience=4, monitor='val_loss'
            )
        ]
        
        history = self.model.critic_model.fit(
            self.X_train_supervised, rewards_train, # üî• –ò–ó–ú–ï–ù–ï–ù–û
            validation_data=(self.X_val_supervised, rewards_val), # üî• –ò–ó–ú–ï–ù–ï–ù–û
            epochs=config.REWARD_MODEL_EPOCHS,
            batch_size=config.REWARD_MODEL_BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        val_predictions = self.model.critic_model.predict(self.X_val_supervised, verbose=0) # üî• –ò–ó–ú–ï–ù–ï–ù–û
        correlation = np.corrcoef(rewards_val, val_predictions.flatten())[0, 1]
        
        print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏: {correlation:.4f}")
        
        self.model.save(stage="_reward_model")
        self.model.is_reward_model_trained = True
        
        self._plot_training_history(history, "reward_model")
        
        return {
            'correlation': correlation,
            'history': history.history
        }
    
    def stage3_rl_finetuning(self):
        """–≠–¢–ê–ü 3: RL Fine-tuning"""
        print("=== –≠–¢–ê–ü 3: RL FINE-TUNING ===")
        
        if not self.model.is_reward_model_trained:
            print("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å reward model training!")
            return None
        
        rl_agent = RLAgent(
            state_shape=(config.SEQUENCE_LENGTH, len(self.feature_eng.feature_columns)),
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS,
            batch_size=config.RL_BATCH_SIZE
        )
        
        rl_agent.model = self.model
        
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RL-—Å—Ä–µ–¥—ã
        train_env = TradingEnvironment(self.X_rl_train_by_symbol, sequence_length=config.SEQUENCE_LENGTH)
        val_env = TradingEnvironment(self.X_rl_val_by_symbol, sequence_length=config.SEQUENCE_LENGTH)
        
        decision_maker = HybridDecisionMaker(rl_agent)
        train_sim = SimulationEngine(train_env, decision_maker)
        val_sim = SimulationEngine(val_env, decision_maker)
        
        rl_metrics = {
            'episode_rewards': [],
            'episode_profits': [],
            'val_rewards': [],
            'val_profits': [],
            'actor_losses': [],
            'critic_losses': []
        }
        
        best_val_profit = -float('inf')
        
        print(f"–ù–∞—á–∏–Ω–∞–µ–º RL fine-tuning –Ω–∞ {config.RL_EPISODES} —ç–ø–∏–∑–æ–¥–æ–≤...")
        
        for episode in range(config.RL_EPISODES):
            print(f"RL –≠–ø–∏–∑–æ–¥ {episode+1}/{config.RL_EPISODES}")
            
            train_results = train_sim.run_simulation(episodes=1, training=True)
            episode_reward = train_results[0]['total_reward']
            episode_profit = train_results[0]['profit_percentage']
            
            rl_metrics['episode_rewards'].append(episode_reward)
            rl_metrics['episode_profits'].append(episode_profit)
            
            if (episode + 1) % 10 == 0:
                val_results = val_sim.run_simulation(episodes=1, training=False)
                val_reward = val_results[0]['total_reward']
                val_profit = val_results[0]['profit_percentage']
                
                rl_metrics['val_rewards'].append(val_reward)
                rl_metrics['val_profits'].append(val_profit)
                
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π RL-–∞–≥–µ–Ω—Ç–∞, –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –µ–º—É –Ω–∞–±–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏–π
                # –ó–¥–µ—Å—å –º—ã –±–µ—Ä–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ –ø–µ—Ä–≤–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                if len(self.X_rl_val_by_symbol) > 0:
                    first_symbol_data = next(iter(self.X_rl_val_by_symbol.values()))
                    sample_size = min(500, len(first_symbol_data))
                    action_dist = rl_agent.log_action_distribution(first_symbol_data[:sample_size])
                else:
                    action_dist = {'buy_count': 0, 'hold_count': 0, 'sell_count': 0, 'total': 0}
                
                print(f"–≠–ø–∏–∑–æ–¥ {episode+1}:")
                print(f"  –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ - –ù–∞–≥—Ä–∞–¥–∞: {episode_reward:.4f}, –ü—Ä–∏–±—ã–ª—å: {episode_profit:.2f}%")
                print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ù–∞–≥—Ä–∞–¥–∞: {val_reward:.4f}, –ü—Ä–∏–±—ã–ª—å: {val_profit:.2f}%")
                print(f"  –î–µ–π—Å—Ç–≤–∏—è - BUY: {action_dist['buy_count']}, HOLD: {action_dist['hold_count']}, SELL: {action_dist['sell_count']}")
                print(f"  Epsilon: {rl_agent.epsilon:.4f}")
                
                if val_profit > best_val_profit:
                    print(f"  –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! –ü—Ä–∏–±—ã–ª—å: {val_profit:.2f}%")
                    self.model.save(stage="_rl_finetuned")
                    best_val_profit = val_profit
        
        print("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ RL FINE-TUNING ===")
        print(f"–õ—É—á—à–∞—è –ø—Ä–∏–±—ã–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_profit:.2f}%")
        print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_rewards']):.4f}")
        print(f"–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_profits']):.2f}%")
        
        self._plot_rl_metrics(rl_metrics)
        
        return rl_metrics
    
    def _generate_simulated_rewards(self, X, y_true):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏"""
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        y_pred_probs = self.model.actor_model.predict(X, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        rewards = []
        for true_label, pred_label, pred_probs in zip(y_true, y_pred, y_pred_probs):
            if true_label == pred_label:
                # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                reward = 1.0
            else:
                # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                reward = -1.0
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = pred_probs[pred_label]
            reward *= confidence
            
            rewards.append(reward)
        
        return np.array(rewards)
    
    def _plot_training_history(self, history, stage_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –∫–ª—é—á–∞–º history"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–ª–æ–≤–∞—Ä—é –∏—Å—Ç–æ—Ä–∏–∏
        hist = getattr(history, 'history', {}) or {}
        available_keys = set(hist.keys())

        # –ü–æ—Ç–µ—Ä–∏: –∏—â–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª—é—á
        loss_candidates = ['loss', 'total_loss', 'train_loss', 'training_loss', 'supervised_loss']
        loss_key = next((k for k in loss_candidates if k in available_keys), None)
        val_loss_key = 'val_loss' if 'val_loss' in available_keys else None

        if loss_key:
            axes[0].plot(hist[loss_key], label=f'{loss_key}')
            if val_loss_key:
                axes[0].plot(hist[val_loss_key], label=f'{val_loss_key}')
            axes[0].set_title(f'{stage_name.capitalize()} Training Loss')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
        elif val_loss_key:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏
            axes[0].plot(hist[val_loss_key], label=f'{val_loss_key}')
            axes[0].set_title(f'{stage_name.capitalize()} Validation Loss')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
        else:
            # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ loss ‚Äî –Ω–µ –ø–∞–¥–∞–µ–º, –∞ —Å–æ–æ–±—â–∞–µ–º
            axes[0].text(0.5, 0.5, 'Loss data not available',
                         ha='center', va='center', transform=axes[0].transAxes)
            axes[0].set_title(f'{stage_name.capitalize()} - No Loss Data')

        # –ú–µ—Ç—Ä–∏–∫–∏: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
        if 'accuracy' in available_keys:
            axes[1].plot(hist['accuracy'], label='Training Accuracy')
            if 'val_accuracy' in available_keys:
                axes[1].plot(hist['val_accuracy'], label='Validation Accuracy')
            axes[1].set_title(f'{stage_name.capitalize()} Accuracy')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].legend()
        elif 'mae' in available_keys:
            axes[1].plot(hist['mae'], label='Training MAE')
            if 'val_mae' in available_keys:
                axes[1].plot(hist['val_mae'], label='Validation MAE')
            axes[1].set_title(f'{stage_name.capitalize()} MAE')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('MAE')
            axes[1].legend()
        else:
            axes[1].text(0.5, 0.5, 'Metric data not available',
                         ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title(f'{stage_name.capitalize()} - No Metric Data')

        plt.tight_layout()
        plt.savefig(f'plots/{stage_name}_training_history.png')
        plt.close()
    
    def _plot_rl_metrics(self, metrics):
        """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç RL –º–µ—Ç—Ä–∏–∫–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # Episode rewards
        axes[0,0].plot(metrics['episode_rewards'])
        axes[0,0].set_title('Episode Rewards')
        axes[0,0].set_xlabel('Episode')
        axes[0,0].set_ylabel('Reward')
        axes[0,0].grid(True)
        
        # Episode profits
        axes[0,1].plot(metrics['episode_profits'])
        axes[0,1].set_title('Episode Profits (%)')
        axes[0,1].set_xlabel('Episode')
        axes[0,1].set_ylabel('Profit %')
        axes[0,1].grid(True)
        
        # Validation rewards (–∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤)
        if metrics['val_rewards']:
            val_episodes = range(10, len(metrics['val_rewards']) * 10 + 1, 10)
            axes[1,0].plot(val_episodes, metrics['val_rewards'])
            axes[1,0].set_title('Validation Rewards')
            axes[1,0].set_xlabel('Episode')
            axes[1,0].set_ylabel('Reward')
            axes[1,0].grid(True)
        
        # Validation profits
        if metrics['val_profits']:
            val_episodes = range(10, len(metrics['val_profits']) * 10 + 1, 10)
            axes[1,1].plot(val_episodes, metrics['val_profits'])
            axes[1,1].set_title('Validation Profits (%)')
            axes[1,1].set_xlabel('Episode')
            axes[1,1].set_ylabel('Profit %')
            axes[1,1].grid(True)
        
        plt.tight_layout()
        plt.savefig('plots/rl_training_metrics.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def run_full_training(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        print("üöÄ –ó–ê–ü–£–°–ö –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø xLSTM + RL") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            print("üîÑ –≠—Ç–∞–ø: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            if not self.load_and_prepare_data():
                print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                return None
            print("‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            results = {}
            
            # –≠—Ç–∞–ø 1: Supervised Pre-training
            print("üîÑ –≠—Ç–∞–ø 1: Supervised Pre-training...")
            try:
                supervised_results = self.stage1_supervised_pretraining()
                if supervised_results is None:
                    print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ supervised pre-training") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                    return None
                results['supervised'] = supervised_results
                print("‚úÖ –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –Ω–∞ —ç—Ç–∞–ø–µ 1 (Supervised): {e}")
                import traceback
                traceback.print_exc()
                return None
            
            # –≠—Ç–∞–ø 2: Reward Model Training
            print("üîÑ –≠—Ç–∞–ø 2: Reward Model Training...")
            try:
                reward_results = self.stage2_reward_model_training()
                if reward_results is None:
                    print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ reward model training") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                    return None
                results['reward_model'] = reward_results
                print("‚úÖ –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –Ω–∞ —ç—Ç–∞–ø–µ 2 (Reward Model): {e}")
                import traceback
                traceback.print_exc()
                return None
            
            # –≠—Ç–∞–ø 3: RL Fine-tuning
            print("üîÑ –≠—Ç–∞–ø 3: RL Fine-tuning...")
            try:
                rl_results = self.stage3_rl_finetuning()
                if rl_results is None:
                    print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ RL fine-tuning") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
                    return None
                results['rl_finetuning'] = rl_results
                print("‚úÖ –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –Ω–∞ —ç—Ç–∞–ø–µ 3 (RL Fine-tuning): {e}")
                import traceback
                traceback.print_exc()
                return None
            
            print("‚úÖ –¢–†–Å–•–≠–¢–ê–ü–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
            return results
            
        except Exception as e:
            print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ run_full_training: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö
    data_path = "historical_data.csv"  
    if not os.path.exists(data_path):
        print(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print
        return
    
    # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    has_gpu, num_gpus = DeviceConfig.setup()
    
    # –ü–µ—Ä–µ–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö –≤ ThreeStageTrainer
    trainer = ThreeStageTrainer(data_path, has_gpu=has_gpu, num_gpus=num_gpus)
    results = trainer.run_full_training()
    
    if results:
        print("üéâ –í–°–ï –≠–¢–ê–ü–´ –û–ë–£–ß–ï–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´!") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("=== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print(f"Supervised Accuracy: {results['supervised']['accuracy']:.4f}") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print(f"Reward Model Correlation: {results['reward_model']['correlation']:.4f}") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RL Fine-tuning
        if 'rl_finetuning' in results and results['rl_finetuning'] is not True and 'episode_profits' in results['rl_finetuning'] and len(results['rl_finetuning']['episode_profits']) > 0:
            print(f"RL Final Profit: {np.mean(results['rl_finetuning']['episode_profits'][-10:]):.2f}%") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        else:
            print("RL Fine-tuning –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–∏–±—ã–ª–∏ (–≤–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏)") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
    else:
        print("‚ùå –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ò–õ–û–°–¨ –° –û–®–ò–ë–ö–ê–ú–ò!") # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.error -> print

if __name__ == "__main__":
    main()