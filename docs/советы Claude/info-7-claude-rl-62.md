üî• –¢–û–ß–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –í –ö–û–î–ï:
1. –í —Ñ–∞–π–ª–µ xlstm_rl_model.py:
# –î–û–ë–ê–í–ò–¢–¨ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤:
import tensorflow.keras.backend as K

def f1_score(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è F1-–º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è TensorFlow"""
    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    
    precision_val = precision(y_true, y_pred)
    recall_val = recall(y_true, y_pred)
    return 2*((precision_val*recall_val)/(precision_val+recall_val+K.epsilon()))

# –ò–ó–ú–ï–ù–ò–¢–¨ –º–µ—Ç–æ–¥ compile_for_supervised_learning():
def compile_for_supervised_learning(self):
    """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 1: Supervised Learning"""
    self.actor_model.compile(
        optimizer=self.supervised_optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy', f1_score]  # üî• –ò–ó–ú–ï–ù–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω–∞ f1_score
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è supervised learning")

2. –í —Ñ–∞–π–ª–µ train_model.py:
# –î–û–ë–ê–í–ò–¢–¨ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤:
import math

class CosineDecayCallback(tf.keras.callbacks.Callback):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π Cosine Decay callback –¥–ª—è TensorFlow 2.19.0"""
    def __init__(self, initial_learning_rate, decay_steps, alpha=0.0):
        super().__init__()
        self.initial_learning_rate = initial_learning_rate
        self.decay_steps = decay_steps
        self.alpha = alpha
    
    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.decay_steps:
            cosine_decay = 0.5 * (1 + math.cos(math.pi * epoch / self.decay_steps))
            decayed_learning_rate = (self.initial_learning_rate - self.alpha) * cosine_decay + self.alpha
            tf.keras.backend.set_value(self.model.optimizer.learning_rate, decayed_learning_rate)

# –ó–ê–ú–ï–ù–ò–¢–¨ callbacks –≤ –º–µ—Ç–æ–¥–µ stage1_supervised_pretraining():
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        patience=15,
        restore_best_weights=True, 
        monitor='val_accuracy',
        min_delta=0.001
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        factor=0.3,
        patience=7,
        monitor='val_loss',
        min_lr=1e-7
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'models/best_supervised_model.keras', 
        save_best_only=True, 
        monitor='val_f1_score'  # üî• –ò–ó–ú–ï–ù–ï–ù–û: –º–æ–Ω–∏—Ç–æ—Ä–∏–º F1-score
    ),
    # üî• –ó–ê–ú–ï–ù–ï–ù–û: –≤–º–µ—Å—Ç–æ CosineRestartScheduler
    CosineDecayCallback(
        initial_learning_rate=0.001,
        decay_steps=config.SUPERVISED_EPOCHS,
        alpha=1e-6
    ),
    ValidationMetricsCallback(self.X_val_supervised, self.y_val_supervised)
]

–í–°–Å! –ë–æ–ª—å—à–µ –Ω–∏–∫–∞–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.