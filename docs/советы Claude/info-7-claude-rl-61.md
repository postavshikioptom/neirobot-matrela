
–†–µ—à–µ–Ω–∏–µ:
def _add_technical_indicators(self, df):
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RSI
    if len(df) < config.RSI_PERIOD + 5:  # RSI_PERIOD = 14, –Ω—É–∂–µ–Ω –∑–∞–ø–∞—Å
        print(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RSI: {len(df)} —Å—Ç—Ä–æ–∫, –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {config.RSI_PERIOD + 5}")
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª
        df['RSI'] = 0.0
        df['MACD'] = 0.0
        df['MACDSIGNAL'] = 0.0
        df['MACDHIST'] = 0.0
        df['STOCH_K'] = 0.0
        df['STOCH_D'] = 0.0
        df['WILLR'] = 0.0
        df['AO'] = 0.0
        return df
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    for col in ['open', 'high', 'low', 'close', 'volume']:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –ø–µ—Ä–µ–¥ —Ä–∞—Å—á—ë—Ç–æ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    if df['close'].isna().sum() > len(df) * 0.5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% NaN
        print(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN –≤ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª")
        return None
    
    try:
        # RSI —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        df['RSI'] = talib.RSI(df['close'].fillna(method='ffill'), timeperiod=config.RSI_PERIOD)
        
        # MACD —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫  
        macd, macdsignal, macdhist = talib.MACD(
            df['close'].fillna(method='ffill'), 
            fastperiod=config.MACD_FASTPERIOD, 
            slowperiod=config.MACD_SLOWPERIOD, 
            signalperiod=config.MACD_SIGNALPERIOD
        )
        df['MACD'] = macd
        df['MACDSIGNAL'] = macdsignal
        df['MACDHIST'] = macdhist
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ...
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {e}")
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        for col in ['RSI', 'MACD', 'MACDSIGNAL', 'MACDHIST', 'STOCH_K', 'STOCH_D', 'WILLR', 'AO']:
            if col not in df.columns:
                df[col] = 0.0
    
    # üî• –ö–†–ò–¢–ò–ß–ù–û: –ë–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN
    df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    return df


–ü—Ä–æ–±–ª–µ–º–∞: –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è deep learning
–†–µ—à–µ–Ω–∏–µ:
# –í config.py
MIN_ROWS_PER_SYMBOL = 200  # –£–º–µ–Ω—å—à–∏—Ç—å —Å 500
MAX_SYMBOLS_FOR_TRAINING = 200  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 100
TARGET_TOTAL_ROWS = 500000  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 350000

# –í ThreeStageTrainer.load_and_prepare_data()
# –î–æ–±–∞–≤–∏—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
def augment_sequences(X, y, factor=2):
    augmented_X, augmented_y = [], []
    for i in range(len(X)):
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        augmented_X.append(X[i])
        augmented_y.append(y[i])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º (5% –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è)
        noise = np.random.normal(0, 0.05 * np.std(X[i]), X[i].shape)
        augmented_X.append(X[i] + noise)
        augmented_y.append(y[i])
    
    return np.array(augmented_X), np.array(augmented_y)

=========================

2. –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:
SELL=248, HOLD=310, BUY=182

–†–µ—à–µ–Ω–∏–µ: –£–ª—É—á—à–∏—Ç—å class weights
# –í stage1_supervised_pretraining()
from sklearn.utils.class_weight import compute_class_weight

# –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
class_weights_array = compute_class_weight(
    'balanced',
    classes=np.unique(self.y_train_supervised),
    y=self.y_train_supervised
)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è BUY (—Å–∞–º—ã–π —Ä–µ–¥–∫–∏–π)
class_weights = {}
for i, weight in enumerate(class_weights_array):
    if i == 2:  # BUY class
        class_weights[i] = weight * 1.5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 1.5 —Ä–∞–∑–∞
    else:
        class_weights[i] = weight

print(f"–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")

3. –£–ª—É—á—à–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏:
# –í XLSTMRLModel._build_actor_model()
def _build_actor_model(self):
    inputs = layers.Input(shape=self.input_shape)
    
    # üî• –î–û–ë–ê–í–õ–ï–ù–û: Batch Normalization –Ω–∞ –≤—Ö–æ–¥–µ
    x = layers.BatchNormalization()(inputs)
    
    # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π xLSTM —Å –±–æ–ª—å—à–∏–º dropout
    x = layers.RNN(XLSTMMemoryCell(units=self.memory_units, 
                                   memory_size=self.memory_size),
                  return_sequences=True)(x)
    x = layers.LayerNormalization()(x)
    x = layers.Dropout(0.3)(x)  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 0.2
    
    # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π xLSTM
    x = layers.RNN(XLSTMMemoryCell(units=self.memory_units//2,  # –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä
                                   memory_size=self.memory_size),
                  return_sequences=False)(x)
    x = layers.LayerNormalization()(x)
    x = layers.Dropout(0.3)(x)
    
    # üî• –î–û–ë–ê–í–õ–ï–ù–û: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π —Å residual connection
    dense1 = layers.Dense(128, activation='relu')(x)
    dense1 = layers.Dropout(0.2)(dense1)
    dense2 = layers.Dense(64, activation='relu')(dense1)
    
    # Residual connection
    if x.shape[-1] == 64:
        x = layers.Add()([x, dense2])
    else:
        x = dense2
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å focal loss –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    outputs = layers.Dense(3, activation='softmax')(x)
    
    model = models.Model(inputs=inputs, outputs=outputs)
    return model

4. –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è:
# –í stage1_supervised_pretraining()
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        patience=15,  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 10
        restore_best_weights=True, 
        monitor='val_accuracy',
        min_delta=0.001  # –î–æ–±–∞–≤–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        factor=0.3,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
        patience=7,  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 5
        monitor='val_loss',
        min_lr=1e-7  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π learning rate
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'models/best_supervised_model.keras', 
        save_best_only=True, 
        monitor='val_f1_score'  # –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å F1 –≤–º–µ—Å—Ç–æ accuracy
    ),
    # üî• –î–û–ë–ê–í–õ–ï–ù–û: Cosine Annealing
    tf.keras.callbacks.CosineRestartScheduler(
        first_restart_step=10,
        t_mul=2.0,
        m_mul=0.9,
        alpha=1e-6
    )
]


==========================

–†–µ—à–µ–Ω–∏–µ:
# –î–æ–±–∞–≤–∏—Ç—å –≤ ValidationMetricsCallback
def on_epoch_end(self, epoch, logs=None):
    train_acc = logs.get('accuracy', 0)
    val_acc = logs.get('val_accuracy', 0)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
    if train_acc - val_acc > 0.15:  # –†–∞–∑—Ä—ã–≤ –±–æ–ª—å—à–µ 15%
        print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}")
        print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å dropout –∏–ª–∏ regularization")

üéØ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:

–ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É RSI –≤ feature engineering
–í–ê–ñ–ù–û: –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
–í–ê–ñ–ù–û: –£–ª—É—á—à–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
–°–†–ï–î–ù–ï: –î–æ–±–∞–≤–∏—Ç—å regularization –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
–°–†–ï–î–ù–ï: –£–ª—É—á—à–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
