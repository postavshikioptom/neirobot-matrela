–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ models/xlstm_rl_model.py:
# üî• –ó–ê–ú–ï–ù–ò–¢–¨ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é f1_score —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞:
import tensorflow_addons as tfa

def compile_for_supervised_learning(self):
    """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 1: Supervised Learning"""
    self.actor_model.compile(
        optimizer=self.supervised_optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy', tfa.metrics.F1Score(num_classes=3, average='macro')]  # üî• –ü–†–ê–í–ò–õ–¨–ù–ê–Ø F1 –¥–ª—è multiclass
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è supervised learning")

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ, –µ—Å–ª–∏ tensorflow_addons –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω:
class MulticlassF1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=3, **kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes
        self.f1_scores = []
        for i in range(num_classes):
            self.f1_scores.append(tf.keras.metrics.F1Score(threshold=0.5))
    
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.argmax(y_pred, axis=-1)
        for i in range(self.num_classes):
            y_true_binary = tf.cast(tf.equal(y_true, i), tf.float32)
            y_pred_binary = tf.cast(tf.equal(y_pred, i), tf.float32)
            self.f1_scores[i].update_state(y_true_binary, y_pred_binary, sample_weight)
    
    def result(self):
        return tf.reduce_mean([f1.result() for f1 in self.f1_scores])
    
    def reset_state(self):
        for f1 in self.f1_scores:
            f1.reset_state()

2. Residual Connection - –õ–û–ì–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê ‚úÖ
–°–æ–≥–ª–∞—Å–µ–Ω! –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ RNN –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ 64.
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ models/xlstm_rl_model.py:
def _build_actor_model(self):
    # ... –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–¥ ...
    
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º residual connection
    dense1 = layers.Dense(128, activation='relu')(x)
    dense1 = layers.Dropout(0.2)(dense1)
    dense2 = layers.Dense(64, activation='relu')(dense1)
    
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∏–≤–æ–¥–∏–º x –∫ —Ä–∞–∑–º–µ—Ä—É 64 –ø–µ—Ä–µ–¥ Add
    x_resized = layers.Dense(64)(x)  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
    x = layers.Add()([x_resized, dense2])  # –¢–µ–ø–µ—Ä—å —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    outputs = layers.Dense(3, activation='softmax')(x)

3. Deprecated fillna method ‚úÖ
–ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≥–ª–∞—Å–µ–Ω! method='ffill' deprecated –≤ pandas 2.0+
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ feature_engineering.py:
def _add_technical_indicators(self, df):
    try:
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–º–µ–Ω—è–µ–º deprecated method
        df['RSI'] = talib.RSI(df['close'].ffill(), timeperiod=config.RSI_PERIOD)  # üî• –ò–ó–ú–ï–ù–ï–ù–û
        
        macd, macdsignal, macdhist = talib.MACD(
            df['close'].ffill(),  # üî• –ò–ó–ú–ï–ù–ï–ù–û
            fastperiod=config.MACD_FASTPERIOD, 
            slowperiod=config.MACD_SLOWPERIOD, 
            signalperiod=config.MACD_SIGNALPERIOD
        )
        # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ ...
        
    except Exception as e:
        # ... –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ ...
    
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN
    df = df.ffill().bfill().fillna(0)  # üî• –ò–ó–ú–ï–ù–ï–ù–û
    
    return df

4. Memory Leak –≤ predict –º–µ—Ç–æ–¥–∞—Ö ‚úÖ
–°–æ–≥–ª–∞—Å–µ–Ω! –ù—É–∂–Ω–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏.
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ models/xlstm_rl_model.py –∏ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–∞—Ö:
class XLSTMRLModel:
    def __init__(self, input_shape, memory_size=64, memory_units=128):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        self.prediction_count = 0  # üî• –î–û–ë–ê–í–õ–ï–ù–û: –°—á—ë—Ç—á–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    def predict_action(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if len(state.shape) == 2:
            state = np.expand_dims(state, axis=0)
        
        if state.dtype != np.float32:
            state = state.astype(np.float32)
        
        action_probs = self.actor_model.predict(state, verbose=0)[0]
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self.prediction_count += 1
        if self.prediction_count % 100 == 0:  # –ö–∞–∂–¥—ã–µ 100 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            tf.keras.backend.clear_session()
            print(f"–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ {self.prediction_count} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        
        return action_probs

5. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ return –≤ stage3_rl_finetuning ‚úÖ
–°–æ–≥–ª–∞—Å–µ–Ω! –ï—Å—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π return.
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ train_model.py:
def stage3_rl_finetuning(self):
    # ... –≤–µ—Å—å –∫–æ–¥ –º–µ—Ç–æ–¥–∞ ...
    
    print("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ RL FINE-TUNING ===")
    print(f"–õ—É—á—à–∞—è –ø—Ä–∏–±—ã–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_profit:.2f}%")
    print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_rewards']):.4f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_profits']):.2f}%")
    
    self._plot_rl_metrics(rl_metrics)
    
    return rl_metrics  # üî• –£–î–ê–õ–ò–¢–¨ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π return
    # return rl_metrics  # üî• –£–î–ê–õ–ï–ù–û: –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π return

6. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ XLSTMMemoryCell ‚úÖ
–°–æ–≥–ª–∞—Å–µ–Ω! –ù—É–∂–Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–∞.
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ models/xlstm_rl_model.py:
# üî• –î–û–ë–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç
try:
    from models.xlstm_memory_cell import XLSTMMemoryCell
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ XLSTMMemoryCell: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª models/xlstm_memory_cell.py —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    raise ImportError("XLSTMMemoryCell –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –í –º–µ—Ç–æ–¥–µ load():
def load(self, path='models', stage=""):
    # ... –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ ...
    
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
    try:
        if os.path.exists(actor_path) and os.path.exists(critic_path):
            self.actor_model = tf.keras.models.load_model(
                actor_path, 
                custom_objects={'XLSTMMemoryCell': XLSTMMemoryCell, 'f1_score': MulticlassF1Score}
            )
            # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
        else:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–∞–ø–∞: {stage}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
        return False

7. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ _create_sequences ‚úÖ
–°–æ–≥–ª–∞—Å–µ–Ω! –•–∞—Ä–¥–∫–æ–¥ –∏–Ω–¥–µ–∫—Å–∞ 3 –Ω–µ–±–µ–∑–æ–ø–∞—Å–µ–Ω.
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ feature_engineering.py:
def _create_sequences(self, data):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    X = []
    y_close = []
    
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ 'close'
    try:
        close_index = self.base_features.index('close')
    except ValueError:
        print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'close' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ base_features")
        close_index = 3  # Fallback –∫ –∏–Ω–¥–µ–∫—Å—É 3
    
    for i in range(len(data) - self.sequence_length):
        X.append(data[i:i+self.sequence_length])
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        y_close.append(data[i+self.sequence_length, close_index])
    
    return np.array(X), np.array(y_close)

üéØ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
–í config.py –¥–æ–±–∞–≤–∏—Ç—å:
# üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è memory leak
MEMORY_CLEANUP_FREQUENCY = 100  # –û—á–∏—Å—Ç–∫–∞ –∫–∞–∂–¥—ã–µ 100 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
MAX_PREDICTION_BATCH_SIZE = 32  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞

–í validation_metrics_callback.py –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:
def on_epoch_end(self, epoch, logs=None):
    if (epoch + 1) % 5 == 0:
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        train_acc = logs.get('accuracy', 0) if logs else 0
        val_acc = logs.get('val_accuracy', 0) if logs else 0
        
        if train_acc - val_acc > 0.15:  # –†–∞–∑—Ä—ã–≤ –±–æ–ª—å—à–µ 15%
            print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}")
            print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å dropout –∏–ª–∏ regularization")
