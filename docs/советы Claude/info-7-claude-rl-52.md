–ú–æ—è –æ—à–∏–±–∫–∞! –í—ã –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–∞–≤—ã. –í —Ñ–∞–π–ª–µ three_stage_trainer.py —è –ø–æ –æ—à–∏–±–∫–µ —É–∫–∞–∑–∞–ª market_data.csv –≤–º–µ—Å—Ç–æ historical_data.csv.
–í–æ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π three_stage_trainer.py:
import os
import sys
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print(f"GPU –Ω–∞–π–¥–µ–Ω: {physical_devices[0]}")
else:
    print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineering
from trading_env import TradingEnvironment
from rl_agent import RLAgent
from hybrid_decision_maker import HybridDecisionMaker
from simulation_engine import SimulationEngine
from models.xlstm_rl_model import XLSTMRLModel
import config
from validation_metrics_callback import ValidationMetricsCallback # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç –∫–∞–ª–ª–±—ç–∫–∞

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('three_stage_training.log')
    ]
)

logger = logging.getLogger('three_stage_trainer')

class ThreeStageTrainer:
    """
    –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è xLSTM + RL –º–æ–¥–µ–ª–∏
    """
    def __init__(self, data_path):
        self.data_path = data_path
        self.feature_eng = FeatureEngineering(sequence_length=config.SEQUENCE_LENGTH)
        self.model = None
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs('models', exist_ok=True)
        os.makedirs('results', exist_ok=True)
        os.makedirs('plots', exist_ok=True)
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤"""
        logger.info("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(self.data_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_counts = df['symbol'].value_counts()
        valid_symbols = symbol_counts[symbol_counts >= config.MIN_ROWS_PER_SYMBOL].index.tolist()
        
        if len(valid_symbols) == 0:
            valid_symbols = symbol_counts.head(20).index.tolist()
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(valid_symbols)} —Å–∏–º–≤–æ–ª–æ–≤: {valid_symbols[:5]}...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df_filtered = df[df['symbol'].isin(valid_symbols)].copy()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è supervised learning
        all_X = []
        all_y = []
        
        for i, symbol in enumerate(valid_symbols):
            symbol_data = df_filtered[df_filtered['symbol'] == symbol].copy()
            
            if len(symbol_data) < config.SEQUENCE_LENGTH + config.FUTURE_WINDOW + 10:
                logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª {symbol}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(symbol_data)} —Å—Ç—Ä–æ–∫)")
                continue
            
            try:
                # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∫–µ–π–ª–µ—Ä–∞
                if i == 0:
                    # –î–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –æ–±—É—á–∞–µ–º —Å–∫–µ–π–ª–µ—Ä –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                    X_scaled_sequences, labels = self.feature_eng.prepare_supervised_data(
                        symbol_data, 
                        threshold=config.PRICE_CHANGE_THRESHOLD,
                        future_window=config.FUTURE_WINDOW
                    )
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
                    # –∏ —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
                    # –°–Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                    temp_df_for_scaling = symbol_data.copy()
                    for col in self.feature_eng.feature_columns:
                        temp_df_for_scaling[col] = pd.to_numeric(temp_df_for_scaling[col], errors='coerce')
                    scaled_data = self.feature_eng.scaler.transform(temp_df_for_scaling[self.feature_eng.feature_columns].values)
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    X_scaled_sequences, _ = self.feature_eng._create_sequences(scaled_data)
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω
                    labels = self.feature_eng.create_trading_labels(
                        symbol_data,
                        threshold=config.PRICE_CHANGE_THRESHOLD,
                        future_window=config.FUTURE_WINDOW
                    )
                    
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                    min_len = min(len(X_scaled_sequences), len(labels))
                    X_scaled_sequences = X_scaled_sequences[:min_len]
                    labels = labels[:min_len]
                
                if len(X_scaled_sequences) > 0:
                    all_X.append(X_scaled_sequences)
                    all_y.append(labels)
                    logger.info(f"–°–∏–º–≤–æ–ª {symbol}: {len(X_scaled_sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")
                continue
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X = np.vstack(all_X)
        y = np.concatenate(all_y)
        
        logger.info(f"–ò—Ç–æ–≥–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: X={X.shape}, y={y.shape}")
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: SELL={np.sum(y==0)}, HOLD={np.sum(y==1)}, BUY={np.sum(y==2)}")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_temp, self.X_test, y_temp, self.y_test = train_test_split(
            X, y, test_size=0.1, shuffle=True, random_state=42, stratify=y
        )
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X_temp, y_temp, test_size=config.SUPERVISED_VALIDATION_SPLIT, 
            shuffle=True, random_state=42, stratify=y_temp
        )
        
        logger.info(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: Train={len(self.X_train)}, Val={len(self.X_val)}, Test={len(self.X_test)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        self.feature_eng.save_scaler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        input_shape = (config.SEQUENCE_LENGTH, X.shape[2])
        self.model = XLSTMRLModel(
            input_shape=input_shape,
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS
        )
        
        return True
    
    def stage1_supervised_pretraining(self):
        """–≠–¢–ê–ü 1: Supervised Pre-training"""
        logger.info("=== –≠–¢–ê–ü 1: SUPERVISED PRE-TRAINING ===")
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è supervised learning
        self.model.compile_for_supervised_learning()
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–±—ç–∫–∏
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=10, restore_best_weights=True, monitor='val_accuracy'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5, patience=5, monitor='val_loss'
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'models/best_supervised_model.keras', 
                save_best_only=True, monitor='val_accuracy'
            ),
            ValidationMetricsCallback(self.X_val, self.y_val) # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ö–∞–ª–ª–±—ç–∫ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º supervised –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {config.SUPERVISED_EPOCHS} —ç–ø–æ—Ö...")
        
        history = self.model.actor_model.fit(
            self.X_train, self.y_train,
            validation_data=(self.X_val, self.y_val),
            epochs=config.SUPERVISED_EPOCHS,
            batch_size=config.SUPERVISED_BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ SUPERVISED –û–ë–£–ß–ï–ù–ò–Ø ===")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
        y_pred_probs = self.model.actor_model.predict(self.X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(self.y_test, y_pred)
        logger.info(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {accuracy:.4f}")
        
        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
        class_names = ['SELL', 'HOLD', 'BUY']
        report = classification_report(self.y_test, y_pred, target_names=class_names, zero_division=0) # üî• –î–û–ë–ê–í–õ–ï–ù–û: zero_division
        logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã
        cm = confusion_matrix(self.y_test, y_pred)
        logger.info(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã:\n{cm}")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_dist = np.bincount(y_pred, minlength=3)
        total_pred = len(y_pred)
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        logger.info(f"SELL: {pred_dist[0]} ({pred_dist[0]/total_pred:.2%})")
        logger.info(f"HOLD: {pred_dist[1]} ({pred_dist[1]/total_pred:.2%})")
        logger.info(f"BUY: {pred_dist[2]} ({pred_dist[2]/total_pred:.2%})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.model.save(stage="_supervised")
        self.model.is_supervised_trained = True
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_training_history(history, "supervised")
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'history': history.history
        }
    
    def stage2_reward_model_training(self):
        """–≠–¢–ê–ü 2: Reward Model Training"""
        logger.info("=== –≠–¢–ê–ü 2: REWARD MODEL TRAINING ===")
        
        if not self.model.is_supervised_trained:
            logger.error("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å supervised pre-training!")
            return None
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è reward modeling
        self.model.compile_for_reward_modeling()
        
        # –°–æ–∑–¥–∞—ë–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–∫—Ç–æ—Ä–∞
        logger.info("–°–æ–∑–¥–∞—ë–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã...")
        
        rewards_train = self._generate_simulated_rewards(self.X_train, self.y_train)
        rewards_val = self._generate_simulated_rewards(self.X_val, self.y_val)
        
        logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞–≥—Ä–∞–¥: Train={len(rewards_train)}, Val={len(rewards_val)}")
        logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥: Mean={np.mean(rewards_train):.4f}, Std={np.std(rewards_train):.4f}")
        
        # –û–±—É—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫–∞
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=8, restore_best_weights=True, monitor='val_loss'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5, patience=4, monitor='val_loss'
            )
        ]
        
        history = self.model.critic_model.fit(
            self.X_train, rewards_train,
            validation_data=(self.X_val, rewards_val),
            epochs=config.REWARD_MODEL_EPOCHS,
            batch_size=config.REWARD_MODEL_BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∫–∞ reward model
        val_predictions = self.model.critic_model.predict(self.X_val, verbose=0)
        correlation = np.corrcoef(rewards_val, val_predictions.flatten())[0, 1]
        
        logger.info(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏: {correlation:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.model.save(stage="_reward_model")
        self.model.is_reward_model_trained = True
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_training_history(history, "reward_model")
        
        return {
            'correlation': correlation,
            'history': history.history
        }
    
    def stage3_rl_finetuning(self):
        """–≠–¢–ê–ü 3: RL Fine-tuning"""
        logger.info("=== –≠–¢–ê–ü 3: RL FINE-TUNING ===")
        
        if not self.model.is_reward_model_trained:
            logger.error("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å reward model training!")
            return None
        
        # –°–æ–∑–¥–∞—ë–º RL –∞–≥–µ–Ω—Ç–∞ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        rl_agent = RLAgent(
            state_shape=(config.SEQUENCE_LENGTH, self.X_train.shape[2]),
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS,
            batch_size=config.RL_BATCH_SIZE
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        rl_agent.model = self.model
        
        # –°–æ–∑–¥–∞—ë–º —Ç–æ—Ä–≥–æ–≤—ã–µ —Å—Ä–µ–¥—ã
        train_env = TradingEnvironment(self.X_train, sequence_length=config.SEQUENCE_LENGTH)
        val_env = TradingEnvironment(self.X_val, sequence_length=config.SEQUENCE_LENGTH)
        
        # –°–æ–∑–¥–∞—ë–º decision maker –∏ simulation engine
        decision_maker = HybridDecisionMaker(rl_agent)
        train_sim = SimulationEngine(train_env, decision_maker)
        val_sim = SimulationEngine(val_env, decision_maker)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è RL
        rl_metrics = {
            'episode_rewards': [],
            'episode_profits': [],
            'val_rewards': [],
            'val_profits': [],
            'actor_losses': [],
            'critic_losses': []
        }
        
        best_val_profit = -float('inf')
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º RL fine-tuning –Ω–∞ {config.RL_EPISODES} —ç–ø–∏–∑–æ–¥–æ–≤...")
        
        for episode in range(config.RL_EPISODES):
            logger.info(f"RL –≠–ø–∏–∑–æ–¥ {episode+1}/{config.RL_EPISODES}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_results = train_sim.run_simulation(episodes=1, training=True)
            episode_reward = train_results[0]['total_reward']
            episode_profit = train_results[0]['profit_percentage']
            
            rl_metrics['episode_rewards'].append(episode_reward)
            rl_metrics['episode_profits'].append(episode_profit)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
            if (episode + 1) % 10 == 0:
                val_results = val_sim.run_simulation(episodes=1, training=False)
                val_reward = val_results[0]['total_reward']
                val_profit = val_results[0]['profit_percentage']
                
                rl_metrics['val_rewards'].append(val_reward)
                rl_metrics['val_profits'].append(val_profit)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π
                sample_size = min(500, len(self.X_val))
                action_dist = rl_agent.log_action_distribution(self.X_val[:sample_size])
                
                logger.info(f"–≠–ø–∏–∑–æ–¥ {episode+1}:")
                logger.info(f"  –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ - –ù–∞–≥—Ä–∞–¥–∞: {episode_reward:.4f}, –ü—Ä–∏–±—ã–ª—å: {episode_profit:.2f}%")
                logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ù–∞–≥—Ä–∞–¥–∞: {val_reward:.4f}, –ü—Ä–∏–±—ã–ª—å: {val_profit:.2f}%")
                logger.info(f"  –î–µ–π—Å—Ç–≤–∏—è - BUY: {action_dist['buy_count']}, HOLD: {action_dist['hold_count']}, SELL: {action_dist['sell_count']}")
                logger.info(f"  Epsilon: {rl_agent.epsilon:.4f}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                if val_profit > best_val_profit:
                    logger.info(f"  –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! –ü—Ä–∏–±—ã–ª—å: {val_profit:.2f}%")
                    self.model.save(stage="_rl_finetuned")
                    best_val_profit = val_profit
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        logger.info("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ RL FINE-TUNING ===")
        logger.info(f"–õ—É—á—à–∞—è –ø—Ä–∏–±—ã–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_profit:.2f}%")
        logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_rewards']):.4f}")
        logger.info(f"–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å –∑–∞ —ç–ø–∏–∑–æ–¥: {np.mean(rl_metrics['episode_profits']):.2f}%")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è RL –º–µ—Ç—Ä–∏–∫
        self._plot_rl_metrics(rl_metrics)
        
        return rl_metrics
    
    def _generate_simulated_rewards(self, X, y_true):
        &quot;&quot;&quot;–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏&quot;&quot;&quot;
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        y_pred_probs &#x3D; self.model.actor_model.predict(X, verbose&#x3D;0)
        y_pred &#x3D; np.argmax(y_pred_probs, axis&#x3D;1)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        rewards &#x3D; []
        for true_label, pred_label, pred_probs in zip(y_true, y_pred, y_pred_probs):
            if true_label &#x3D;&#x3D; pred_label:
                # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                reward &#x3D; 1.0
            else:
                # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
                reward &#x3D; -1.0
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence &#x3D; pred_probs[pred_label]
            reward *&#x3D; confidence
            
            rewards.append(reward)
        
        return np.array(rewards)
    
    def _plot_training_history(self, history, stage_name):
        &quot;&quot;&quot;–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è&quot;&quot;&quot;
        fig, axes &#x3D; plt.subplots(1, 2, figsize&#x3D;(15, 5))
        
        # –ü–æ—Ç–µ—Ä–∏
        axes[0].plot(history.history[&#39;loss&#39;], label&#x3D;&#39;Training Loss&#39;)
        if &#39;val_loss&#39; in history.history:
            axes[0].plot(history.history[&#39;val_loss&#39;], label&#x3D;&#39;Validation Loss&#39;)
        axes[0].set_title(f&#39;{stage_name.capitalize()} Training Loss&#39;)
        axes[0].set_xlabel(&#39;Epoch&#39;)
        axes[0].set_ylabel(&#39;Loss&#39;)
        axes[0].legend()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        if &#39;accuracy&#x39; in history.history:
            axes[1].plot(history.history[&#39;accuracy&#x39;], label&#x3D;&#39;Training Accuracy&#39;)
            if &#39;val_accuracy&#x39; in history.history:
                axes[1].plot(history.history[&#39;val_accuracy&#39;], label&#x3D;&#39;Validation Accuracy&#39;)
            axes[1].set_title(f&#39;{stage_name.capitalize()} Accuracy&#39;)
            axes[1].set_xlabel(&#39;Epoch&#39;)
            axes[1].set_ylabel(&#39;Accuracy&#39;)
            axes[1].legend()
        elif &#39;mae&#x39; in history.history:
            axes[1].plot(history.history[&#39;mae&#39;], label&#x3D;&#39;Training MAE&#39;)
            if &#39;val_mae&#x39; in history.history:
                axes[1].plot(history.history[&#39;val_mae&#x39;], label&#x3D;&#39;Validation MAE&#39;)
            axes[1].set_title(f&#39;{stage_name.capitalize()} MAE&#39;)
            axes[1].set_xlabel(&#39;Epoch&#39;)
            axes[1].set_ylabel(&#39;MAE&#39;)
            axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(f&#39;plots&#x2F;{stage_name}_training_history.png&#39;)
        plt.close()
    
    def _plot_rl_metrics(self, metrics):
        &quot;&quot;&quot;–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç RL –º–µ—Ç—Ä–∏–∫–∏&quot;&quot;&quot;
        fig, axes &#x3D; plt.subplots(2, 2, figsize&#x3D;(12, 8))
        
        # Episode rewards
        axes[0,0].plot(metrics[&#39;episode_rewards&#39;])
        axes[0,0].set_title(&#39;Episode Rewards&#39;)
        axes[0,0].set_xlabel(&#39;Episode&#39;)
        axes[0,0].set_ylabel(&#39;Reward&#39;)
        axes[0,0].grid(True)
        
        # Episode profits
        axes[0,1].plot(metrics[&#39;episode_profits&#39;])
        axes[0,1].set_title(&#39;Episode Profits (%)&#39;)
        axes[0,1].set_xlabel(&#39;Episode&#39;)
        axes[0,1].set_ylabel(&#39;Profit %&#39;)
        axes[0,1].grid(True)
        
        # Validation rewards (–∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤)
        if metrics[&#39;val_rewards&#39;]:
            val_episodes &#x3D; range(10, len(metrics[&#39;val_rewards&#39;]) * 10 + 1, 10)
            axes[1,0].plot(val_episodes, metrics[&#39;val_rewards&#39;])
            axes[1,0].set_title(&#39;Validation Rewards&#39;)
            axes[1,0].set_xlabel(&#39;Episode&#39;)
            axes[1,0].set_ylabel(&#39;Reward&#39;)
            axes[1,0].grid(True)
        
        # Validation profits
        if metrics['val_profits']:
            val_episodes = range(10, len(metrics['val_profits']) * 10 + 1, 10)
            axes[1,1].plot(val_episodes, metrics['val_profits'])
            axes[1,1].set_title('Validation Profits (%)')
            axes[1,1].set_xlabel('Episode')
            axes[1,1].set_ylabel('Profit %') # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∞
            axes[1,1].grid(True)
        
        plt.tight_layout()
        plt.savefig('plots/rl_training_metrics.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def run_full_training(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info("üöÄ –ó–ê–ü–£–°–ö –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø xLSTM + RL")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if not self.load_and_prepare_data():
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö")
            return None
        
        results = {}
        
        # –≠—Ç–∞–ø 1: Supervised Pre-training
        supervised_results = self.stage1_supervised_pretraining()
        if supervised_results is None:
            logger.error("–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ supervised pre-training")
            return None
        results['supervised'] = supervised_results
        
        # –≠—Ç–∞–ø 2: Reward Model Training
        reward_results = self.stage2_reward_model_training()
        if reward_results is None:
            logger.error("–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ reward model training")
            return None
        results['reward_model'] = reward_results
        
        # –≠—Ç–∞–ø 3: RL Fine-tuning
        rl_results = self.stage3_rl_finetuning()
        if rl_results is None:
            logger.error("–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ RL fine-tuning")
            return None
        results['rl_finetuning'] = rl_results
        
        logger.info("‚úÖ –¢–†–Å–•–≠–¢–ê–ü–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö
    data_path = "historical_data.csv"  
    if not os.path.exists(data_path):
        logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {len(gpus)}")
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info("–ù–∞—Å—Ç—Ä–æ–µ–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏ GPU")
        except RuntimeError as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞–º—è—Ç—å GPU: {e}")
    else:
        logger.info("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")
    
    # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = ThreeStageTrainer(data_path)
    results = trainer.run_full_training()
    
    if results:
        logger.info("üéâ –í–°–ï –≠–¢–ê–ü–´ –û–ë–£–ß–ï–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´!")
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("=== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
        logger.info(f"Supervised Accuracy: {results['supervised']['accuracy']:.4f}")
        logger.info(f"Reward Model Correlation: {results['reward_model']['correlation']:.4f}")
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RL Fine-tuning
        if 'rl_finetuning' in results and results['rl_finetuning'] is not True and 'episode_profits' in results['rl_finetuning'] and len(results['rl_finetuning']['episode_profits']) > 0:
            logger.info(f"RL Final Profit: {np.mean(results['rl_finetuning']['episode_profits'][-10:]):.2f}%")
        else:
            logger.info("RL Fine-tuning –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–∏–±—ã–ª–∏ (–≤–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏)")
    else:
        logger.error("‚ùå –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ò–õ–û–°–¨ –° –û–®–ò–ë–ö–ê–ú–ò!")

if __name__ == "__main__":
    main()
