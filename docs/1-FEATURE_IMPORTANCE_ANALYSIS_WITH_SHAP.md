# Анализ Важности Признаков с Использованием SHAP

## Введение

SHAP (SHapley Additive exPlanations) - это игровая теоретическая методология для объяснения вывода любой модели машинного обучения. Она соединяет оптимальное распределение кредитов с локальными объяснениями, используя классические значения Шапли из теории игр и их связанные расширения.

Для нашей торговой модели DQN, SHAP может помочь понять, какие признаки модель действительно использует и правильно ли она учитывает сигналы от всех моделей.

## Установка SHAP

Для использования SHAP в проекте, необходимо установить библиотеку:

```bash
pip install shap
```

## Подход к анализу важности признаков

### 1. Подготовка данных

Для анализа важности признаков нам нужно:
1. Загрузить обученную модель DQN
2. Подготовить набор данных для анализа (из логов торговли)
3. Определить признаки, которые поступают на вход модели

### 2. Использование DeepExplainer

Для глубоких нейронных сетей, таких как наша DQN модель, рекомендуется использовать `DeepExplainer`:

```python
import shap
import numpy as np

# Загрузка модели
from stable_baselines3 import DQN
model = DQN.load("dqn_trading_model.zip")

# Подготовка данных для анализа
# X_train - набор признаков из логов торговли
# background - выборка для базового объяснения (обычно 100 случайных примеров)

background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]
explainer = shap.DeepExplainer(model.q_net, background)
shap_values = explainer.shap_values(X_train[:1000])  # Анализ первых 1000 примеров
```

### 3. Интерпретация результатов

SHAP возвращает значения для каждого признака, показывающие их вклад в предсказание модели. Для интерпретации результатов можно использовать:

1. **Сводной график важности признаков** - показывает среднюю абсолютную важность каждого признака
2. **Графики зависимости признаков** - показывают влияние отдельных признаков на предсказания
3. **Водопадные диаграммы** - детализируют вклад каждого признака в конкретное предсказание

## Признаки для анализа

В нашей торговой модели используются следующие признаки:

1. **Технические индикаторы**:
   - Bollinger Bands (BBL_20_2.0, BBM_20_2.0, BBU_20_2.0, BBB_20_2.0, BBP_20_2.0)
   - MACD (MACD_12_26_9, MACDh_12_26_9, MACDs_12_26_9)
   - OBV (On Balance Volume)
   - ATR (Average True Range)
   - Williams %R (WILLR_14)
   - RSI (Relative Strength Index)
   - CCI (Commodity Channel Index)
   - ADX (Average Directional Index)

2. **Результаты математических моделей**:
   - XGBoost (xgboost_prediction)
   - Kalman Filter (kalman_price, kalman_trend)
   - GPR (gpr_prediction, gpr_confidence)
   - LSTM (lstm_prediction, lstm_confidence)

## Ожидаемые результаты

Анализ SHAP должен помочь ответить на следующие вопросы:
1. Какие признаки модель использует чаще всего?
2. Правильно ли модель учитывает сигналы от всех моделей?
3. Есть ли признаки, которые имеют противоречивое влияние на предсказания?
4. Какие признаки можно исключить из модели для упрощения?

## План реализации

1. Создать скрипт для анализа важности признаков (переключиться в режим Code)
2. Интегрировать SHAP в процесс обучения модели
3. Разработать методику интерпретации результатов
4. Создать визуализации важности признаков
5. Обновить документацию системы наград

## Выводы

Анализ важности признаков с использованием SHAP позволит нам лучше понять, как наша DQN модель принимает торговые решения, и оптимизировать процесс обучения, обеспечивая более эффективное использование сигналов от всех математических моделей.

## Что сделано:
Создана документация по анализу важности признаков с использованием SHAP

Файл: docs/FEATURE_IMPORTANCE_ANALYSIS_WITH_SHAP.md
Описывает основы SHAP, подход к анализу важности признаков и план реализации
Создан скрипт для анализа важности признаков

Файл: analyze_feature_importance.py
Реализует анализ SHAP для обученной DQN модели
Генерирует визуализации важности признаков
Интегрирован анализ SHAP в процесс обучения модели

Модифицирован файл: train_model.py
Анализ запускается автоматически после каждого цикла обучения
Разработана методика интерпретации результатов SHAP

Файл: docs/SHAP_INTERPRETATION_METHOD.md
Описывает типы визуализаций, критерии оценки и рекомендации по улучшению модели
Созданы визуализации важности признаков

Скрипт автоматически генерирует графики важности признаков
Результаты сохраняются в папке shap_analysis
Добавлен анализ SHAP в документацию системы наград

Обновлен файл: docs/REWARD_SYSTEM_IMPLEMENTATION_PLAN.md
Добавлен раздел о целях, методике и интерпретации результатов анализа SHAP
Результаты
Реализация анализа важности признаков с использованием SHAP позволит:

Понять, какие признаки модель действительно использует
Проверить, правильно ли модель учитывает сигналы от всех математических моделей
Выявить возможные проблемы в обучении
Оптимизировать процесс обучения и улучшить качество торговых решений