# XLSTM Improvement Plan (v4)

В этом документе отслеживается реализация 8 улучшений стабилизации и производительности для конвейера обучения xLSTM. Каждый элемент будет отмечен галочкой после его реализации и ссылки на него будут указаны в кодах.

- [x] 1) График скорости обучения и ограничение градиента
  - Warmup+ косинусоидальное затухание с более низким пиковым LR
  - Обеспечьте clipnorm на всех оптимизаторах
  - Реализовано в: models/xlstm_rl_model.py (WarmUpCosineDecayScheduler, clipnorm в оптимизаторах)
- [x] 2) Инициализируйте смещение классификатора в соответствии с целевыми приоритетами (SELL, HOLD, BUY ≈ 0,33, 0,40, 0,33)
  - Реализовано в: models/xlstm_rl_model.py (инициализация bias слоя 'classifier' из TARGET_CLASS_RATIOS)
- [x] 3) Плавный переход CE → AFL (смешанные потери для ранних эпох)
  - Реализовано в: models/xlstm_rl_model.py (MixedLoss + EpochTracker; config: AFL_WARMUP_EPOCHS, CE_WEIGHT_START, CE_WEIGHT_END)
- [x] 4) EMA весов для стабильности проверки (обратный вызов EMA)
  - Реализовано в: models/xlstm_rl_model.py (EMAWeightsCallback + интеграция в get_training_callbacks; config: EMA_DECAY, USE_EMA_VALIDATION)
- [x] 5) Планирование жесткого негативного майнинга: начните с периода ≥5; смешайте жесткий / случайный майнинг; увеличьте 20%→50% к 15 периоду
  - Реализовано: train_model.py (balanced_batch_generator) — параметры HNM_HARD_SAMPLE_START/END, HNM_UPDATE_PERIOD, HNM_TOP_K_FRACTION; интеграция в config.py
- [x] 6) Регуляризация внутри xLSTM: dropout, recurrent_dropout, L2 в ядрах; ограничение входа в систему на раннем этапе
  - Реализовано: models/xlstm_rl_model.py — dropout после RNN-слоёв теперь берётся из config (DROPOUT_RNN1/2), L2 (WEIGHT_DECAY_L2) подхватывается из config; clipnorm уже включён в оптимизаторах.
- [x] 7 ) Незначительные дополнения: гауссовский шум, небольшие временные сдвиги, крошечная временная маска.
  - Реализовано: train_model.py — augment_sequences_batched с контролем памяти и параметрами из config (USE_AUGMENTATIONS, AUG_*).
- [x] 8) Партии с разбивкой по символам: смешайте много символов в одной партии.
  - Реализовано: train_model.py — опциональная символ-стратификация батчей при SYMBOL_STRATIFIED_BATCHING=True (равномерное включение символов внутри батча поверх класс-стратификации и HNM).
Записи:
- Не запускайте сценарии обучения/ симуляции автоматически; пользователь выполнит их извне.
- Сохраняйте локализацию изменений и их удобочитаемость; по возможности переключайте функции через config.
- отвечай всегда на русском