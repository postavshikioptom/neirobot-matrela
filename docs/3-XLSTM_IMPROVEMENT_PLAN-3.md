# План улучшений xLSTM RL модели v3.0

## Краткое резюме логов и текущего состояния
- Баланс меток (данные): SELL=28.31%, HOLD=33.86%, BUY=37.83% (X=(281060, 60, 25))
- Распределение предсказаний (на ранних эпохах): BUY ~95%, HOLD ~4%, SELL <1%
- val_loss стабилизировался (улучшение), но перекос предсказаний в BUY сохраняется.

## Внесенные изменения перед v3
- Увеличен порог для формирования меток:
  - PRICE_CHANGE_THRESHOLD: 0.008 → 0.010
  - ADAPTIVE_THRESHOLD_MIN: 0.002 → 0.003
  - ADAPTIVE_THRESHOLD_MULTIPLIER: 1.0 → 1.2
- Перенастроены sample_weight:
  - BUY: ×0.8 (снижение веса)
  - SELL: ×1.10 (усиление)
  - HOLD: ×1.05 (усиление)
- Убрано жёсткое ожидание числа признаков в модели (предупреждение «ожидалось 14» устранено).
- Dropout увеличен на +0.1 по всем слоям (выполнено ранее).
- SHAP интегрирован в simulation_engine для пост-хок анализа важности признаков.

## Анализ проблемы «95% BUY»
- Причины:
  - Лейблинг: низкая нейтральная зона → мало HOLD; BUY/SELL порог недостаточно строгий.
  - Потеря и веса: Focal Loss + статичные веса не корректируют переуверенность в BUY.
  - Батчевое распределение: модель видит реальные дисбалансы и закрепляет лёгкую стратегию.
  - SMOTE: ограничение памяти → не сработал по факту (лог: «Недостаточно памяти 1.30GB < 2.0GB»).

## 10 новых улучшений (без дублирования предыдущих планов)
1) Класс-балансировщик батчей (Class-Balanced Batch Sampler)
- Идея: формировать батчи с целевым распределением (напр. 30/30/40) без SMOTE.
- Плюсы: низкая память, стабильное обучение; Минусы: возможна небольшая потеря представительности.
- Реализация: внутри train_model при формировании tf.data.Dataset выбрать индексаторы по классам и собирать батчи по квотам.

2) Динамические веса классов по эпохам (Prediction-Aware Reweighting)
- Идея: корректировать sample_weight на следующую эпоху исходя из распределения предсказаний и ошибок текущей эпохи.
- Плюсы: адаптивно гасит завал в BUY; Минусы: усложняет цикл обучения.
- Реализация: кастомный Callback, считающий предикт-дистрибуцию и обновляющий коэффициенты.

3) Ассиметричная Focal Loss (AFL) с разными гамма/альфа по классам
- Идея: γ_BUY > γ_SELL/HOLD для более агрессивного подавления лёгких BUY.
- Плюсы: таргетно бьёт по перекосу; Минусы: tuned hyperparams.
- Реализация: модифицированный FocalLoss (per-class alpha, gamma).

4) Class-Conditional Label Smoothing
- Идея: больше сглаживание для BUY, меньше для SELL/HOLD.
- Плюсы: снижает overconfidence по BUY; Минусы: нужно аккуратно подобрать коэффициенты.
- Реализация: в лоссе использовать per-class smoothing.

5) Hard Negative Mining для SELL/HOLD
- Идея: в каждом шаге подмешивать самые сложные SELL/HOLD из валидации/трейна (по высокой потере).
- Плюсы: ускоряет обучение различению; Минусы: оверхед на отбор примеров.
- Реализация: периодическая выборка топ-К трудных семплов по loss и включение в батчи.

6) Grid/BO оптимизация порогов лейблинга оффлайн
- Идея: прогнать оффлайн сетку для PRICE_CHANGE_THRESHOLD, ADAPTIVE_* по целевому распределению и метрикам.
- Плюсы: формализует выбор порогов; Минусы: требует оффлайн просчёта.
- Реализация: отдельный скрипт, сохраняющий лучший набор в config.

7) Confidence-penalty (Entropy Regularization) на BUY
- Идея: добавлять штраф к потере за сверхуверенные BUY (низкая энтропия).
- Плюсы: снижает уверенность там, где модель «переезжает»; Минусы: балансировать уровень штрафа.
- Реализация: custom loss = focal + λ * entropy_penalty(only BUY predictions).

8) Balanced TTA валидация (Test-Time Augmentation для временных рядов)
- Идея: усреднять предсказания по лёгким трансформациям (нормализация окон, сглаживание) для снижения шумов в BUY.
- Плюсы: улучшает калибровку; Минусы: чуть медленнее инференс.
- Реализация: валидационный предикт через набор простых трансформаций.

9) Пост-тренировочная калибровка Temperature Scaling
- Идея: обучить один параметр температуры на валидации; использовать при инференсе.
- Плюсы: проще внедрить; Минусы: не меняет саму обученную модель, только вероятности.
- Реализация: подгонка T на val по NLL/ACE, сохранение T в модели/конфиге.

10) Лёгкая версия Memory-Aware SMOTE
- Идея: SMOTE по блокам (chunked) и только для меньшинств, с контролем использования памяти.
- Плюсы: даёт синтетику без OOM; Минусы: сложнее реализация, риск артефактов.
- Реализация: pipeline, который добавляет по N синтетики за итерацию и сбрасывает в mmap/диск.

## План внедрения (этапы)
Этап A (быстро, низкие риски, 2-3 часа):
- Class-Balanced Batch Sampler (#1)
- Динамические веса по эпохам (#2) — в «мягком» режиме (небольшие шаги)
- Temperature Scaling на валидации (#9)

Этап B (средняя сложность, 3-5 часов):
- Ассиметричная Focal Loss (#3)
- Class-Conditional Label Smoothing (#4)
- Hard Negative Mining (#5)

Этап C (исследовательский, 4-6 часов):
- Grid/BO поиск порогов лейблинга (#6)
- Confidence-penalty на BUY (#7)
- Balanced TTA валидация (#8)
- Memory-Aware SMOTE (#10)

## Риски и меры предосторожности
- Чрезмерное усиление SELL/HOLD может ухудшить общую точность — используем плавные коэффициенты.
- Ассиметричная Focal и штрафы требуют мониторинга F1 Macro и калибровки (ECE/NLL).
- SMOTE-генерацию делаем поэтапно и валидируем на метриках/симуляциях.

## Готовность к запуску
- Код обновлён под новые пороги и веса классов.
- Предупреждение о размерности входа устранено.
- SHAP-анализ доступен через simulation_engine.analyze_feature_importance_shap().
- Не запускаю обучение — ждём вашего сигнала. Рекомендуем сначала запустить с Этапом A.

---
Создано: $(Get-Date)
Статус: Готово к обсуждению и реализации
Приоритет: Этап A → Этап B → Этап C

## Выполнено (v3)
- [x] (#3) Ассиметричная Focal Loss с per-class alpha/gamma — реализовано в models/xlstm_rl_model.FocalLoss и подключено через config (AFL_ALPHA, AFL_GAMMA)
- [x] (#4) Class-Conditional Label Smoothing — реализовано в FocalLoss (CLASS_SMOOTHING)
- [x] (#7) Confidence-penalty на BUY — опциональная энтропийная регуляризация (ENTROPY_PENALTY_LAMBDA)
- [x] Проведена интеграция параметров лосса в compile_for_supervised_learning
- [x] (#1) Class-Balanced Batch Sampler — генератор батчей с целевыми долями классов (train_model.stage1)
- [x] (#2) Динамическая корректировка per_class_alpha по дистрибуции предсказаний (мягкий режим — Callback)
- [x] (#5) Hard Negative Mining (SELL/HOLD) — подмешивание топ-loss примеров в батчи при генерации
- [x] (#8) Balanced TTA валидация — усреднение вероятностей по трансформациям (identity, zscore_window с осями (1,2), gaussian_smooth); TTA применяется также на тесте перед температурным скейлингом
- [x] (#9) Temperature Scaling — подбор T по NLL на валидации и применение на тесте

Следующие шаги:
- [x] (#6) Оффлайн Grid/BO оптимизация порогов лейблинга — добавлен скрипт optimize_label_thresholds.py (grid по PRICE_CHANGE_THRESHOLD, ADAPTIVE_*; метрика: баланс + энтропия; вывод best и JSON)
- [x] (#10) Chunked SMOTE — реализована функция apply_chunked_smote в feature_engineering.py; train_model.py переключается на неё при USE_CHUNKED_SMOTE=True