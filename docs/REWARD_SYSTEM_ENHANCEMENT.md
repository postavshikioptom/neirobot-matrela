# Улучшение системы наград для модели DQN

## Проблема

SHAP анализ показал, что модель DQN практически не использует сигналы от других математических моделей (XGBoost, LSTM, GPR, Kalman Filter). Это может быть связано с тем, что награды за согласие с этими моделями недостаточно велики.

## Решение

Увеличить награды за сигналы математических моделей с 5x до 10x, чтобы модель больше внимания уделяла этим сигналам.

## Необходимые изменения

### 1. Модификация train_model.py

#### Увеличение веса согласия с моделями
В функции `calculate_comprehensive_reward()` изменить строку 47:
```python
# Было:
model_weight = 5.0

# Стало:
model_weight = 10.0
```

#### Увеличение веса штрафа за несогласие с моделями
В функции `calculate_comprehensive_reward()` изменить строку 122:
```python
# Было:
model_disagreement_weight = 5.0

# Стало:
model_disagreement_weight = 10.0
```

## Преимущества подхода

1. **Увеличение влияния математических моделей**: Увеличение веса наград и штрафов в 2 раза должно привести к тому, что модель будет больше учитывать сигналы от математических моделей.
2. **Сохранение баланса**: Остальные компоненты системы наград остаются без изменений, что сохраняет общий баланс системы.
3. **Простота реализации**: Изменения минимальны и легко реализуемы.