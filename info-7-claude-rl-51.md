–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–¥–µ–ª–∞–µ–º –≤–µ—Å—å –∫–æ–¥ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–π –Ω–∞—É—á–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ. –í–æ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω –∏–∑–º–µ–Ω–µ–Ω–∏–π:
üèóÔ∏è –ü–õ–ê–ù –ò–ó–ú–ï–ù–ï–ù–ò–ô –ü–û –§–ê–ô–õ–ê–ú
üìÅ 1. models/xlstm_memory_cell.py
‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô - —Ñ–∞–π–ª —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω

üìÅ 2. models/xlstm_rl_model.py
üîÑ –î–û–ë–ê–í–õ–Ø–ï–ú –ú–ï–¢–û–î–´ –î–õ–Ø –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø:
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from models.xlstm_memory_cell import XLSTMMemoryCell
import os

class XLSTMRLModel:
    """
    –ú–æ–¥–µ–ª—å xLSTM —Å RL –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ - –¢–†–Å–•–≠–¢–ê–ü–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
    """
    def __init__(self, input_shape, memory_size=64, memory_units=128):
        self.input_shape = input_shape
        self.memory_size = memory_size
        self.memory_units = memory_units
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∞–∫—Ç–æ—Ä–∞ (–ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è –æ –¥–µ–π—Å—Ç–≤–∏—è—Ö)
        self.actor_model = self._build_actor_model()
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫–∞ (–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –∞–∫—Ç–æ—Ä–∞)
        self.critic_model = self._build_critic_model()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤
        self.supervised_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
        
        # –§–ª–∞–≥–∏ —ç—Ç–∞–ø–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        self.is_supervised_trained = False
        self.is_reward_model_trained = False

    def _build_actor_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∞–∫—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π"""
        inputs = layers.Input(shape=self.input_shape)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        x = layers.LayerNormalization()(inputs)
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π xLSTM
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units, 
                                       memory_size=self.memory_size),
                      return_sequences=True)(x)
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π xLSTM
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units,
                                       memory_size=self.memory_size),
                      return_sequences=False)(x)
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dense(32, activation='relu')(x)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π (BUY, HOLD, SELL)
        outputs = layers.Dense(3, activation='softmax')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        return model

    def _build_critic_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π"""
        inputs = layers.Input(shape=self.input_shape)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        x = layers.LayerNormalization()(inputs)
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π xLSTM
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units,
                                       memory_size=self.memory_size),
                      return_sequences=True)(x)
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π xLSTM
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units,
                                       memory_size=self.memory_size),
                      return_sequences=False)(x)
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dense(32, activation='relu')(x)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è (—Å–∫–∞–ª—è—Ä)
        outputs = layers.Dense(1)(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        return model

    def compile_for_supervised_learning(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 1: Supervised Learning"""
        self.actor_model.compile(
            optimizer=self.supervised_optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è supervised learning")

    def compile_for_reward_modeling(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 2: Reward Model Training"""
        self.critic_model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è reward modeling")

    def save(self, path='models', stage=""):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞"""
        if not os.path.exists(path):
            os.makedirs(path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞
        actor_name = f'xlstm_rl_actor{stage}.keras'
        critic_name = f'xlstm_rl_critic{stage}.keras'
        
        self.actor_model.save(os.path.join(path, actor_name))
        self.critic_model.save(os.path.join(path, critic_name))
        
        print(f"–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path} (—ç—Ç–∞–ø: {stage})")

    def load(self, path='models', stage=""):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞"""
        actor_name = f'xlstm_rl_actor{stage}.keras'
        critic_name = f'xlstm_rl_critic{stage}.keras'
        
        actor_path = os.path.join(path, actor_name)
        critic_path = os.path.join(path, critic_name)
        
        if os.path.exists(actor_path) and os.path.exists(critic_path):
            self.actor_model = tf.keras.models.load_model(
                actor_path, 
                custom_objects={'XLSTMMemoryCell': XLSTMMemoryCell}
            )
            self.critic_model = tf.keras.models.load_model(
                critic_path,
                custom_objects={'XLSTMMemoryCell': XLSTMMemoryCell}
            )
            print(f"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—ç—Ç–∞–ø: {stage})")
        else:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–∞–ø–∞: {stage}")

    def predict_action(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if len(state.shape) == 2:
            state = np.expand_dims(state, axis=0)
        
        if state.dtype != np.float32:
            state = state.astype(np.float32)
        
        action_probs = self.actor_model.predict(state, verbose=0)[0]
        return action_probs

    def predict_value(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if len(state.shape) == 2:
            state = np.expand_dims(state, axis=0)
        
        if state.dtype != np.float32:
            state = state.astype(np.float32)
        
        value = self.critic_model.predict(state, verbose=0)[0]
        return value


üìÅ 3. config.py
üîÑ –î–û–ë–ê–í–õ–Ø–ï–ú –ü–ê–†–ê–ú–ï–¢–†–´ –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø:
# Bybit API credentials
BYBIT_API_KEY = "OOofB1HzYVpySyMPom"
BYBIT_API_SECRET = "e4AkAz9x1ycOMCtXKa1milmShfk61KZxJyhG"
API_URL = "https://api-demo.bybit.com"
WEBSOCKET_URL = "wss://stream.bybit.com/v5/public/linear"

# --- Trading Parameters ---
ORDER_USDT_AMOUNT = 11.0
LEVERAGE = "2"
REQUIRED_CANDLES = 100
SYMBOLS = ["SOLUSDT"]
TIMEFRAME = "1"
SEQUENCE_LENGTH = 60
REQUIRED_CANDLES = 65

# VSA, adaptive thresholds, dynamic stops, auto-optimization, notifications
VSA_ENABLED = True
ADAPTIVE_THRESHOLDS = True
DYNAMIC_STOPS = True
AUTO_OPTIMIZATION = True
NOTIFICATIONS_ENABLED = True

# xLSTM memory parameters
XLSTM_MEMORY_SIZE = 64
XLSTM_MEMORY_UNITS = 128

# VSA parameters
VSA_VOLUME_THRESHOLD = 1.5
VSA_STRENGTH_THRESHOLD = 2.0
VSA_FILTER_ENABLED = True

# Optimization parameters
OPTIMIZATION_FREQUENCY = 50
PERFORMANCE_HISTORY_SIZE = 1000

# Training parameters
MIN_ROWS_PER_SYMBOL = 500
MAX_SYMBOLS_FOR_TRAINING = 100

# üî• –ù–û–í–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
# –≠—Ç–∞–ø 1: Supervised Pre-training
SUPERVISED_EPOCHS = 30
SUPERVISED_BATCH_SIZE = 32
SUPERVISED_VALIDATION_SPLIT = 0.2

# –≠—Ç–∞–ø 2: Reward Model Training
REWARD_MODEL_EPOCHS = 20
REWARD_MODEL_BATCH_SIZE = 64

# –≠—Ç–∞–ø 3: RL Fine-tuning
RL_EPISODES = 100
RL_BATCH_SIZE = 64

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫
PRICE_CHANGE_THRESHOLD = 0.01  # 1% –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è BUY/SELL
FUTURE_WINDOW = 5  # –û–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω


üìÅ 4. feature_engineering.py
üîÑ –î–û–ë–ê–í–õ–Ø–ï–ú –ú–ï–¢–û–î–´ –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø –ú–ï–¢–û–ö:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import pickle
import os

class FeatureEngineering:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
    """
    def __init__(self, sequence_length=60):
        self.sequence_length = sequence_length
        self.scaler = StandardScaler()
        self.feature_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
        
    def prepare_data(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        """
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('timestamp')
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        for col in self.feature_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        data = df[self.feature_columns].values
        
        # –û–±—É—á–∞–µ–º —Å–∫–µ–π–ª–µ—Ä –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        scaled_data = self.scaler.fit_transform(data)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X, y_close = self._create_sequences(scaled_data)
        
        return X, y_close, df
    
    def prepare_test_data(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Å–∫–µ–π–ª–µ—Ä–∞
        """
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('timestamp')
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        for col in self.feature_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        data = df[self.feature_columns].values
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        scaled_data = self.scaler.transform(data)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X, y_close = self._create_sequences(scaled_data)
        
        return X, y_close, df
    
    def _create_sequences(self, data):
        """
        –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        X = []
        y_close = []
        
        for i in range(len(data) - self.sequence_length):
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X.append(data[i:i+self.sequence_length])
            # –¶–µ–ª–µ–≤–∞—è —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è (–¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞)
            y_close.append(data[i+self.sequence_length, 3])  # –∏–Ω–¥–µ–∫—Å 3 - —ç—Ç–æ 'close'
        
        return np.array(X), np.array(y_close)
    
    # üî• –ù–û–í–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
    def create_trading_labels(self, df, threshold=0.01, future_window=5):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ BUY/HOLD/SELL –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            threshold: –ø–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1%)
            future_window: –æ–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
            
        Returns:
            labels: –º–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ (0=SELL, 1=HOLD, 2=BUY)
        """
        df = df.sort_values('timestamp')
        prices = df['close'].values
        labels = []
        
        for i in range(len(prices) - future_window):
            current_price = prices[i]
            future_price = prices[i + future_window]
            
            price_change = (future_price - current_price) / current_price
            
            if price_change > threshold:
                labels.append(2)  # BUY
            elif price_change < -threshold:
                labels.append(0)  # SELL
            else:
                labels.append(1)  # HOLD
        
        return np.array(labels)
    
    def prepare_supervised_data(self, df, threshold=0.01, future_window=5):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è supervised learning (—ç—Ç–∞–ø 1)
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X, _, processed_df = self.prepare_data(df)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
        labels = self.create_trading_labels(processed_df, threshold, future_window)
        
        # –û–±—Ä–µ–∑–∞–µ–º X –¥–æ –¥–ª–∏–Ω—ã –º–µ—Ç–æ–∫
        min_len = min(len(X), len(labels))
        X = X[:min_len]
        labels = labels[:min_len]
        
        return X, labels
    
    def save_scaler(self, path='models'):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        """
        if not os.path.exists(path):
            os.makedirs(path)
        
        with open(os.path.join(path, 'scaler.pkl'), 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print(f"–°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}/scaler.pkl")
    
    def load_scaler(self, path='models'):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        """
        scaler_path = os.path.join(path, 'scaler.pkl')
        
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print("–°–∫–µ–π–ª–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return True
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä")
            return False


üìÅ 5. –°–û–ó–î–ê–Å–ú –ù–û–í–´–ô –§–ê–ô–õ: three_stage_trainer.py
üÜï –ì–õ–ê–í–ù–´–ô –§–ê–ô–õ –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø:
import os
import sys
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print(f"GPU –Ω–∞–π–¥–µ–Ω: {physical_devices[0]}")
else:
    print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineering
from trading_env import TradingEnvironment
from rl_agent import RLAgent
from hybrid_decision_maker import HybridDecisionMaker
from simulation_engine import SimulationEngine
from models.xlstm_rl_model import XLSTMRLModel
import config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('three_stage_training.log')
    ]
)

logger = logging.getLogger('three_stage_trainer')

class ThreeStageTrainer:
    """
    –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è xLSTM + RL –º–æ–¥–µ–ª–∏
    """
    def __init__(self, data_path):
        self.data_path = data_path
        self.feature_eng = FeatureEngineering(sequence_length=config.SEQUENCE_LENGTH)
        self.model = None
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs('models', exist_ok=True)
        os.makedirs('results', exist_ok=True)
        os.makedirs('plots', exist_ok=True)
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤"""
        logger.info("=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(self.data_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_counts = df['symbol'].value_counts()
        valid_symbols = symbol_counts[symbol_counts >= config.MIN_ROWS_PER_SYMBOL].index.tolist()
        
        if len(valid_symbols) == 0:
            valid_symbols = symbol_counts.head(20).index.tolist()
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(valid_symbols)} —Å–∏–º–≤–æ–ª–æ–≤: {valid_symbols[:5]}...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        df_filtered = df[df['symbol'].isin(valid_symbols)].copy()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è supervised learning
        all_X = []
        all_y = []
        
        for i, symbol in enumerate(valid_symbols):
            symbol_data = df_filtered[df_filtered['symbol'] == symbol].copy()
            
            if len(symbol_data) < config.SEQUENCE_LENGTH + config.FUTURE_WINDOW + 10:
                continue
            
            try:
                if i == 0:
                    X_symbol, y_symbol = self.feature_eng.prepare_supervised_data(
                        symbol_data, 
                        threshold=config.PRICE_CHANGE_THRESHOLD,
                        future_window=config.FUTURE_WINDOW
                    )
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
                    X_temp, _ = self.feature_eng.prepare_supervised_data(
                        symbol_data,
                        threshold=config.PRICE_CHANGE_THRESHOLD,
                        future_window=config.FUTURE_WINDOW
                    )
                    X_symbol = X_temp
                    y_symbol = self.feature_eng.create_trading_labels(
                        symbol_data,
                        threshold=config.PRICE_CHANGE_THRESHOLD,
                        future_window=config.FUTURE_WINDOW
                    )
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                    min_len = min(len(X_symbol), len(y_symbol))
                    X_symbol = X_symbol[:min_len]
                    y_symbol = y_symbol[:min_len]
                
                if len(X_symbol) > 0:
                    all_X.append(X_symbol)
                    all_y.append(y_symbol)
                    logger.info(f"–°–∏–º–≤–æ–ª {symbol}: {len(X_symbol)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")
                continue
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X = np.vstack(all_X)
        y = np.concatenate(all_y)
        
        logger.info(f"–ò—Ç–æ–≥–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: X={X.shape}, y={y.shape}")
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: SELL={np.sum(y==0)}, HOLD={np.sum(y==1)}, BUY={np.sum(y==2)}")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_temp, self.X_test, y_temp, self.y_test = train_test_split(
            X, y, test_size=0.1, shuffle=True, random_state=42, stratify=y
        )
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X_temp, y_temp, test_size=config.SUPERVISED_VALIDATION_SPLIT, 
            shuffle=True, random_state=42, stratify=y_temp
        )
        
        logger.info(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: Train={len(self.X_train)}, Val={len(self.X_val)}, Test={len(self.X_test)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        self.feature_eng.save_scaler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        input_shape = (config.SEQUENCE_LENGTH, X.shape[2])
        self.model = XLSTMRLModel(
            input_shape=input_shape,
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS
        )
        
        return True
    
    def stage1_supervised_pretraining(self):
        """–≠–¢–ê–ü 1: Supervised Pre-training"""
        logger.info("=== –≠–¢–ê–ü 1: SUPERVISED PRE-TRAINING ===")
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è supervised learning
        self.model.compile_for_supervised_learning()
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–±—ç–∫–∏
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=10, restore_best_weights=True, monitor='val_accuracy'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5, patience=5, monitor='val_loss'
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'models/best_supervised_model.keras', 
                save_best_only=True, monitor='val_accuracy'
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º supervised –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {config.SUPERVISED_EPOCHS} —ç–ø–æ—Ö...")
        
        history = self.model.actor_model.fit(
            self.X_train, self.y_train,
            validation_data=(self.X_val, self.y_val),
            epochs=config.SUPERVISED_EPOCHS,
            batch_size=config.SUPERVISED_BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ SUPERVISED –û–ë–£–ß–ï–ù–ò–Ø ===")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
        y_pred_probs = self.model.actor_model.predict(self.X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(self.y_test, y_pred)
        logger.info(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {accuracy:.4f}")
        
        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
        class_names = ['SELL', 'HOLD', 'BUY']
        report = classification_report(self.y_test, y_pred, target_names=class_names)
        logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:\n{report}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã
        cm = confusion_matrix(self.y_test, y_pred)
        logger.info(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã:\n{cm}")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_dist = np.bincount(y_pred, minlength=3)
        total_pred = len(y_pred)
        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        logger.info(f"SELL: {pred_dist[0]} ({pred_dist[0]/total_pred:.2%})")
        logger.info(f"HOLD: {pred_dist[1]} ({pred_dist[1]/total_pred:.2%})")
        logger.info(f"BUY: {pred_dist[2]} ({pred_dist[2]/total_pred:.2%})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.model.save(stage="_supervised")
        self.model.is_supervised_trained = True
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_training_history(history, "supervised")
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'history': history.history
        }
    
    def stage2_reward_model_training(self):
        """–≠–¢–ê–ü 2: Reward Model Training"""
        logger.info("=== –≠–¢–ê–ü 2: REWARD MODEL TRAINING ===")
        
        if not self.model.is_supervised_trained:
            logger.error("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å supervised pre-training!")
            return None
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è reward modeling
        self.model.compile_for_reward_modeling()
        
        # –°–æ–∑–¥–∞—ë–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–∫—Ç–æ—Ä–∞
        logger.info("–°–æ–∑–¥–∞—ë–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã...")
        
        rewards_train = self._generate_simulated_rewards(self.X_train, self.y_train)
        rewards_val = self._generate_simulated_rewards(self.X_val, self.y_val)
        
        logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞–≥—Ä–∞–¥: Train={len(rewards_train)}, Val={len(rewards_val)}")
        logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥: Mean={np.mean(rewards_train):.4f}, Std={np.std(rewards_train):.4f}")
        
        # –û–±—É—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫–∞
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=8, restore_best_weights=True, monitor='val_loss'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.5, patience=4, monitor='val_loss'
            )
        ]
        
        history = self.model.critic_model.fit(
            self.X_train, rewards_train,
            validation_data=(self.X_val, rewards_val),
            epochs=config.REWARD_MODEL_EPOCHS,
            batch_size=config.REWARD_MODEL_BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∫–∞ reward model
        val_predictions = self.model.critic_model.predict(self.X_val, verbose=0)
        correlation = np.corrcoef(rewards_val, val_predictions.flatten())[0, 1]
        
        logger.info(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏: {correlation:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.model.save(stage="_reward_model")
        self.model.is_reward_model_trained = True
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_training_history(history, "reward_model")
        
        return {
            'correlation': correlation,
            'history': history.history
        }
    
    def stage3_rl_finetuning(self):
        """–≠–¢–ê–ü 3: RL Fine-tuning"""
        logger.info("=== –≠–¢–ê–ü 3: RL FINE-TUNING ===")
        
        if not self.model.is_reward_model_trained:
            logger.error("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å reward model training!")
            return None
        
        # –°–æ–∑–¥–∞—ë–º RL –∞–≥–µ–Ω—Ç–∞ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        rl_agent = RLAgent(
            state_shape=(config.SEQUENCE_LENGTH, self.X_train.shape[2]),
            memory_size=config.XLSTM_MEMORY_SIZE,
            memory_units=config.XLSTM_MEMORY_UNITS,
            batch_size=config.RL_BATCH_SIZE
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        rl_agent.model = self.model
        
        import numpy as np
import tensorflow as tf
from models.xlstm_rl_model import XLSTMRLModel
import os
import logging

class RLAgent:
    """
    –ê–≥–µ–Ω—Ç Reinforcement Learning –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ - –ü–û–î–î–ï–†–ñ–ö–ê –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
    """
    def __init__(self, state_shape, memory_size=64, memory_units=128, gamma=0.99, epsilon=0.3, epsilon_min=0.1, epsilon_decay=0.995, batch_size=64):
        self.state_shape = state_shape
        self.gamma = gamma  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ù–∞—á–∞–ª—å–Ω—ã–π epsilon –Ω–∏–∂–µ –¥–ª—è fine-tuning –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.epsilon = epsilon  # –ù–∞—á–∏–Ω–∞–µ–º —Å –º–µ–Ω—å—à–µ–≥–æ epsilon –¥–ª—è fine-tuning
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = XLSTMRLModel(input_shape=state_shape, 
                                 memory_size=memory_size, 
                                 memory_units=memory_units)
        
        # –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞
        self.memory = []
        self.max_memory_size = 10000
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = logging.getLogger('rl_agent')
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def act(self, state, training=True):
        """–í—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if training and np.random.rand() < self.epsilon:
            # –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            return np.random.randint(0, 3)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –æ—Ç –º–æ–¥–µ–ª–∏ –∞–∫—Ç–æ—Ä–∞
        action_probs = self.model.predict_action(state)
        
        # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        return np.argmax(action_probs)
    
    def remember(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä"""
        self.memory.append((state, action, reward, next_state, done))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        if len(self.memory) > self.max_memory_size:
            self.memory.pop(0)
    
    –ü—Ä–æ–¥–æ–ª–∂–∞—é –∫–æ–¥ –¥–ª—è rl_agent.py, –≥–¥–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è:
    def update_epsilon(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ epsilon –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞"""
        if len(self.memory) < self.batch_size:
            return None
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –±—É—Ñ–µ—Ä–∞
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = np.array([experience[0] for experience in batch])
        actions = np.array([experience[1] for experience in batch])
        rewards = np.array([experience[2] for experience in batch])
        next_states = np.array([experience[3] for experience in batch])
        dones = np.array([experience[4] for experience in batch])
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫—Ä–∏—Ç–∏–∫–∞
        with tf.GradientTape() as tape:
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
            values = self.model.critic_model(states, training=True)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
            next_values = self.model.critic_model(next_states, training=True)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            targets = rewards + self.gamma * tf.squeeze(next_values) * (1 - dones)
            targets = tf.expand_dims(targets, axis=1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
            critic_loss = tf.reduce_mean(tf.square(targets - values))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∞
        critic_grads = tape.gradient(critic_loss, self.model.critic_model.trainable_variables)
        self.model.critic_optimizer.apply_gradients(zip(critic_grads, self.model.critic_model.trainable_variables))
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–∫—Ç–æ—Ä–∞
        with tf.GradientTape() as tape:
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π
            action_probs = self.model.actor_model(states, training=True)
            
            # –°–æ–∑–¥–∞–µ–º one-hot –≤–µ–∫—Ç–æ—Ä –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            action_masks = tf.one_hot(actions, 3)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
            values = self.model.critic_model(states, training=True)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ (advantage)
            advantages = targets - values
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∞–∫—Ç–æ—Ä–∞
            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)
            log_probs = tf.math.log(selected_action_probs + 1e-10)
            actor_loss = -tf.reduce_mean(log_probs * tf.squeeze(advantages))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —ç–Ω—Ç—Ä–æ–ø–∏–∏
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-10), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∞–∫—Ç–æ—Ä–∞
        actor_grads = tape.gradient(actor_loss, self.model.actor_model.trainable_variables)
        self.model.actor_optimizer.apply_gradients(zip(actor_grads, self.model.actor_model.trainable_variables))
        
        return {
            'critic_loss': float(critic_loss),
            'actor_loss': float(actor_loss),
            'mean_value': float(tf.reduce_mean(values)),
            'mean_reward': float(np.mean(rewards))
        }
    
    def save(self, path='models'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.save(path, stage="_rl_final")
    
    def load(self, path='models'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.load(path, stage="_rl_finetuned")
    
    def log_action_distribution(self, states):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
        if len(states) == 0:
            return {'buy_count': 0, 'hold_count': 0, 'sell_count': 0, 'total': 0}
        
        actions = []
        for state in states:
            action_probs = self.model.predict_action(state)
            actions.append(np.argmax(action_probs))
        
        actions = np.array(actions)
        buy_count = np.sum(actions == 0)
        hold_count = np.sum(actions == 1)
        sell_count = np.sum(actions == 2)
        
        total = len(actions)
        self.logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π: BUY: {buy_count/total:.2%}, HOLD: {hold_count/total:.2%}, SELL: {sell_count/total:.2%}")
        
        return {
            'buy_count': int(buy_count),
            'hold_count': int(hold_count),
            'sell_count': int(sell_count),
            'total': total
        }


üìÅ 7. –û–ë–ù–û–í–õ–Ø–ï–ú hybrid_decision_maker.py
‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô - —Ñ–∞–π–ª —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –¥–ª—è —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

üìÅ 8. –û–ë–ù–û–í–õ–Ø–ï–ú simulation_engine.py
‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô - —Ñ–∞–π–ª —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω

üìÅ 9. –û–ë–ù–û–í–õ–Ø–ï–ú trading_env.py
‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô - —Ñ–∞–π–ª —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω

üìÅ 10. –û–ë–ù–û–í–õ–Ø–ï–ú trade_manager.py
‚úÖ –û–°–¢–ê–í–õ–Ø–ï–ú –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô - —Ñ–∞–π–ª –≥–æ—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

üìÅ 11. –û–ë–ù–û–í–õ–Ø–ï–ú run_live_trading.py
üîÑ –î–û–ë–ê–í–õ–Ø–ï–ú –ü–û–î–î–ï–†–ñ–ö–£ –ó–ê–ì–†–£–ó–ö–ò –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò:
import os
import sys
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import tensorflow as tf
from pybit.unified_trading import HTTP

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from feature_engineering import FeatureEngineering
from models.xlstm_rl_model import XLSTMRLModel
from hybrid_decision_maker import HybridDecisionMaker
from trade_manager import TradeManager
from rl_agent import RLAgent
import config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('live_trading.log')
    ]
)

logger = logging.getLogger('live_trading')

def fetch_latest_data(session, symbol, timeframe, limit=100):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–≤–µ—á–∏ —Å –±–∏—Ä–∂–∏"""
    try:
        response = session.get_kline(
            category="linear",
            symbol=symbol,
            interval=timeframe,
            limit=limit
        )
        
        if response['retCode'] == 0:
            data = response['result']['list']
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
            df = pd.DataFrame(data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
            ])
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df['timestamp'] = pd.to_datetime(pd.to_numeric(df['timestamp']), unit='ms')
            df['symbol'] = symbol
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            df.sort_values('timestamp', inplace=True)
            
            return df
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {response['retMsg']}")
            return None
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∂–∏–≤–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –ñ–ò–í–û–ô –¢–û–†–ì–û–í–õ–ò –° –¢–†–Å–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–¨–Æ")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    api_key = config.BYBIT_API_KEY
    api_secret = config.BYBIT_API_SECRET
    api_url = config.API_URL
    symbol = config.SYMBOLS[0]
    timeframe = config.TIMEFRAME
    order_amount = config.ORDER_USDT_AMOUNT
    leverage = config.LEVERAGE
    sequence_length = config.SEQUENCE_LENGTH
    required_candles = config.REQUIRED_CANDLES
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API
    session = HTTP(
        testnet=(api_url == "https://api-demo.bybit.com"),
        api_key=api_key,
        api_secret=api_secret
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
    feature_engineering = FeatureEngineering(sequence_length=sequence_length)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
    if not feature_engineering.load_scaler():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∫–µ–π–ª–µ—Ä. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    input_shape = (sequence_length, len(feature_engineering.feature_columns))
    rl_model = XLSTMRLModel(input_shape=input_shape, 
                          memory_size=config.XLSTM_MEMORY_SIZE, 
                          memory_units=config.XLSTM_MEMORY_UNITS)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    try:
        rl_model.load(stage="_rl_finetuned")
        logger.info("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ supervised –º–æ–¥–µ–ª–∏...")
        try:
            rl_model.load(stage="_supervised")
            logger.info("‚úÖ Supervised –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–∞–∫ fallback")
        except Exception as e2:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏–∫–∞–∫—É—é –º–æ–¥–µ–ª—å: {e2}")
            return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RL-–∞–≥–µ–Ω—Ç–∞
    rl_agent = RLAgent(state_shape=input_shape, 
                      memory_size=config.XLSTM_MEMORY_SIZE, 
                      memory_units=config.XLSTM_MEMORY_UNITS)
    rl_agent.model = rl_model
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
    decision_maker = HybridDecisionMaker(rl_agent)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏
    trade_manager = TradeManager(
        api_key=api_key,
        api_secret=api_secret,
        api_url=api_url,
        order_amount=order_amount,
        symbol=symbol,
        leverage=leverage
    )
    
    logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é...")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ç–æ—Ä–≥–æ–≤–ª–∏
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
            current_time = datetime.now()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
            df = fetch_latest_data(session, symbol, timeframe, limit=required_candles)
            
            if df is None or len(df) < sequence_length:
                logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–ª—É—á–µ–Ω–æ: {len(df) if df is not None else 0} —Å—Ç—Ä–æ–∫")
                time.sleep(10)
                continue
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X, _, _ = feature_engineering.prepare_test_data(df)
            
            if len(X) == 0:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                time.sleep(10)
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞
            current_state = X[-1]
            
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ (–ø–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é —Ç—Ä–µ–π–¥-–º–µ–Ω–µ–¥–∂–µ—Ä–∞)
            action, confidence = decision_maker.make_decision(
                current_state,
                position=trade_manager.position
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ
            action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
            logger.info(f"üìä –†–µ—à–µ–Ω–∏–µ: {action_names[action]} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.4f})")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è
            explanation = decision_maker.explain_decision(current_state)
            logger.info(f"üß† –ê–Ω–∞–ª–∏–∑: BUY={explanation['all_probs']['BUY']:.3f}, "
                       f"HOLD={explanation['all_probs']['HOLD']:.3f}, "
                       f"SELL={explanation['all_probs']['SELL']:.3f}, "
                       f"Value={explanation['state_value']:.4f}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            if trade_manager.place_order(action):
                logger.info(f"‚úÖ –û—Ä–¥–µ—Ä —Ä–∞–∑–º–µ—â–µ–Ω: {action_names[action]}")
            else:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å –æ—Ä–¥–µ—Ä: {action_names[action]}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏
            position_info = trade_manager.get_position_info()
            if position_info and position_info['size'] > 0:
                logger.info(f"üí∞ –ü–æ–∑–∏—Ü–∏—è: {position_info['side']} {position_info['size']}, "
                           f"PnL: {position_info['unrealised_pnl']}")
            
            # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–µ–π
            time.sleep(30)
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –¢–æ—Ä–≥–æ–≤–ª—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç–æ—Ä–≥–æ–≤–ª–∏: {e}")
            time.sleep(10)

if __name__ == "__main__":
    main()


üìÅ 12. –£–î–ê–õ–Ø–ï–ú –°–¢–ê–†–´–ï –§–ê–ô–õ–´
üóëÔ∏è –ú–û–ñ–ù–û –£–î–ê–õ–ò–¢–¨:

simple_train_model.py (–∑–∞–º–µ–Ω–µ–Ω –Ω–∞ `three_stage_trainer.py
–ü—Ä–æ–¥–æ–ª–∂–∞—é —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:
üóëÔ∏è –ú–û–ñ–ù–û –£–î–ê–õ–ò–¢–¨:

simple_train_model.py (–∑–∞–º–µ–Ω–µ–Ω –Ω–∞ three_stage_trainer.py)
train_model.py (–∑–∞–º–µ–Ω–µ–Ω –Ω–∞ three_stage_trainer.py)


üìÅ 13. –°–û–ó–î–ê–Å–ú –ù–û–í–´–ô –§–ê–ô–õ: validation_metrics_callback.py
üÜï –ö–ê–õ–õ–ë–≠–ö –î–õ–Ø –î–ï–¢–ê–õ–¨–ù–û–ì–û –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ù–ê –≠–¢–ê–ü–ï 1:
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import logging

logger = logging.getLogger('validation_callback')

class ValidationMetricsCallback(tf.keras.callbacks.Callback):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è Supervised Pre-training (–≠—Ç–∞–ø 1)
    """
    def __init__(self, X_val, y_val, class_names=['SELL', 'HOLD', 'BUY']):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.class_names = class_names
        
    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 5 == 0:  # –ö–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            logger.info(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}:")
            
            y_pred_probs = self.model.predict(self.X_val, verbose=0)
            y_pred_classes = np.argmax(y_pred_probs, axis=1)
            
            # –ï—Å–ª–∏ y_val —É–∂–µ one-hot, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º
            if self.y_val.ndim > 1 and self.y_val.shape[1] > 1:
                y_true_classes = np.argmax(self.y_val, axis=1)
            else:
                y_true_classes = self.y_val
            
            # Confusion Matrix
            cm = confusion_matrix(y_true_classes, y_pred_classes)
            logger.info("Confusion Matrix:")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã
            header = "     " + " ".join([f"{name:4s}" for name in self.class_names])
            logger.info(header)
            for i, row in enumerate(cm):
                row_str = " ".join([f"{val:4d}" for val in row])
                logger.info(f"{self.class_names[i]:4s} {row_str}")
            
            # Classification Report
            report_dict = classification_report(
                y_true_classes, y_pred_classes, 
                target_names=self.class_names,
                output_dict=True,
                zero_division=0 # –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –ø—Ä–∏ –Ω—É–ª–µ–≤–æ–º –¥–µ–ª–µ–Ω–∏–∏
            )
            
            logger.info(f"Macro Avg F1-Score: {report_dict['macro avg']['f1-score']:.3f}")
            logger.info(f"Weighted Avg F1-Score: {report_dict['weighted avg']['f1-score']:.3f}")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            pred_distribution = np.bincount(y_pred_classes, minlength=len(self.class_names)) / len(y_pred_classes)
            pred_dist_str = ", ".join([f"{name}={dist:.1%}" for name, dist in zip(self.class_names, pred_distribution)])
            logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_dist_str}")



üìã –û–ë–©–ò–ô –û–ë–ó–û–† –ò–ó–ú–ï–ù–ï–ù–ò–ô:
1. models/xlstm_rl_model.py (–û–ë–ù–û–í–õ–Å–ù)

–î–æ–±–∞–≤–ª–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è Supervised –∏ RL
–î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç–æ–¥—ã compile_for_supervised_learning() –∏ compile_for_reward_modeling()
–ú–µ—Ç–æ–¥ save() –∏ load() —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞—é—Ç stage –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö.

2. config.py (–û–ë–ù–û–í–õ–Å–ù)

–î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è (SUPERVISED_EPOCHS, REWARD_MODEL_EPOCHS, RL_EPISODES –∏ —Ç.–¥.).
–î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫ (PRICE_CHANGE_THRESHOLD, FUTURE_WINDOW).

3. feature_engineering.py (–û–ë–ù–û–í–õ–Å–ù)

–î–æ–±–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ create_trading_labels() –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç–æ–∫ BUY/HOLD/SELL –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω.
–î–æ–±–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ prepare_supervised_data() –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ –ø–µ—Ä–≤–æ–º—É —ç—Ç–∞–ø—É.

4. three_stage_trainer.py (–ù–û–í–´–ô –§–ê–ô–õ)

–≠—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è.
–°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å ThreeStageTrainer —Å –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞:

load_and_prepare_data(): –ó–∞–≥—Ä—É–∑–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö.
stage1_supervised_pretraining(): –û–±—É—á–µ–Ω–∏–µ Actor-–º–æ–¥–µ–ª–∏ –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
stage2_reward_model_training(): –û–±—É—á–µ–Ω–∏–µ Critic-–º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥.
stage3_rl_finetuning(): –î–æ–æ–±—É—á–µ–Ω–∏–µ Actor-Critic –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é RL.


–í–∫–ª—é—á–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ.

5. rl_agent.py (–û–ë–ù–û–í–õ–Å–ù)

–ò–∑–º–µ–Ω–µ–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π epsilon –Ω–∞ –º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (0.3), —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å —É–∂–µ –±—É–¥–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞.
–ú–µ—Ç–æ–¥—ã save() –∏ load() —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä stage –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è XLSTMRLModel.
–£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç–æ–≥–æ states –≤ log_action_distribution.

6. run_live_trading.py (–û–ë–ù–û–í–õ–Å–ù)

–¢–µ–ø–µ—Ä—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (_rl_finetuned) –∏–ª–∏ _supervised –∫–∞–∫ fallback.
–û–±–Ω–æ–≤–ª–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–µ—à–µ–Ω–∏–∏ –∏ –ø–æ–∑–∏—Ü–∏–∏.

7. validation_metrics_callback.py (–ù–û–í–´–ô –§–ê–ô–õ)

–ö–∞–ª–ª–±—ç–∫ –¥–ª—è Keras, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Confusion Matrix, F1-score, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π) –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö –Ω–∞ —ç—Ç–∞–ø–µ Supervised Pre-training.

8. –£–¥–∞–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:

simple_train_model.py –∏ train_model.py –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω—ã, —Ç–∞–∫ –∫–∞–∫ –∏—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ three_stage_trainer.py.


–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–∞—è, –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è! –≠—Ç–∞ —Å—Ö–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∞—à–µ–π xLSTM + RL —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ.

		
		
