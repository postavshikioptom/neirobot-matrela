import numpy as np
import tensorflow as tf
from models.xlstm_rl_model import XLSTMRLModel
import os
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class RLAgent:
    """
    –ê–≥–µ–Ω—Ç Reinforcement Learning –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ - –ü–û–î–î–ï–†–ñ–ö–ê –¢–†–Å–•–≠–¢–ê–ü–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
    """
    def __init__(self, state_shape, memory_size=64, memory_units=128, gamma=0.99, epsilon=0.3, epsilon_min=0.1, epsilon_decay=0.995, batch_size=64):
        self.state_shape = state_shape
        self.gamma = gamma  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ù–∞—á–∞–ª—å–Ω—ã–π epsilon –Ω–∏–∂–µ –¥–ª—è fine-tuning –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.epsilon = epsilon  # –ù–∞—á–∏–Ω–∞–µ–º —Å –º–µ–Ω—å—à–µ–≥–æ epsilon –¥–ª—è fine-tuning
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = XLSTMRLModel(input_shape=state_shape, 
                                 memory_size=memory_size, 
                                 memory_units=memory_units)
        
        # –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞
        self.memory = []
        self.max_memory_size = 10000
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        # self.logger = logging.getLogger('rl_agent')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
    
    def act(self, state, training=True):
        """–í—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if training and np.random.rand() < self.epsilon:
            # –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            return np.random.randint(0, 3)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –æ—Ç –º–æ–¥–µ–ª–∏ –∞–∫—Ç–æ—Ä–∞
        action_probs = self.model.predict_action(state)
        
        # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        return np.argmax(action_probs)
    
    def remember(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä"""
        self.memory.append((state, action, reward, next_state, done))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        if len(self.memory) > self.max_memory_size:
            self.memory.pop(0)
    
    def update_epsilon(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ epsilon –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞"""
        if len(self.memory) < self.batch_size:
            return None
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –±—É—Ñ–µ—Ä–∞
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = np.array([experience[0] for experience in batch])
        actions = np.array([experience[1] for experience in batch])
        rewards = np.array([experience[2] for experience in batch])
        next_states = np.array([experience[3] for experience in batch])
        dones = np.array([experience[4] for experience in batch])
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫—Ä–∏—Ç–∏–∫–∞
        with tf.GradientTape() as tape:
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
            values = self.model.critic_model(states, training=True)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
            next_values = self.model.critic_model(next_states, training=True)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            targets = rewards + self.gamma * tf.squeeze(next_values) * (1 - dones)
            targets = tf.expand_dims(targets, axis=1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
            critic_loss = tf.reduce_mean(tf.square(targets - values))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∞
        critic_grads = tape.gradient(critic_loss, self.model.critic_model.trainable_variables)
        self.model.critic_optimizer.apply_gradients(zip(critic_grads, self.model.critic_model.trainable_variables))
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–∫—Ç–æ—Ä–∞
        with tf.GradientTape() as tape:
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π
            action_probs = self.model.actor_model(states, training=True)
            
            # –°–æ–∑–¥–∞–µ–º one-hot –≤–µ–∫—Ç–æ—Ä –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            action_masks = tf.one_hot(actions, 3)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
            values = self.model.critic_model(states, training=True)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ (advantage)
            advantages = targets - values
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∞–∫—Ç–æ—Ä–∞
            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)
            log_probs = tf.math.log(selected_action_probs + 1e-10)
            actor_loss = -tf.reduce_mean(log_probs * tf.squeeze(advantages))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —ç–Ω—Ç—Ä–æ–ø–∏–∏
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-10), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∞–∫—Ç–æ—Ä–∞
        actor_grads = tape.gradient(actor_loss, self.model.actor_model.trainable_variables)
        self.model.actor_optimizer.apply_gradients(zip(actor_grads, self.model.actor_model.trainable_variables))
        
        return {
            'critic_loss': float(critic_loss),
            'actor_loss': float(actor_loss),
            'mean_value': float(tf.reduce_mean(values)),
            'mean_reward': float(np.mean(rewards))
        }
    
    def save(self, path='models'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.save(path, stage="_rl_final")
    
    def load(self, path='models'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        self.model.load(path, stage="_rl_finetuned")
    
    def log_action_distribution(self, states):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
        if len(states) == 0:
            return {'buy_count': 0, 'hold_count': 0, 'sell_count': 0, 'total': 0}
        
        actions = []
        for state in states:
            action_probs = self.model.predict_action(state)
            actions.append(np.argmax(action_probs))
        
        actions = np.array(actions)
        buy_count = np.sum(actions == 0)
        hold_count = np.sum(actions == 1)
        sell_count = np.sum(actions == 2)
        
        total = len(actions)
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.info -> print
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π: BUY: {buy_count/total:.2%}, HOLD: {hold_count/total:.2%}, SELL: {sell_count/total:.2%}")
        
        return {
            'buy_count': int(buy_count),
            'hold_count': int(hold_count),
            'sell_count': int(sell_count),
            'total': total
        }