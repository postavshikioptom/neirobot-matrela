import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import os
import tensorflow.keras.backend as K
import gc

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç
try:
    from models.xlstm_memory_cell import XLSTMMemoryCell
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ XLSTMMemoryCell: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª models/xlstm_memory_cell.py —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    raise ImportError("XLSTMMemoryCell –Ω–µ –Ω–∞–π–¥–µ–Ω")


class FocalLoss(tf.keras.losses.Loss):
    """
    Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –ª—é–±–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –∫–ª–∞—Å—Å–æ–≤
    """
    def __init__(self, alpha=0.25, gamma=2.0, label_smoothing=0.1, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha
        self.gamma = gamma
        self.label_smoothing = label_smoothing
    
    def call(self, y_true, y_pred):
        # Label smoothing –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        num_classes = tf.cast(tf.shape(y_pred)[-1], tf.float32)
        y_true_smooth = y_true * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes
        
        # –í—ã—á–∏—Å–ª—è–µ–º cross-entropy loss
        ce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)
        
        # –í—ã—á–∏—Å–ª—è–µ–º p_t (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞)
        p_t = tf.exp(-ce_loss)
        
        # Focal Loss —Ñ–æ—Ä–º—É–ª–∞: FL(p_t) = -Œ±_t * (1 - p_t)^Œ≥ * log(p_t)
        focal_loss = self.alpha * tf.pow(1 - p_t, self.gamma) * ce_loss
        
        return focal_loss
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'alpha': self.alpha,
            'gamma': self.gamma,
            'label_smoothing': self.label_smoothing
        })
        return config


class WarmUpCosineDecayScheduler(tf.keras.callbacks.Callback):
    """
    Learning Rate Scheduler —Å WarmUp –∏ Cosine Decay
    –£–ª—É—á—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—é
    """
    def __init__(self, warmup_epochs=5, total_epochs=50, base_lr=0.001, min_lr=1e-6):
        super().__init__()
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        
    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            # WarmUp phase: –ª–∏–Ω–µ–π–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # Cosine Decay phase
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        self.model.optimizer.learning_rate.assign(lr)
        print(f"Epoch {epoch + 1}: –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {lr:.6f}")





class XLSTMRLModel:
    """
    –ú–æ–¥–µ–ª—å xLSTM —Å RL –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ - –¢–†–Å–•–≠–¢–ê–ü–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
    """
    def __init__(self, input_shape, memory_size=64, memory_units=128, weight_decay=1e-4, gradient_clip_norm=1.0):
        self.input_shape = input_shape
        self.memory_size = memory_size
        self.memory_units = memory_units
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        self.weight_decay = 5e-4
        self.gradient_clip_norm = gradient_clip_norm
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
        self.actor_model = self._build_actor_model()
        self.critic_model = self._build_critic_model()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self._configure_optimizers()
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ë—É—Ñ–µ—Ä –¥–ª—è –±–∞—Ç—á–µ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.prediction_count = 0
        self.batch_predictions = []
        self.batch_size = 32

    def _configure_optimizers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã —Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω—ã –ª–∏ GPU
            physical_devices = tf.config.list_physical_devices('GPU')
            if len(physical_devices) > 0:
                # –î–ª—è GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                self.supervised_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.supervised_optimizer.learning_rate = tf.Variable(0.001)
                self.actor_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.actor_optimizer.learning_rate = tf.Variable(0.0005)
                self.critic_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.critic_optimizer.learning_rate = tf.Variable(0.001)
                print("–ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è GPU")
            else:
                # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                self.supervised_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.supervised_optimizer.learning_rate = tf.Variable(0.0005)
                self.actor_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.actor_optimizer.learning_rate = tf.Variable(0.0001)
                self.critic_optimizer = tf.keras.optimizers.Adam(
                    clipnorm=self.gradient_clip_norm
                )
                self.critic_optimizer.learning_rate = tf.Variable(0.0005)
                print("–ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è CPU")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤: {e}")
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.supervised_optimizer = tf.keras.optimizers.Adam(
                clipnorm=self.gradient_clip_norm
            )
            self.supervised_optimizer.learning_rate = tf.Variable(0.001)
            self.actor_optimizer = tf.keras.optimizers.Adam(
                clipnorm=self.gradient_clip_norm
            )
            self.actor_optimizer.learning_rate = tf.Variable(0.001)
            self.critic_optimizer = tf.keras.optimizers.Adam(
                clipnorm=self.gradient_clip_norm
            )
            self.critic_optimizer.learning_rate = tf.Variable(0.001)

    def _build_actor_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∞–∫—Ç–æ—Ä–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ —Ä–∞–∑–º–µ—Ä–∞"""
        inputs = layers.Input(shape=self.input_shape)
        
        # Batch Normalization –Ω–∞ –≤—Ö–æ–¥–µ
        # print(f"DEBUG Actor Model: inputs shape={inputs.shape} (before BatchNormalization)")
        x = layers.BatchNormalization()(inputs)
        # print(f"DEBUG Actor Model: x shape={x.shape} (after BatchNormalization)")
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤—Ö–æ–¥–∞ (–æ—Å—Ç–∞–≤–ª—è–µ–º)
        expected_features = 14  # –±–∞–∑–æ–≤—ã–µ + –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if self.input_shape[-1] != expected_features:
            print(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞: {self.input_shape[-1]}, –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_features}")
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π xLSTM —Å weight decay
        # print(f"DEBUG Actor Model: x shape={x.shape} (before first RNN)")
        x = layers.RNN(
            XLSTMMemoryCell(units=self.memory_units, memory_size=self.memory_size),
            return_sequences=True
        )(x)
        # print(f"DEBUG Actor Model: x shape={x.shape} (after first RNN)")
        
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.4)(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π xLSTM (—É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä)
        # print(f"DEBUG Actor Model: x shape={x.shape} (before second RNN)")
        x = layers.RNN(
            XLSTMMemoryCell(units=self.memory_units//2, memory_size=self.memory_size),
            return_sequences=False
        )(x)
        # print(f"DEBUG Actor Model: x shape={x.shape} (after second RNN)")
        
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ residual connections —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        dense1 = layers.Dense(
            128, 
            activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay)
        )(x)
        dense1 = layers.Dropout(0.3)(dense1)
        
        dense2 = layers.Dense(
            64, 
            activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay)
        )(dense1)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω–æ –ª–∏—à–Ω–µ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º –ø–µ—Ä–µ–¥ –æ–ø–µ—Ä–∞—Ü–∏–µ–π —Å–ª–æ–∂–µ–Ω–∏—è
        # print(f"DEBUG Actor Model: x (before resize) shape={x.shape}")
        
        x_static_shape = x.shape[-1]
        if x_static_shape != 64:
            x_resized = layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay))(x)
            # print(f"DEBUG Actor Model: x_resized shape={x_resized.shape}")
        else:
            x_resized = x
            # print(f"DEBUG Actor Model: x_resized (no change) shape={x_resized.shape}")
        
        # print(f"DEBUG Actor Model: dense2 shape={dense2.shape}")
        
        # Residual connection
        # print(f"DEBUG Actor Model: x_resized.shape={x_resized.shape}, dense2.shape={dense2.shape}") # –û—Å—Ç–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–æ—Ä–º—ã –ø–µ—Ä–µ–¥ —Å–ª–æ–∂–µ–Ω–∏–µ–º
        if x_resized.shape[-1] != dense2.shape[-1]:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –§–æ—Ä–º—ã –¥–ª—è residual connection –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: x_resized={x_resized.shape}, dense2={dense2.shape}")
        
        x = layers.Add()([x_resized, dense2])
        # print(f"DEBUG Actor Model: x (after add) shape={x.shape}")
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤
        outputs = layers.Dense(
            3, 
            activation='softmax',
            kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay),
            kernel_constraint=tf.keras.constraints.MaxNorm(max_value=2.0) # üî• –†–ê–°–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–¢–¨
        )(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º)
        total_params = model.count_params()
        max_params = 10_000_000  # 10M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–∞–∫—Å–∏–º—É–º
        if total_params > max_params:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–º–∞–∫—Å–∏–º—É–º: {max_params:,})")
        else:
            print(f"‚úÖ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Actor: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        return model

    def _build_critic_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π"""
        inputs = layers.Input(shape=self.input_shape)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        # print(f"DEBUG Critic Model: inputs shape={inputs.shape} (before LayerNormalization)")
        x = layers.LayerNormalization()(inputs)
        # print(f"DEBUG Critic Model: x shape={x.shape} (after LayerNormalization)")
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π xLSTM
        # print(f"DEBUG Critic Model: x shape={x.shape} (before first RNN)")
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units,
                                       memory_size=self.memory_size),
                      return_sequences=True)(x)
        # print(f"DEBUG Critic Model: x shape={x.shape} (after first RNN)")
        
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π xLSTM
        # print(f"DEBUG Critic Model: x shape={x.shape} (before second RNN)")
        x = layers.RNN(XLSTMMemoryCell(units=self.memory_units,
                                       memory_size=self.memory_size),
                      return_sequences=False)(x)
        # print(f"DEBUG Critic Model: x shape={x.shape} (after second RNN)")
        
        x = layers.LayerNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –° –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ï–ô
        # print(f"DEBUG Critic Model: x shape={x.shape} (before first Dense)")
        x = layers.Dense(64, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay))(x)
        # print(f"DEBUG Critic Model: x shape={x.shape} (after first Dense)")
        
        # print(f"DEBUG Critic Model: x shape={x.shape} (before second Dense)")
        x = layers.Dense(32, activation='relu',
                        kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay))(x)
        # print(f"DEBUG Critic Model: x shape={x.shape} (after second Dense)")
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –° –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–ï–ô
        # print(f"DEBUG Critic Model: x shape={x.shape} (before output Dense)")
        outputs = layers.Dense(1, 
                              kernel_regularizer=tf.keras.regularizers.l2(self.weight_decay))(x)
        # print(f"DEBUG Critic Model: outputs shape={outputs.shape} (after output Dense)")
        
        model = models.Model(inputs=inputs, outputs=outputs)
        
        # üî• –î–û–ë–ê–í–ò–¢–¨: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º)
        total_params = model.count_params()
        max_params = 10_000_000
        if total_params > max_params:
            print(f"‚ö†Ô∏è Critic –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        else:
            print(f"‚úÖ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Critic: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        return model

    def compile_for_supervised_learning(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 1: Supervised Learning —Å Focal Loss"""
        # –°–æ–∑–¥–∞–µ–º Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤
        focal_loss = FocalLoss(alpha=0.25, gamma=2.0, label_smoothing=0.1)
        
        self.actor_model.compile(
            optimizer=self.supervised_optimizer,
            loss=focal_loss,  # –ó–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ Focal Loss
            metrics=['accuracy'],
            run_eagerly=False
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è supervised learning —Å Focal Loss (Œ±=0.25, Œ≥=2.0)")

    def compile_for_reward_modeling(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ 2: Reward Model Training"""
        optimizer = tf.keras.optimizers.Adam(
            clipnorm=self.gradient_clip_norm  # üî• –î–û–ë–ê–í–õ–ï–ù–û: Gradient clipping
        )
        optimizer.learning_rate = tf.Variable(0.001)
        self.critic_model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è reward modeling")

    def get_training_callbacks(self, total_epochs=50, patience=10):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        callbacks = [
            # WarmUp + Cosine Decay Learning Rate
            WarmUpCosineDecayScheduler(
                warmup_epochs=5,
                total_epochs=total_epochs,
                base_lr=0.001,
                min_lr=1e-6
            ),
            
            # Early Stopping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=patience,
                restore_best_weights=True,
                verbose=1
            ),
            
            # Model Checkpoint –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            tf.keras.callbacks.ModelCheckpoint(
                filepath='models/best_xlstm_model.keras',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            
            # Reduce LR on Plateau (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞)
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        return callbacks

    def save(self, path='models', stage=""):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞"""
        if not os.path.exists(path):
            os.makedirs(path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞
        actor_name = f'xlstm_rl_actor{stage}.keras'
        critic_name = f'xlstm_rl_critic{stage}.keras'
        
        self.actor_model.save(os.path.join(path, actor_name))
        self.critic_model.save(os.path.join(path, critic_name))
        
        print(f"–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path} (—ç—Ç–∞–ø: {stage})")

    def load(self, path='models', stage=""):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —ç—Ç–∞–ø–∞"""
        actor_name = f'xlstm_rl_actor{stage}.keras'
        critic_name = f'xlstm_rl_critic{stage}.keras'
        
        actor_path = os.path.join(path, actor_name)
        critic_path = os.path.join(path, critic_name)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        try:
            if os.path.exists(actor_path) and os.path.exists(critic_path):
                self.actor_model = tf.keras.models.load_model(
                    actor_path, 
                    custom_objects={'XLSTMMemoryCell': XLSTMMemoryCell}
                )
                self.critic_model = tf.keras.models.load_model(
                    critic_path,
                    custom_objects={'XLSTMMemoryCell': XLSTMMemoryCell}
                )
                print(f"–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—ç—Ç–∞–ø: {stage})")
            else:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–∞–ø–∞: {stage}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def predict_action(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if len(state.shape) == 2:
            state = np.expand_dims(state, axis=0)
        
        if state.dtype != np.float32:
            state = state.astype(np.float32)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º predict_on_batch –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        try:
            action_probs = self.actor_model.predict_on_batch(state)[0]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ predict_on_batch, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: {e}")
            action_probs = self.actor_model.predict(state, verbose=0)[0]
        
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self.prediction_count += 1
        if self.prediction_count % 50 == 0:  # –£–≤–µ–ª–∏—á–µ–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞ –æ—á–∏—Å—Ç–∫–∏
            gc.collect()
            if self.prediction_count % 500 == 0:  # –ì–ª—É–±–æ–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–∞–∂–¥—ã–µ 500 –≤—ã–∑–æ–≤–æ–≤
                tf.keras.backend.clear_session()
                print(f"–ì–ª—É–±–æ–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ {self.prediction_count} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        
        return action_probs
    
    def predict_value(self, state):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if len(state.shape) == 2:
            state = np.expand_dims(state, axis=0)
        
        if state.dtype != np.float32:
            state = state.astype(np.float32)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º predict_on_batch
        try:
            value = self.critic_model.predict_on_batch(state)[0]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ predict_on_batch –¥–ª—è critic: {e}")
            value = self.critic_model.predict(state, verbose=0)[0]
        
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self.prediction_count += 1
        if self.prediction_count % 50 == 0:
            gc.collect()
            if self.prediction_count % 500 == 0:
                tf.keras.backend.clear_session()
        
        return value
    
    def predict_batch_actions(self, states):
        """üî• –î–û–ë–ê–í–õ–ï–ù–û: –ë–∞—Ç—á–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
        if len(states) == 0:
            return np.array([])
        
        states = np.array(states, dtype=np.float32)
        if len(states.shape) == 2:
            states = np.expand_dims(states, axis=1)
        
        action_probs = self.actor_model.predict_on_batch(states)
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ –±–∞—Ç—á–µ–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        gc.collect()
        
        return action_probs