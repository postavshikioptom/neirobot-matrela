import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input, Attention
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
import pickle
import os
from .xlstm_memory_cell import XLSTMLayer  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π xLSTM
from sklearn.utils.class_weight import compute_class_weight

class XLSTMRLModel:
    """
    –ù–∞—Å—Ç–æ—è—â–∞—è xLSTM –º–æ–¥–µ–ª—å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
    """
    
    def __init__(self, input_shape, memory_units=128, memory_size=64, attention_units=64):
        self.input_shape = input_shape
        self.memory_units = memory_units
        self.memory_size = memory_size
        self.attention_units = attention_units
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def build_model(self):
        """
        –°—Ç—Ä–æ–∏—Ç –Ω–∞—Å—Ç–æ—è—â—É—é xLSTM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –ø–∞–º—è—Ç—å—é - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
        """
        # ‚úÖ –§–∏–∫—Å–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        inputs = Input(shape=self.input_shape, name='input_layer')
        
        # –ü–µ—Ä–≤—ã–π xLSTM —Å–ª–æ–π —Å –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç—å—é
        xlstm1 = XLSTMLayer(
            units=self.memory_units,
            memory_size=self.memory_size,
            return_sequences=True,
            name='xlstm_memory_layer_1'
        )(inputs)
        
        # –í—Ç–æ—Ä–æ–π xLSTM —Å–ª–æ–π
        xlstm2 = XLSTMLayer(
            units=self.memory_units // 2,
            memory_size=self.memory_size // 2,
            return_sequences=True,
            name='xlstm_memory_layer_2'
        )(xlstm1)
        
        # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        attention = Attention(name='attention_mechanism')([xlstm2, xlstm2])
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π xLSTM —Å–ª–æ–π
        xlstm_final = XLSTMLayer(
            units=self.attention_units,
            memory_size=self.attention_units,
            return_sequences=False,
            name='xlstm_memory_final'
        )(attention)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏
        dense1 = Dense(64, activation='relu', name='dense_1')(xlstm_final)
        dropout1 = Dropout(0.3)(dense1)
        
        dense2 = Dense(32, activation='relu', name='dense_2')(dropout1)
        dropout2 = Dropout(0.2)(dense2)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        outputs = Dense(3, activation='softmax', name='output_layer')(dropout2)
        
        self.model = Model(inputs=inputs, outputs=outputs, name='True_xLSTM_RL_Model')
        
        
        print("‚úÖ –ù–∞—Å—Ç–æ—è—â–∞—è xLSTM –º–æ–¥–µ–ª—å —Å –ø–∞–º—è—Ç—å—é —Å–æ–∑–¥–∞–Ω–∞!")
        return self.model
    
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, custom_callbacks=None):
        """–û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é"""
        if self.model is None:
            self.build_model()
        
        # –î–û–ë–ê–í–¨–¢–ï: –û–±—É—á–µ–Ω–∏–µ scaler –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
        self.scaler.fit(X_train_reshaped)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        X_train_scaled = self.scaler.transform(X_train_reshaped).reshape(X_train.shape)
        X_val_scaled = self.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
            
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=25,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'models/xlstm_checkpoint_epoch_{epoch:02d}.keras',
                monitor='val_loss',
                save_best_only=False, # –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å False, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤—Å–µ —ç–ø–æ—Ö–∏
                save_freq='epoch',  # <-- –ò–ó–ú–ï–ù–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
                verbose=0 # <-- –ò–ó–ú–ï–ù–ï–ù–û: –æ—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'models/xlstm_checkpoint_latest.keras',
                monitor='val_loss',
                save_best_only=False,
                save_freq='epoch',
                verbose=0
            ),
            tf.keras.callbacks.CSVLogger('training_log.csv', append=True)
        ]
        
        if custom_callbacks:
            callbacks.extend(custom_callbacks)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=0.001,
            clipnorm=1.0
        )
        self.model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        # –î–û–ë–ê–í–¨–¢–ï: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
        y_integers = np.argmax(y_train, axis=1) # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º one-hot –≤ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞
        class_weights_array = compute_class_weight(
            'balanced',
            classes=np.unique(y_integers),
            y=y_integers
        )
        class_weight_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}

        print(f"üìä –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {class_weight_dict}")

        # –û–±—É—á–µ–Ω–∏–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_val_scaled, y_val),
            epochs=epochs,
            batch_size=batch_size,
            class_weight=class_weight_dict,  # <-- –î–û–ë–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
            callbacks=callbacks,
            verbose=0, # –ò–∑–º–µ–Ω—è–µ–º –Ω–∞ 0 –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            shuffle=True
        )
        
        self.is_trained = True
        return history

    def predict(self, X):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")
            
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        return self.model.predict(X_scaled, verbose=0)
    
    def save_model(self, model_path='models/xlstm_rl_model.keras', scaler_path='models/xlstm_rl_scaler.pkl'):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–∞
        """
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        self.model.save(model_path)
        
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
            
        print(f"xLSTM-RL –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        print(f"–°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_path}")
    
    def load_model(self, model_path='models/xlstm_rl_model.keras', scaler_path='models/xlstm_rl_scaler.pkl'):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–∞
        """
        self.model = tf.keras.models.load_model(model_path, custom_objects={'XLSTMLayer': XLSTMLayer})
        
        with open(scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)
            
        self.is_trained = True
        print(f"xLSTM-RL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")