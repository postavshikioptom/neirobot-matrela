import numpy as np
# import logging # üî• –£–î–ê–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç logging

class HybridDecisionMaker:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π RL-–∞–≥–µ–Ω—Ç–∞
    """
    def __init__(self, rl_agent):
        self.rl_agent = rl_agent
        
        # üî• –£–î–ê–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        # self.logger = logging.getLogger('hybrid_decision_maker')
        # self.logger.setLevel(logging.INFO)
        # if not self.logger.handlers:
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)
    
    def make_decision(self, state, training=False, position=0):
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ—Ä–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä—ã–Ω–∫–∞
        
        Args:
            state: —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞
            training: —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
            position: —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (0, 1, -1)
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - action: 0 (BUY), 1 (HOLD), –∏–ª–∏ 2 (SELL)
        - confidence: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ—à–µ–Ω–∏–∏ (0-1)
        """
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –æ—Ç RL-–∞–≥–µ–Ω—Ç–∞
        action_probs = self.rl_agent.model.predict_action(state)
        
        # –í —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è epsilon-greedy
        if training and np.random.rand() < self.rl_agent.epsilon:
            action = np.random.randint(0, 3)
            confidence = 1.0 / 3.0  # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        else:
            # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            action = np.argmax(action_probs)
            confidence = action_probs[action]
        
        action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: logger.debug -> print
        print(f"–ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ: {action_names[action]} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.4f}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: BUY: {action_probs[0]:.4f}, HOLD: {action_probs[1]:.4f}, SELL: {action_probs[2]:.4f}")
        print(f"–¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è: {position}")
        
        return action, confidence
    
    def explain_decision(self, state):
        """
        –û–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–∏–Ω—è—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        """
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π
        action_probs = self.rl_agent.model.predict_action(state)
        action = np.argmax(action_probs)
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Ç –∫—Ä–∏—Ç–∏–∫–∞
        value = float(self.rl_agent.model.predict_value(state)[0])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        action_names = {0: "BUY", 1: "HOLD", 2: "SELL"}
        explanation = {
            'action': action_names[action],
            'confidence': float(action_probs[action]),
            'all_probs': {
                'BUY': float(action_probs[0]),
                'HOLD': float(action_probs[1]),
                'SELL': float(action_probs[2])
            },
            'state_value': value
        }
        
        return explanation