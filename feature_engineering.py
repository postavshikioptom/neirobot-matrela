import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler
import pickle
import os
import talib # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç TA-Lib
import tensorflow as tf # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç Tensorflow
import config # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç config –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
import gc
from collections import deque
import logging
# üî• –î–û–ë–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç psutil –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏
try:
    import psutil
except Exception:
    psutil = None

def log_nan_inf_stats(df, stage_name="Unknown"):
    """üî• –î–û–ë–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ NaN –∏ inf –∑–Ω–∞—á–µ–Ω–∏–π"""
    nan_stats = {}
    inf_stats = {}
    
    for col in df.columns:
        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            nan_count = df[col].isna().sum()
            inf_count = np.isinf(df[col]).sum()
            
            if nan_count > 0:
                nan_stats[col] = {
                    'count': nan_count,
                    'percentage': nan_count / len(df) * 100
                }
            
            if inf_count > 0:
                inf_stats[col] = {
                    'count': inf_count,
                    'percentage': inf_count / len(df) * 100
                }
    
    if nan_stats:
        print(f"‚ö†Ô∏è NaN –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤ {stage_name}:")
        for col, stats in nan_stats.items():
            print(f"  {col}: {stats['count']} ({stats['percentage']:.2f}%)")
    
    if inf_stats:
        print(f"‚ö†Ô∏è Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤ {stage_name}:")
        for col, stats in inf_stats.items():
            print(f"  {col}: {stats['count']} ({stats['percentage']:.2f}%)")
    
    return nan_stats, inf_stats

def safe_fill_nan_inf(df, method='median'):
    """üî• –î–û–ë–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –∏ inf –∑–Ω–∞—á–µ–Ω–∏–π"""
    df_clean = df.copy()
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ –æ—á–∏—Å—Ç–∫–∏
    nan_stats, inf_stats = log_nan_inf_stats(df_clean, "–î–æ –æ—á–∏—Å—Ç–∫–∏")
    
    for col in df_clean.columns:
        if df_clean[col].dtype in ['float64', 'float32', 'int64', 'int32']:
            # –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN
            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
            if method == 'median':
                fill_value = df_clean[col].median()
                if pd.isna(fill_value):
                    # –ï—Å–ª–∏ –º–µ–¥–∏–∞–Ω–∞ —Ç–æ–∂–µ NaN, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ
                    fill_value = df_clean[col].mean()
                    if pd.isna(fill_value):
                        # –ï—Å–ª–∏ –∏ —Å—Ä–µ–¥–Ω–µ–µ NaN, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                        fill_value = 0.0
            elif method == 'mean':
                fill_value = df_clean[col].mean()
                if pd.isna(fill_value):
                    fill_value = 0.0
            else:  # method == 'zero'
                fill_value = 0.0
            
            df_clean[col] = df_clean[col].fillna(fill_value)
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    post_nan, post_inf = log_nan_inf_stats(df_clean, "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
    # if for any important indicator post_nan still > 0:
    for col, stats in post_nan.items():
        if stats['percentage'] > 0.5:
            print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ {col} –∏–º–µ–µ—Ç {stats['percentage']:.2f}% NaN ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö")
    
    return df_clean

class FeatureEngineering:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    """
    def __init__(self, sequence_length=60):
        self.sequence_length = sequence_length
        self.scaler = RobustScaler()  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º —á–µ–º StandardScaler
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –ò—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        self.base_features = ['open', 'high', 'low', 'close', 'volume', 'turnover']
        self.feature_columns = list(self.base_features) # –ë—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.indicator_cache = {}
        self.cache_max_size = 1000
        self.fallback_retry_count = 0
        self.max_fallback_retries = 3
    
    def _validate_data_for_indicators(self, df):
        """üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        if df is None or df.empty:
            return False, "–ü—É—Å—Ç–æ–π DataFrame"
        
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            return False, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}"
        
        if len(df) < config.RSI_PERIOD + 5:
            return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} < {config.RSI_PERIOD + 5}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN
        for col in required_cols:
            nan_percent = df[col].isna().sum() / len(df)
            if nan_percent > 0.5:
                return False, f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {nan_percent:.2%}"
        
        return True, "OK"
    
    def _get_cache_key(self, df):
        """üî• –î–û–ë–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è DataFrame"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash –æ—Ç –ø–µ—Ä–≤—ã—Ö/–ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ —Ä–∞–∑–º–µ—Ä–∞
            first_vals = tuple(df.iloc[0][['open', 'high', 'low', 'close']].values)
            last_vals = tuple(df.iloc[-1][['open', 'high', 'low', 'close']].values)
            return hash((first_vals, last_vals, len(df)))
        except:
            return None
        
    def _add_technical_indicators(self, df):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        try:
            # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
            is_valid, error_msg = self._validate_data_for_indicators(df)
            if not is_valid:
                print(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {error_msg}")
                return self._create_fallback_indicators_df()
            
            # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = self._get_cache_key(df)
            if cache_key and cache_key in self.indicator_cache:
                print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
                cached_result = self.indicator_cache[cache_key].copy()
                return cached_result
            
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ú–∞—Å—Å–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            indicators_success = self._calculate_all_indicators_batch(df)
            
            if not indicators_success:
                print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—Å–æ–≤–æ–º —Ä–∞—Å—á–µ—Ç–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._create_fallback_indicators_df()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self.feature_columns = self.base_features + [
                # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'EMA_7', 'EMA_14', 'EMA_21',
                'MACD', 'MACDSIGNAL', 'MACDHIST',
                'KAMA', 'SUPERTREND',
                
                # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'RSI', 'CMO', 'ROC',
                
                # Volume –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'OBV', 'MFI',
                
                # Volatility –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'ATR', 'NATR',
                
                # Statistical –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'STDDEV',
                
                # Cycle –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'HT_DCPERIOD', 'HT_SINE', 'HT_LEADSINE'
            ]
            
            # üî• –î–û–ë–ê–í–õ–ï–ù–û: –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if cache_key and len(self.indicator_cache) < self.cache_max_size:
                self.indicator_cache[cache_key] = df.copy()
            elif len(self.indicator_cache) >= self.cache_max_size:
                # –û—á–∏—â–∞–µ–º –∫—ç—à –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏
                self.indicator_cache.clear()
                gc.collect()
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ _add_technical_indicators: {e}")
            return self._create_fallback_indicators_df()
        
        # –ù–∞–¥—ë–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN
        try:
            df = safe_fill_nan_inf(df, method='median')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ NaN/inf: {e}")
            df = df.fillna(0)
        
        return df
    
    def _calculate_all_indicators_batch(self, df):
        """üî• –û–ë–ù–û–í–õ–ï–ù–û: –ú–∞—Å—Å–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        try:
            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in df.columns:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º forward fill, –∑–∞—Ç–µ–º backward fill –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è NaN
                    df[col] = df[col].ffill().bfill()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float64 –¥–ª—è talib
            close_prices = df['close'].astype(np.float64).values
            high_prices = df['high'].astype(np.float64).values
            low_prices = df['low'].astype(np.float64).values
            volume_prices = df['volume'].astype(np.float64).values
            
            # –ó–∞–º–µ–Ω—è–µ–º –Ω—É–ª–µ–≤—ã–µ —Ü–µ–Ω—ã –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            close_prices = np.where(close_prices <= 0, 1e-8, close_prices)
            high_prices = np.where(high_prices <= 0, 1e-8, high_prices)
            low_prices = np.where(low_prices <= 0, 1e-8, low_prices)
            volume_prices = np.where(volume_prices <= 0, 1e-8, volume_prices)
            
            # –ú–∞—Å—Å–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            indicators = {}
            
            # === –¢–†–ï–ù–î–û–í–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # EMA (7, 14, 21)
            for period in config.EMA_PERIODS:
                indicators[f'EMA_{period}'] = talib.EMA(close_prices, timeperiod=period)
            
            # MACD (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)
            macd, macdsignal, macdhist = talib.MACD(
                close_prices,
                fastperiod=config.MACD_FASTPERIOD,
                slowperiod=config.MACD_SLOWPERIOD,
                signalperiod=config.MACD_SIGNALPERIOD
            )
            indicators['MACD'] = macd
            indicators['MACDSIGNAL'] = macdsignal
            indicators['MACDHIST'] = macdhist
            
            # KAMA
            indicators['KAMA'] = talib.KAMA(close_prices, timeperiod=config.KAMA_PERIOD)
            
            # SuperTrend (–∫–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
            indicators['SUPERTREND'] = self._calculate_supertrend(
                high_prices, low_prices, close_prices, 
                config.SUPERTREND_PERIOD, config.SUPERTREND_MULTIPLIER
            )
            
            # === MOMENTUM –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # RSI (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)
            indicators['RSI'] = talib.RSI(close_prices, timeperiod=config.RSI_PERIOD)
            
            # CMO
            indicators['CMO'] = talib.CMO(close_prices, timeperiod=config.CMO_PERIOD)
            
            # ROC
            indicators['ROC'] = talib.ROC(close_prices, timeperiod=config.ROC_PERIOD)
            
            # === VOLUME –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # OBV
            indicators['OBV'] = talib.OBV(close_prices, volume_prices)
            
            # MFI
            indicators['MFI'] = talib.MFI(
                high_prices, low_prices, close_prices, volume_prices, 
                timeperiod=config.RSI_PERIOD
            )
            
            # === VOLATILITY –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # ATR
            indicators['ATR'] = talib.ATR(
                high_prices, low_prices, close_prices, 
                timeperiod=config.ATR_PERIOD
            )
            
            # NATR
            indicators['NATR'] = talib.NATR(
                high_prices, low_prices, close_prices, 
                timeperiod=config.NATR_PERIOD
            )
            
            # === STATISTICAL –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # STDDEV
            indicators['STDDEV'] = talib.STDDEV(close_prices, timeperiod=config.STDDEV_PERIOD)
            
            # === CYCLE –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            
            # HT_DCPERIOD
            indicators['HT_DCPERIOD'] = talib.HT_DCPERIOD(close_prices)
            
            # HT_SINE
            sine, leadsine = talib.HT_SINE(close_prices)
            indicators['HT_SINE'] = sine
            indicators['HT_LEADSINE'] = leadsine
            
            # === –ù–û–í–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ë–ê–õ–ê–ù–°–ê –ö–õ–ê–°–°–û–í ===
            
            # Bollinger Bands
            bb_upper, bb_middle, bb_lower = talib.BBANDS(
                close_prices, 
                timeperiod=20, 
                nbdevup=2, 
                nbdevdn=2, 
                matype=0
            )
            indicators['BB_UPPER'] = bb_upper
            indicators['BB_MIDDLE'] = bb_middle
            indicators['BB_LOWER'] = bb_lower
            indicators['BB_WIDTH'] = (bb_upper - bb_lower) / bb_middle  # –®–∏—Ä–∏–Ω–∞ –ø–æ–ª–æ—Å
            indicators['BB_POSITION'] = (close_prices - bb_lower) / (bb_upper - bb_lower)  # –ü–æ–∑–∏—Ü–∏—è —Ü–µ–Ω—ã –≤ –ø–æ–ª–æ—Å–∞—Ö
            
            # Stochastic RSI
            fastk, fastd = talib.STOCHRSI(
                close_prices, 
                timeperiod=14, 
                fastk_period=5, 
                fastd_period=3, 
                fastd_matype=0
            )
            indicators['STOCHRSI_K'] = fastk
            indicators['STOCHRSI_D'] = fastd
            
            # Williams %R
            indicators['WILLR'] = talib.WILLR(
                high_prices, 
                low_prices, 
                close_prices, 
                timeperiod=14
            )
            
            # Commodity Channel Index (CCI)
            indicators['CCI'] = talib.CCI(
                high_prices, 
                low_prices, 
                close_prices, 
                timeperiod=14
            )
            
            # Average Directional Index (ADX)
            indicators['ADX'] = talib.ADX(
                high_prices, 
                low_prices, 
                close_prices, 
                timeperiod=14
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ DataFrame –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN
            for name, values in indicators.items():
                df[name] = values
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
                if np.isnan(values).any():
                    median_value = np.nanmedian(values)
                    df[name] = df[name].fillna(median_value)
            
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—Å–æ–≤–æ–º —Ä–∞—Å—á–µ—Ç–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {e}")
            return False
    
    def _calculate_supertrend(self, high, low, close, period=10, multiplier=3.0):
        """–†–∞—Å—á–µ—Ç SuperTrend –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞"""
        try:
            # –†–∞—Å—á–µ—Ç ATR
            atr = talib.ATR(high, low, close, timeperiod=period)
            
            # –†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –ª–∏–Ω–∏–π
            hl2 = (high + low) / 2
            upper_band = hl2 + (multiplier * atr)
            lower_band = hl2 - (multiplier * atr)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–æ–≤
            supertrend = np.zeros_like(close)
            direction = np.ones_like(close)
            
            # –†–∞—Å—á–µ—Ç SuperTrend
            for i in range(1, len(close)):
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Ö–Ω–µ–π –ø–æ–ª–æ—Å—ã
                if upper_band[i] < upper_band[i-1] or close[i-1] > upper_band[i-1]:
                    upper_band[i] = upper_band[i]
                else:
                    upper_band[i] = upper_band[i-1]
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∏–∂–Ω–µ–π –ø–æ–ª–æ—Å—ã
                if lower_band[i] > lower_band[i-1] or close[i-1] < lower_band[i-1]:
                    lower_band[i] = lower_band[i]
                else:
                    lower_band[i] = lower_band[i-1]
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
                if close[i] <= lower_band[i-1]:
                    direction[i] = -1
                elif close[i] >= upper_band[i-1]:
                    direction[i] = 1
                else:
                    direction[i] = direction[i-1]
                
                # –†–∞—Å—á–µ—Ç SuperTrend
                if direction[i] == 1:
                    supertrend[i] = lower_band[i]
                else:
                    supertrend[i] = upper_band[i]
            
            return supertrend
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ SuperTrend: {e}")
            return np.zeros_like(close)

    def _create_fallback_indicators_df(self, df=None):
        """üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ—Ç DataFrame —Å fallback –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –±–µ–∑ —Ä–µ–∫—É—Ä—Å–∏–∏"""
        self.fallback_retry_count += 1
        
        if self.fallback_retry_count > self.max_fallback_retries:
            print(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ fallback: {self.max_fallback_retries}")
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –≤—ã–∑–æ–≤–æ–≤
            self.fallback_retry_count = 0
            return None
        
        if df is None:
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π DataFrame
            df = pd.DataFrame({
                'open': [100.0],
                'high': [101.0],
                'low': [99.0],
                'close': [100.5],
                'volume': [1000.0],
                'turnover': [100000.0]
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º fallback –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        for period in config.EMA_PERIODS:
            df[f'EMA_{period}'] = 100.0
        df['MACD'] = 0.0
        df['MACDSIGNAL'] = 0.0
        df['MACDHIST'] = 0.0
        df['KAMA'] = 100.0
        df['SUPERTREND'] = 100.0
        
        # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['RSI'] = 50.0
        df['CMO'] = 0.0
        df['ROC'] = 0.0
        
        # Volume –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['OBV'] = 0.0
        df['MFI'] = 50.0
        
        # Volatility –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['ATR'] = 1.0
        df['NATR'] = 1.0
        
        # Statistical –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['STDDEV'] = 1.0
        
        # Cycle –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['HT_DCPERIOD'] = 20.0
        df['HT_SINE'] = 0.0
        df['HT_LEADSINE'] = 0.0
        
        self.feature_columns = self.base_features + [
            # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'EMA_7', 'EMA_14', 'EMA_21',
            'MACD', 'MACDSIGNAL', 'MACDHIST',
            'KAMA', 'SUPERTREND',
            
            # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'RSI', 'CMO', 'ROC',
            
            # Volume –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'OBV', 'MFI',
            
            # Volatility –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'ATR', 'NATR',
            
            # Statistical –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'STDDEV',
            
            # Cycle –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'HT_DCPERIOD', 'HT_SINE', 'HT_LEADSINE'
        ]
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
        self.fallback_retry_count = 0
        
        return df

    def prepare_data(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ timestamp –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        if 'timestamp' in df.columns:
            if not pd.api.types.is_numeric_dtype(df['timestamp']):
                print(f"‚ö†Ô∏è timestamp –Ω–µ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {df['timestamp'].dtype}, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º")
                df['timestamp'] = pd.to_numeric(df['timestamp'])
            print(f"–¢–∏–ø timestamp: {df['timestamp'].dtype}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp (—á–∏—Å–ª–æ–≤–æ–º—É)
        df = df.sort_values('timestamp')
        
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df_with_indicators = self._add_technical_indicators(df.copy())
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        for col in self.feature_columns:
            df_with_indicators[col] = pd.to_numeric(df_with_indicators[col], errors='coerce')
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        data = df_with_indicators[self.feature_columns].values
        
        # –û–±—É—á–∞–µ–º —Å–∫–µ–π–ª–µ—Ä –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–ø–µ—Ä—å –≤–∫–ª—é—á–∞—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
        scaled_data = self.scaler.fit_transform(data)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X, y_close = self._create_sequences(scaled_data)
        
        return X, y_close, df_with_indicators
    
    def prepare_test_data(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Å–∫–µ–π–ª–µ—Ä–∞ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        """
        df = df.sort_values('timestamp')
        
        # üî• –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df_with_indicators = self._add_technical_indicators(df.copy())
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        for col in self.feature_columns:
            df_with_indicators[col] = pd.to_numeric(df_with_indicators[col], errors='coerce')
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        data = df_with_indicators[self.feature_columns].values
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        scaled_data = self.scaler.transform(data)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X, y_close = self._create_sequences(scaled_data)
        
        return X, y_close, df_with_indicators
    
    def _create_sequences(self, data):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if data is None or len(data) == 0:
                print("‚ùå –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–∞–Ω—ã –≤ _create_sequences")
                return np.array([]), np.array([])
            
            # print(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º—ã {data.shape}") # üî• –£–±—Ä–∞–Ω–æ –ª–∏—à–Ω–µ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            
            if len(data) <= self.sequence_length:
                print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(data)} <= {self.sequence_length}")
                if len(data) > 10:
                    reduced_sequence_length = len(data) - 5
                    print(f"–ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π {reduced_sequence_length}")
                    X = []
                    y_close = []
                    
                    close_index = 3
                    try:
                        if hasattr(self, 'base_features') and 'close' in self.base_features:
                            close_index = self.base_features.index('close')
                    except (ValueError, AttributeError):
                        pass
                    
                    X.append(data[:reduced_sequence_length])
                    y_close.append(data[reduced_sequence_length, close_index])
                    
                    return np.array(X), np.array(y_close)
                else:
                    return np.array([]), np.array([])
            
            X = []
            y_close = []
            
            close_index = 3
            try:
                if hasattr(self, 'base_features') and 'close' in self.base_features:
                    close_index = self.base_features.index('close')
            except (ValueError, AttributeError):
                close_index = 3
            
            for i in range(len(data) - self.sequence_length):
                try:
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                    sequence = data[i:i+self.sequence_length]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/inf –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    if np.isnan(sequence).any() or np.isinf(sequence).any():
                        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω NaN/inf –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {i}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue
                    
                    X.append(sequence)
                    y_close.append(data[i+self.sequence_length, close_index])
                except (IndexError, ValueError) as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {i}: {e}")
                    continue
            
            if len(X) == 0:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                return np.array([]), np.array([])
            
            return np.array(X), np.array(y_close)
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ _create_sequences: {e}")
            return np.array([]), np.array([])
    
    def calculate_adaptive_threshold(self, df, base_threshold=None):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞
        
        Args:
            df (pd.DataFrame): DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ü–µ–Ω
            base_threshold (float, optional): –ë–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è. –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ config.
            
        Returns:
            float: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
        """
        # –ï—Å–ª–∏ base_threshold –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –±–µ—Ä–µ–º –µ–≥–æ –∏–∑ config
        if base_threshold is None:
            base_threshold = config.PRICE_CHANGE_THRESHOLD
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_cols = ['high', 'low', 'close']
            for col in required_cols:
                if col not in df.columns:
                    print(f"–û—à–∏–±–∫–∞: –∫–æ–ª–æ–Ω–∫–∞ {col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö")
                    return base_threshold
            
            # –†–∞—Å—á–µ—Ç True Range –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Å–≤–µ—á–µ–π
            n_periods = min(14, len(df) - 1)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥ ATR=14 –∏–ª–∏ –º–µ–Ω—å—à–µ, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
            
            tr_values = []
            for i in range(1, n_periods + 1):
                if i >= len(df):
                    break
                    
                high = df['high'].iloc[-i]
                low = df['low'].iloc[-i]
                prev_close = df['close'].iloc[-(i+1)]
                
                tr1 = high - low
                tr2 = abs(high - prev_close)
                tr3 = abs(low - prev_close)
                
                tr = max(tr1, tr2, tr3)
                tr_values.append(tr)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º ATR
            if tr_values:
                atr = sum(tr_values) / len(tr_values)
            else:
                print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ ATR, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥")
                return base_threshold
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ATR –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
            last_price = df['close'].iloc[-1]
            if last_price > 0:
                normalized_atr = atr / last_price
            else:
                normalized_atr = 0.001
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–Ω–æ–∂–∏—Ç–µ–ª—è –¥–ª—è –Ω–∏–∑–∫–æ–≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤
            # –£–º–µ–Ω—å—à–∞–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å –ø–æ—Ä–æ–≥ —Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
            atr_multiplier = config.ADAPTIVE_THRESHOLD_MULTIPLIER
            # –ï—Å–ª–∏ normalized_atr –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π, –º—ã –º–æ–∂–µ–º –Ω–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏—Ç—å –º–Ω–æ–∂–∏—Ç–µ–ª—å,
            # –Ω–æ –Ω–µ —Ç–∞–∫ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ, –∫–∞–∫ —Ä–∞–Ω—å—à–µ, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª—É—á–∏—Ç—å —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ BUY/SELL
            if normalized_atr < 0.0005:  # –ï—Å–ª–∏ ATR –º–µ–Ω—å—à–µ 0.05%
                atr_multiplier = 0.9   # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
            elif normalized_atr < 0.001: # –ï—Å–ª–∏ ATR –º–µ–Ω—å—à–µ 0.1%
                atr_multiplier = 0.8
                
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            adaptive_threshold = max(
                config.ADAPTIVE_THRESHOLD_MIN, 
                min(config.ADAPTIVE_THRESHOLD_MAX, normalized_atr * atr_multiplier)
            )
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±–∏—Ä–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞,
            # —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å –º—ã —Ö–æ—Ç–∏–º –µ–≥–æ –Ω–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è HOLD
            # recommended_threshold_from_log = self._get_recommended_threshold_from_data(df, future_window=config.FUTURE_WINDOW)
            # if recommended_threshold_from_log is not None and recommended_threshold_from_log < adaptive_threshold * 0.5:
            #     print(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–≥–æ: {recommended_threshold_from_log:.6f}")
            #     adaptive_threshold = recommended_threshold_from_log
                
            # –ï—Å–ª–∏ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
            # print(f"[ADAPTIVE] Base threshold: {base_threshold:.6f}, ATR: {normalized_atr:.6f}, "
            #       f"Adaptive threshold: {adaptive_threshold:.6f}")
            
            return adaptive_threshold
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞: {e}")
            return base_threshold

    def _get_recommended_threshold_from_data(self, df, future_window):
        """
        –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö,
        —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ 30% —Å–∏–≥–Ω–∞–ª–æ–≤.
        """
        try:
            prices = df['close'].values
            if len(prices) <= future_window:
                return None
                
            sample_changes = []
            for j in range(len(prices) - future_window):
                cp = float(prices[j])
                fp = float(prices[j+future_window])
                if cp == 0:
                    pct = 0.0
                else:
                    pct = (fp - cp) / cp
                sample_changes.append(pct)
            
            changes_abs = np.abs(sample_changes)
            if not changes_abs.any(): # –ï—Å–ª–∏ –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω—É–ª–µ–≤—ã–µ
                return 0.0001 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                
            changes_sorted = np.sort(changes_abs)
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –±—ã –¥–∞–ª –ø—Ä–∏–º–µ—Ä–Ω–æ 30% —Å–∏–≥–Ω–∞–ª–æ–≤ (–Ω–µ HOLD)
            target_idx = int(len(changes_sorted) * 0.7)  # 70-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
            if target_idx < len(changes_sorted):
                return changes_sorted[target_idx]
            else:
                return None
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞: {e}")
            return None

    def create_trading_labels(self, df):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—É–¥—É—â–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã
        —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ config.py.
        
        Args:
            df (pd.DataFrame): DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ü–µ–Ω
            
        Returns:
            np.array: –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ (0: SELL, 1: HOLD, 2: BUY)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if 'timestamp' in df.columns:
            if not pd.api.types.is_numeric_dtype(df['timestamp']):
                print(f"‚ö†Ô∏è timestamp –Ω–µ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {df['timestamp'].dtype}, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º")
                df['timestamp'] = pd.to_numeric(df['timestamp'])
            df = df.sort_values('timestamp')
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º threshold –∏–∑ config.py
        adaptive_threshold = self.calculate_adaptive_threshold(df, config.PRICE_CHANGE_THRESHOLD)
        
        prices = df['close'].values
        labels = []

        # DEBUG: –ª–æ–≥ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Å—Ä–µ–∑–∞ —Ü–µ–Ω
        # –ï—Å–ª–∏ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
        try:
            # print(f"[LABELS DEBUG] adaptive_threshold={adaptive_threshold}, future_window={config.FUTURE_WINDOW}, len(prices)={len(prices)}")
            # print("[LABELS DEBUG] first 8 closes:", prices[:8].tolist())
            # print("[LABELS DEBUG] last 8 closes:", prices[-8:].tolist())
            pass
        except Exception:
            pass

        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º future_window –∏–∑ config.py
        for i in range(len(prices) - config.FUTURE_WINDOW):
            current_price = float(prices[i])
            future_price = float(prices[i + config.FUTURE_WINDOW])

            # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            if current_price == 0 or np.isnan(current_price) or np.isinf(current_price):
                price_change = 0.0
            else:
                price_change = (future_price - current_price) / float(current_price)

        # DEBUG –¥–ª—è –ø–µ—Ä–≤—ã—Ö 20 –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        # if i < 20:
        #     print(f"[LABELS DEBUG] i={i}, cur={current_price:.6f}, fut={future_price:.6f}, change={price_change:.6f}")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
            if price_change > adaptive_threshold:
                labels.append(2)  # BUY
            elif price_change < -adaptive_threshold:
                labels.append(0)  # SELL
            else:
                labels.append(1)  # HOLD

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫
        vals, counts = np.unique(labels, return_counts=True)
        dist = {int(v): int(c) for v, c in zip(vals, counts)}
        # –ï—Å–ª–∏ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –æ–ø—è—Ç—å —ç—Ç–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
        # print(f"[LABELS DEBUG] label distribution (SELL=0,HOLD=1,BUY=2): {dist}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
        total = len(labels)
        hold_count = dist.get(1, 0)
        hold_percentage = hold_count / total if total > 0 else 0
        
        if hold_percentage > 0.8:
            print(f"[HOLD WARNING] –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç HOLD –º–µ—Ç–æ–∫: {hold_percentage:.2%}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            if total > 0:
                sample_changes = []
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º config.FUTURE_WINDOW
                for j in range(min(200, len(prices) - config.FUTURE_WINDOW)):
                    cp = float(prices[j])
                    fp = float(prices[j+config.FUTURE_WINDOW])
                    if cp == 0:
                        pct = 0.0
                    else:
                        pct = (fp - cp) / cp
                    sample_changes.append(pct)
                
                # print(f"[HOLD DEBUG] Symbol likely all-HOLD. sample changes (first 50): {np.array(sample_changes)[:50].tolist()}")
                # print(f"[HOLD DEBUG] Change stats: min={np.min(sample_changes):.6f}, max={np.max(sample_changes):.6f}, "
                #       f"mean={np.mean(sample_changes):.6f}, std={np.std(sample_changes):.6f}")
                print(f"[HOLD DEBUG] Current adaptive threshold: {adaptive_threshold:.6f}")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, –∫–∞–∫–æ–π –ø–æ—Ä–æ–≥ –Ω—É–∂–µ–Ω –¥–ª—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                changes_abs = np.abs(sample_changes)
                changes_sorted = np.sort(changes_abs)
                
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –±—ã –¥–∞–ª –ø—Ä–∏–º–µ—Ä–Ω–æ 30% —Å–∏–≥–Ω–∞–ª–æ–≤ (–Ω–µ HOLD)
                target_idx = int(len(changes_sorted) * 0.7)  # 70-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
                if target_idx < len(changes_sorted):
                    suggested_threshold = changes_sorted[target_idx]
                    print(f"[HOLD DEBUG] –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ~30% —Å–∏–≥–Ω–∞–ª–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä–æ–≥: {suggested_threshold:.6f}")
        
        return np.array(labels)
    
    def prepare_supervised_data(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è supervised learning (—ç—Ç–∞–ø 1)
        —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ config.py.
        
        Args:
            df (pd.DataFrame): DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ü–µ–Ω
            
        Returns:
            tuple: (X, labels) - –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–∫–∏
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º)
        X, _, processed_df = self.prepare_data(df)
        
        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –±–µ–∑ –ø–µ—Ä–µ–¥–∞—á–∏ threshold –∏ future_window
        labels = self.create_trading_labels(processed_df)
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–ª–∏–Ω—ã X –∏ labels —Å–æ–≤–ø–∞–¥–∞—é—Ç
        min_len = min(len(X), len(labels))
        print(f"[PREPARE DEBUG] before trim: len(X)={len(X)}, len(labels)={len(labels)}, using min_len={min_len}")
        X = X[:min_len]
        labels = labels[:min_len]
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 30 –º–µ—Ç–æ–∫
        print(f"[PREPARE DEBUG] labels sample (first 30): {labels[:30].tolist()}")
        
        return X, labels
    
    def save_scaler(self, path='models'):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        """
        if not os.path.exists(path):
            os.makedirs(path)
        
        with open(os.path.join(path, 'scaler.pkl'), 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print(f"–°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}/scaler.pkl")

    def load_scaler(self, path='models'):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä
        """
        scaler_path = os.path.join(path, 'scaler.pkl')
        
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print("–°–∫–µ–π–ª–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return True
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä")
            return False


def smote_time_series(X, y, minority_class=0, k_neighbors=5, n_samples=None):
    """
    SMOTE –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π

    Args:
        X (np.array): –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ shape (n_samples, seq_len, n_features)
        y (np.array): –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
        minority_class (int): –ö–ª–∞—Å—Å –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        k_neighbors (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏
        n_samples (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

    Returns:
        tuple: (X_augmented, y_augmented) - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    from sklearn.neighbors import NearestNeighbors
    import random
    import psutil
    import gc

    print(f"üîÑ SMOTE: –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∞ {minority_class}")

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        print(f"üíæ SMOTE: –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {available_memory_gb:.2f} GB")
        
        if available_memory_gb < 1.0:  # üî• –ò–ó–ú–ï–ù–ï–ù–û: –£–º–µ–Ω—å—à–∏–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å 2.0 –¥–æ 1.0 GB
            print(f"‚ö†Ô∏è SMOTE: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ ({available_memory_gb:.2f} GB < 1.0 GB). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º SMOTE.")
            return X, y

        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã minority –∫–ª–∞—Å—Å–∞
        minority_indices = np.where(y == minority_class)[0]
        n_minority = len(minority_indices)

        if n_minority == 0:
            print(f"‚ö†Ô∏è SMOTE: –ù–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤ –∫–ª–∞—Å—Å–∞ {minority_class}")
            return X, y

        # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï: –ï—Å–ª–∏ minority –∫–ª–∞—Å—Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –µ–≥–æ
        MAX_MINORITY_SIZE = 50000  # –ú–∞–∫—Å–∏–º—É–º 50k –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è SMOTE
        if n_minority > MAX_MINORITY_SIZE:
            print(f"‚ö†Ô∏è SMOTE: Minority –∫–ª–∞—Å—Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({n_minority} > {MAX_MINORITY_SIZE})")
            print(f"üîÑ SMOTE: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ {MAX_MINORITY_SIZE} –æ–±—Ä–∞–∑—Ü–æ–≤")
            
            # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ minority –∫–ª–∞—Å—Å–∞
            random_indices = np.random.choice(minority_indices, size=MAX_MINORITY_SIZE, replace=False)
            minority_indices = random_indices
            n_minority = len(minority_indices)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        if n_samples is None:
            n_samples = n_minority
        
        # üî• –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        MAX_SYNTHETIC_SAMPLES = 25000
        if n_samples > MAX_SYNTHETIC_SAMPLES:
            print(f"‚ö†Ô∏è SMOTE: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å {n_samples} –¥–æ {MAX_SYNTHETIC_SAMPLES} –æ–±—Ä–∞–∑—Ü–æ–≤")
            n_samples = MAX_SYNTHETIC_SAMPLES

        print(f"üîÑ SMOTE: Minority –∫–ª–∞—Å—Å –∏–º–µ–µ—Ç {n_minority} –æ–±—Ä–∞–∑—Ü–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º {n_samples} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–∞—Å—Å–∏–≤—ã –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        synthetic_X = []
        synthetic_y = []

        # –û–±—É—á–∞–µ–º KNN –Ω–∞ minority –∫–ª–∞—Å—Å–µ
        minority_X = X[minority_indices]
        print(f"üîÑ SMOTE: –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è KNN, —Ä–∞–∑–º–µ—Ä: {minority_X.shape}")
        
        minority_X_flat = minority_X.reshape(minority_X.shape[0], -1)  # Flatten –¥–ª—è KNN
        print(f"üîÑ SMOTE: Flattened —Ä–∞–∑–º–µ—Ä: {minority_X_flat.shape}")

        # üî• –î–û–ë–ê–í–õ–Ø–ï–ú –æ–±—Ä–∞–±–æ—Ç–∫—É –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è KNN
        print(f"üîÑ SMOTE: –û–±—É—á–∞–µ–º KNN —Å {k_neighbors} —Å–æ—Å–µ–¥—è–º–∏...")
        nbrs = NearestNeighbors(n_neighbors=min(k_neighbors+1, n_minority), algorithm='ball_tree')
        nbrs.fit(minority_X_flat)
        print(f"‚úÖ SMOTE: KNN –æ–±—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        distances, indices = nbrs.kneighbors(minority_X_flat)
        print(f"‚úÖ SMOTE: –ù–∞–π–¥–µ–Ω—ã –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—Ä–∞–∑—Ü—ã –±–∞—Ç—á–∞–º–∏
        BATCH_SIZE = 1000
        for batch_start in range(0, n_samples, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, n_samples)
            batch_size = batch_end - batch_start
            
            print(f"üîÑ SMOTE: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {batch_start+1}-{batch_end}/{n_samples}")
            
            for i in range(batch_size):
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü –∏–∑ minority –∫–ª–∞—Å—Å–∞
                random_idx = random.randint(0, n_minority - 1)
                sample = minority_X[random_idx]

                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Å–æ—Å–µ–¥–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º–æ–≥–æ —Å–µ–±—è)
                available_neighbors = indices[random_idx][1:]  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º–æ–≥–æ —Å–µ–±—è
                if len(available_neighbors) == 0:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ—Å–µ–¥–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º –æ–±—Ä–∞–∑–µ—Ü
                    neighbor = sample
                else:
                    neighbor_idx = random.choice(available_neighbors)
                    neighbor = minority_X[neighbor_idx]

                # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º –º–µ–∂–¥—É sample –∏ neighbor
                alpha = random.random()
                synthetic_sample = sample + alpha * (neighbor - sample)

                synthetic_X.append(synthetic_sample)
                synthetic_y.append(minority_class)
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            if (batch_end) % 5000 == 0:
                gc.collect()
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                print(f"üíæ SMOTE: –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –±–∞—Ç—á–∞: {available_memory_gb:.2f} GB")
                
                if available_memory_gb < 1.0:
                    print(f"‚ö†Ô∏è SMOTE: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ –ø–∞–º—è—Ç–∏! –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–∞ {batch_end} –æ–±—Ä–∞–∑—Ü–∞—Ö")
                    break

        print(f"üîÑ SMOTE: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º {len(synthetic_X)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ numpy –º–∞—Å—Å–∏–≤—ã...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã
        if len(synthetic_X) > 0:
            synthetic_X = np.array(synthetic_X)
            synthetic_y = np.array(synthetic_y)
            
            print(f"‚úÖ SMOTE: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã: {synthetic_X.shape}")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            X_augmented = np.concatenate([X, synthetic_X], axis=0)
            y_augmented = np.concatenate([y, synthetic_y], axis=0)

            print(f"‚úÖ SMOTE: –î–∞–Ω–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Å {len(X)} –¥–æ {len(X_augmented)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            print(f"   –ö–ª–∞—Å—Å {minority_class}: {n_minority} -> {n_minority + len(synthetic_y)}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del synthetic_X, synthetic_y, minority_X, minority_X_flat
            gc.collect()

            return X_augmented, y_augmented
        else:
            print(f"‚ö†Ô∏è SMOTE: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—Ä–∞–∑—Ü—ã")
            return X, y

    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ SMOTE: {e}")
        import traceback
        traceback.print_exc()
        print(f"üîÑ SMOTE: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        return X, y


def apply_smote_to_training_data(X_train, y_train, target_class_distribution=None):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç SMOTE –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤

    Args:
        X_train (np.array): –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        y_train (np.array): –ú–µ—Ç–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        target_class_distribution (dict): –¶–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ {class: percentage}

    Returns:
        tuple: (X_balanced, y_balanced) - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    try:
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º SMOTE –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        print(f"üìä –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: X_train={X_train.shape}, y_train={y_train.shape}")
        
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
        if len(X_train) > 500000:
            print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö ({len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤)")
            print(f"‚ö†Ô∏è SMOTE –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        unique_classes, counts = np.unique(y_train, return_counts=True)
        total_samples = len(y_train)

        current_distribution = {}
        for cls, count in zip(unique_classes, counts):
            current_distribution[cls] = count / total_samples * 100

        print("üìä –¢–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        for cls, percentage in current_distribution.items():
            print(f"   –ö–ª–∞—Å—Å {cls}: {percentage:.2f}% ({counts[list(unique_classes).index(cls)]} –æ–±—Ä–∞–∑—Ü–æ–≤)")

        # –ï—Å–ª–∏ —Ü–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –∑–∞–¥–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ
        if target_class_distribution is None:
            target_percentage = 100.0 / len(unique_classes)
            target_class_distribution = {cls: target_percentage for cls in unique_classes}

        print("üéØ –¶–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        for cls, percentage in target_class_distribution.items():
            print(f"   –ö–ª–∞—Å—Å {cls}: {percentage:.2f}%")

        X_balanced = X_train.copy()
        y_balanced = y_train.copy()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è –∫–∞–∂–¥–æ–≥–æ minority –∫–ª–∞—Å—Å–∞
        for cls in unique_classes:
            if cls not in target_class_distribution:
                continue

            current_count = counts[list(unique_classes).index(cls)]
            target_count = int(total_samples * target_class_distribution[cls] / 100.0)
            samples_to_generate = max(0, target_count - current_count)

            if samples_to_generate > 0:
                print(f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {samples_to_generate} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}")
                try:
                    X_balanced, y_balanced = smote_time_series(
                        X_balanced, y_balanced,
                        minority_class=cls,
                        n_samples=samples_to_generate
                    )
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –æ–±—Ä–∞–∑—Ü—ã –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}: {e}")
                    print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞")
                    continue

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        final_unique_classes, final_counts = np.unique(y_balanced, return_counts=True)
        final_total = len(y_balanced)

        print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ SMOTE:")
        for cls, count in zip(final_unique_classes, final_counts):
            percentage = count / final_total * 100
            print(f"   –ö–ª–∞—Å—Å {cls}: {percentage:.2f}% ({count} –æ–±—Ä–∞–∑—Ü–æ–≤)")

        print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X_balanced.shape}")
        print("üéâ SMOTE –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return X_balanced, y_balanced

    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ apply_smote_to_training_data: {e}")
        import traceback
        traceback.print_exc()
        print(f"üîÑ –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ SMOTE")
        return X_train, y_train


def apply_chunked_smote(X_train, y_train,
                         minority_classes=(0, 1),
                         max_synth_per_class=15000,
                         memory_guard_gb=1.5,
                         chunk_size=2000,
                         verbose=True):
    """
    –ü–∞–º—è—Ç—å-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π (chunked) SMOTE: –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏–∫—É –ø–æ –±–ª–æ–∫–∞–º –∏ —Å –∫–∞–ø–∞–º–∏ –Ω–∞ –∫–ª–∞—Å—Å.
    - –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –∑–∞—Ä–∞–Ω–µ–µ, –µ—Å–ª–∏ –º–∞–ª–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏.
    - –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –æ–±—â–µ–µ —á–∏—Å–ª–æ —Å–∏–Ω—Ç–µ—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.
    - –†–∞–±–æ—Ç–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏.
    """
    try:
        if verbose:
            print("üîÑ –ó–∞–ø—É—Å–∫ apply_chunked_smote (memory-aware)")
        X_bal = X_train.copy()
        y_bal = y_train.copy()

        if psutil is None:
            if verbose:
                print("‚ö†Ô∏è psutil –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–π SMOTE –±–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏")
            return apply_smote_to_training_data(X_bal, y_bal, {0:25.0,1:25.0,2:50.0})

        # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏
        avail_gb = psutil.virtual_memory().available / (1024**3)
        if avail_gb < memory_guard_gb:
            print(f"‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ ({avail_gb:.2f}GB < {memory_guard_gb}GB). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º SMOTE.")
            return X_train, y_train

        # –¢–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Ü–µ–ª–∏
        unique, counts = np.unique(y_bal, return_counts=True)
        dist = {int(k): int(v) for k, v in zip(unique, counts)}
        total = len(y_bal)
        if verbose:
            print(f"üìä –¢–µ–∫—É—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {dist}")

        caps = {cls: max_synth_per_class for cls in minority_classes}
        generated = {cls: 0 for cls in minority_classes}

        while True:
            made_progress = False
            for cls in minority_classes:
                if generated[cls] >= caps[cls]:
                    continue
                # –¶–µ–ª–∏–º—Å—è –∫ –¥–æ–ª—è–º –∏–∑ config.TARGET_CLASS_RATIOS, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –º—è–≥–∫–∞—è —Ü–µ–ª—å 30/30/40
                target_ratios = getattr(config, 'TARGET_CLASS_RATIOS', [0.3,0.3,0.4])
                target_count = int((total + sum(generated.values())) * target_ratios[cls])
                current_count = dist.get(cls, 0) + generated[cls]
                remain = max(0, min(caps[cls] - generated[cls], target_count - current_count))
                if remain <= 0:
                    continue
                step = min(chunk_size, remain)

                if verbose:
                    print(f"üîß –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º chunk {step} –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls} (—Å—É–º–º–∞—Ä–Ω–æ {generated[cls]}/{caps[cls]})")

                # –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–∞–º—è—Ç–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
                avail_gb = psutil.virtual_memory().available / (1024**3)
                if avail_gb < memory_guard_gb:
                    print(f"‚ö†Ô∏è –ü–∞–º—è—Ç—å –ø—Ä–æ—Å–µ–ª–∞ –¥–æ {avail_gb:.2f}GB ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º SMOTE")
                    return X_bal, y_bal

                try:
                    X_bal, y_bal = smote_time_series(
                        X_bal, y_bal,
                        minority_class=cls,
                        n_samples=step
                    )
                    generated[cls] += step
                    total += step
                    made_progress = True
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ SMOTE –≤ –±–ª–æ–∫–µ –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}: {e}")
                    continue

            if not made_progress:
                break

        if verbose:
            u2, c2 = np.unique(y_bal, return_counts=True)
            print("‚úÖ –ò—Ç–æ–≥ –ø–æ—Å–ª–µ chunked SMOTE:", {int(k): int(v) for k, v in zip(u2, c2)})
        return X_bal, y_bal

    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ apply_chunked_smote: {e}")
        import traceback
        traceback.print_exc()
        return X_train, y_train